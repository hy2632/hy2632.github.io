<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.0.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic|Roboto Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"hy2632.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="HW3 Hua Yao, UNI:hy2632 Problem 1: SVM algorithm in action [50 points] Description of the algorithm:  Used dual SVM Did not consider regularization, a.k.a. set \(C&#x3D;\infty\), because this problem (bina">
<meta property="og:type" content="article">
<meta property="og:title" content="Data-Mining-HW3">
<meta property="og:url" content="https://hy2632.github.io/2020/11/28/Data-Mining-HW3/index.html">
<meta property="og:site_name" content="豁蒙楼">
<meta property="og:description" content="HW3 Hua Yao, UNI:hy2632 Problem 1: SVM algorithm in action [50 points] Description of the algorithm:  Used dual SVM Did not consider regularization, a.k.a. set \(C&#x3D;\infty\), because this problem (bina">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://hy2632.github.io/2020/11/28/Data-Mining-HW3/Tutorial_7_0.png">
<meta property="article:published_time" content="2020-11-27T20:31:34.000Z">
<meta property="article:modified_time" content="2020-11-28T02:08:27.696Z">
<meta property="article:author" content="姚华(Hua Yao)">
<meta property="article:tag" content="笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hy2632.github.io/2020/11/28/Data-Mining-HW3/Tutorial_7_0.png">

<link rel="canonical" href="https://hy2632.github.io/2020/11/28/Data-Mining-HW3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Data-Mining-HW3 | 豁蒙楼</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">豁蒙楼</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/hy2632" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hy2632.github.io/2020/11/28/Data-Mining-HW3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars2.githubusercontent.com/u/54726816?s=460&u=39a4b895b570f1c0a114d932d9eff85d49cbe399&v=4">
      <meta itemprop="name" content="姚华(Hua Yao)">
      <meta itemprop="description" content="hy2632@columbia.edu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="豁蒙楼">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Data-Mining-HW3
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-11-27 15:31:34 / 修改时间：21:08:27" itemprop="dateCreated datePublished" datetime="2020-11-27T15:31:34-05:00">2020-11-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Data-Mining/" itemprop="url" rel="index"><span itemprop="name">Data Mining</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2020/11/28/Data-Mining-HW3/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/11/28/Data-Mining-HW3/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="hw3">HW3</h1>
<p>Hua Yao, UNI:hy2632</p>
<h2 id="problem-1-svm-algorithm-in-action-50-points">Problem 1: SVM algorithm in action [50 points]</h2>
<h3 id="description-of-the-algorithm">Description of the algorithm:</h3>
<ol type="1">
<li><p>Used dual SVM</p></li>
<li><p>Did not consider regularization, a.k.a. set <span class="math inline">\(C=\infty\)</span>, because this problem (binary classification between 0/9) should be separable, and the representation of <span class="math inline">\(b\)</span> becomes nasty with regularization.</p></li>
<li><p>Used the SMO (sequential minimal optimization) algorithm. During optimization, within each iteration, randomly select <span class="math inline">\(\alpha_1, \alpha_2\)</span>, optimize the QP w.r.t. <span class="math inline">\(\alpha_2\)</span> and update <span class="math inline">\(\alpha_1\)</span> accordingly. Added constraint <span class="math inline">\(\alpha_2 \geq 0\)</span> to the <span class="math inline">\(\alpha_2\)</span> optimizer. This does not constrain <span class="math inline">\(\alpha_1\geq 0\)</span> directly. However, with the randomization within each iteration, <span class="math inline">\(\alpha_i \geq 0\)</span> is satisfied when the whole optimzation over <span class="math inline">\(\alpha\)</span> finally converges.</p></li>
<li><p>Provide 2 options: Linear Kernel (baseline SVM) or Softmax kernel</p>
<p><span class="math display">\[K_{SM}(x, y) = \exp{x^\top y}\]</span></p>
<p>To avoid explosion on scale, normalized the input <span class="math inline">\(x\)</span>.</p>
<p>Included the trigonometric feature map <span class="math inline">\(\phi(x)\)</span> of the softmax kernel for calculating <span class="math inline">\(b\)</span>, (not for <span class="math inline">\(w\)</span> because when making prediction, we use kernel function instead of <span class="math inline">\(w^\top \phi\)</span>).</p>
<p>Use exact kernel instead of approximating kernel with random feature map, because softmax's dimensionality is infinite. Directly compute the exponential in numpy is more efficient.</p>
<p>The prediction comes like this (vectorized version):</p>
<p><span class="math display">\[y_{new} = K(X_{new}, X)\cdot (\alpha * y) + b\]</span></p>
<p>Note that b is broadcasted to <span class="math inline">\(n&#39;\)</span> new data points. <span class="math inline">\(K(X_{new}, X)\)</span> is <span class="math inline">\(n&#39;  \times n\)</span> kernel matrix. <span class="math inline">\(\alpha*y\)</span> means the elementwise product of two vectors.</p></li>
<li><p>SVM is expensive when <span class="math inline">\(n\)</span> is large. Here in practice, we trained on a small batch (default=64). The randomness here influences the performance.</p></li>
<li><p>Have a few trial runs to get the model with best prediction on the training data. Then it should give good prediction on the validation data. You might also need to tune the hyperparameters a little bit, like <code>batch_size</code> and <code>tol</code>.</p></li>
</ol>
<h3 id="classes-kernel-and-svm">Classes: Kernel and SVM</h3>
<h4 id="kernel_featuremaps.py"><a target="_blank" rel="noopener" href="https://github.com/hy2632/cs229/blob/master/SVM/Kernel_FeatureMaps.py">Kernel_FeatureMaps.py</a></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_schmidt_columns</span>(<span class="params">X</span>):</span></span><br><span class="line">    Q, R = np.linalg.qr(X)</span><br><span class="line">    <span class="keyword">return</span> Q</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">orthgonalize</span>(<span class="params">V</span>):</span></span><br><span class="line">    N = V.shape[<span class="number">0</span>]</span><br><span class="line">    d = V.shape[<span class="number">1</span>]</span><br><span class="line">    turns = int(N / d)</span><br><span class="line">    remainder = N % d</span><br><span class="line"></span><br><span class="line">    V_ = np.zeros_like(V)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(turns):</span><br><span class="line">        v = gram_schmidt_columns(V[i * d:(i + <span class="number">1</span>) * d, :].T).T</span><br><span class="line">        V_[i * d:(i + <span class="number">1</span>) * d, :] = v</span><br><span class="line">    <span class="keyword">if</span> remainder != <span class="number">0</span>:</span><br><span class="line">        V_[turns * d:, :] = gram_schmidt_columns(V[turns * d:, :].T).T</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> V_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate orthogonal normal weights (w1, ..., wm)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateGMatrix</span>(<span class="params">m, d</span>) -&gt; np.array:</span></span><br><span class="line">    G = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, (m, d))</span><br><span class="line">    <span class="comment"># Renormalize</span></span><br><span class="line">    norms = np.linalg.norm(G, axis=<span class="number">1</span>).reshape([m, <span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> orthgonalize(G) * norms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Softmax trignometric feature map Φ(x)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">baseline_SM</span>(<span class="params">x, G</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Calculate the result of softmax trigonometrix random feature mapping</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        x: array, dimension = n*d</span></span><br><span class="line"><span class="string">            Input to the baseline mapping</span></span><br><span class="line"><span class="string">            Required to be of norm 1 for i=1,...n</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        G: matrix, dimension = m*d</span></span><br><span class="line"><span class="string">            The matrix in the baseline random feature mapping</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m = G.shape[<span class="number">0</span>]</span><br><span class="line">    left = np.cos(np.dot(x, G.T).astype(np.float32))</span><br><span class="line">    right = np.sin(np.dot(x, G.T).astype(np.float32))</span><br><span class="line">    <span class="keyword">return</span> np.exp(<span class="number">0.5</span>) * ((<span class="number">1</span> / m)**<span class="number">0.5</span>) * np.hstack([left, right])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Kernel</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Kernels.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        x, y: array of (n_x, d), (n_y, d)</span></span><br><span class="line"><span class="string">        Return</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        K(x,y): kernel matrix of (n_x, n_y), K_ij = K(x_i, y_j)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Linear</span>(<span class="params">x, y</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.dot(x, y.T)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Softmax</span>(<span class="params">x, y</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.exp(np.dot(x, y.T))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureMap</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Feature mapping</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        x: array of (n, d)</span></span><br><span class="line"><span class="string">        Return</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        Φ(x): array of (n, m), where m is usually a higher dimensionality. Here we set m = 2*n</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Linear</span>(<span class="params">x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Softmax_Trigonometric</span>(<span class="params">x</span>):</span></span><br><span class="line">        n, d = x.shape</span><br><span class="line">        <span class="comment"># Increase dimensionality to 2x</span></span><br><span class="line">        G = generateGMatrix(<span class="number">2</span> * d, d)</span><br><span class="line">        phi_x = baseline_SM(x, G)</span><br><span class="line">        <span class="keyword">return</span> phi_x</span><br></pre></td></tr></table></figure>
<h4 id="svm.py"><a target="_blank" rel="noopener" href="https://github.com/hy2632/cs229/blob/master/SVM/SVM.py">SVM.py</a></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"><span class="keyword">from</span> tqdm.notebook <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> Kernel_FeatureMaps <span class="keyword">import</span> *;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVM</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">    ## Author:</span></span><br><span class="line"><span class="string">        Hua Yao (hy2632@columbia.edu)</span></span><br><span class="line"><span class="string">    ## Description:</span></span><br><span class="line"><span class="string">        SVM binary classifier, optimizing with the dual lagrangian program, trained on a sample batch. Uses the SMO algorithm.</span></span><br><span class="line"><span class="string">        Normalizes input X to adapt to kernelized version.</span></span><br><span class="line"><span class="string">        Regularization parameter C set to be `np.inf` for easy closed form solution of b.</span></span><br><span class="line"><span class="string">    ## Reference:</span></span><br><span class="line"><span class="string">        [CS229 - Kernel Methods and SVM](http://cs229.stanford.edu/notes2020fall/notes2020fall/cs229-notes3.pdf)</span></span><br><span class="line"><span class="string">    ---</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        X: (N, d)</span></span><br><span class="line"><span class="string">        Y: (N,)</span></span><br><span class="line"><span class="string">        kernel: kernel used, linear/softmax</span></span><br><span class="line"><span class="string">        featuremap: feature mapping corresponding to the kernel used</span></span><br><span class="line"><span class="string">        batch_size: int, also denoted as n</span></span><br><span class="line"><span class="string">        C: l1 regularization term for soft margin. alpha_i in [0, C]. Set as np.inf (no regularization), because the form of b is nasty under regularization.</span></span><br><span class="line"><span class="string">        tol = 1e-6: tolerance, deciding when to end training</span></span><br><span class="line"><span class="string">        Intermediate parameters:</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        x: (n, d), random batch of X</span></span><br><span class="line"><span class="string">        y: (n)</span></span><br><span class="line"><span class="string">        phi_x: (n, m), feature map(s) of x</span></span><br><span class="line"><span class="string">        M: (n, n), M[i,j] = y_iy_j * K(x_i,x_j), hadamard product of y^Ty and K</span></span><br><span class="line"><span class="string">        Learned parameters:</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        alpha: (n,)</span></span><br><span class="line"><span class="string">        w: (d,)</span></span><br><span class="line"><span class="string">        b: int</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 X,</span></span></span><br><span class="line"><span class="function"><span class="params">                 Y,</span></span></span><br><span class="line"><span class="function"><span class="params">                 kernel=Kernel.Linear,</span></span></span><br><span class="line"><span class="function"><span class="params">                 featuremap=FeatureMap.Linear,</span></span></span><br><span class="line"><span class="function"><span class="params">                 batch_size=<span class="number">64</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 C=np.inf,</span></span></span><br><span class="line"><span class="function"><span class="params">                 tol=<span class="number">1e-6</span></span>):</span></span><br><span class="line">        <span class="comment"># C set as np.inf here -- no regularization</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># X: (N,d), Y: (N,), x: (n,d), y:(n,)</span></span><br><span class="line">        <span class="comment"># Fixed values</span></span><br><span class="line">        self.N, self.d = X.shape</span><br><span class="line">        <span class="comment"># Normalize data</span></span><br><span class="line">        self.X = X</span><br><span class="line">        self.X = self.X / np.linalg.norm(self.X, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        self.Y = Y</span><br><span class="line">        self.kernel = kernel</span><br><span class="line">        self.featuremap = featuremap</span><br><span class="line">        self.n = batch_size</span><br><span class="line">        self.C = C</span><br><span class="line">        self.tol = tol</span><br><span class="line"></span><br><span class="line">        batch_indices = np.random.choice(np.arange(self.N), self.n)</span><br><span class="line">        self.x = self.X[batch_indices]</span><br><span class="line">        self.y = self.Y[batch_indices]</span><br><span class="line">        self.phi_x = self.featuremap(self.x)</span><br><span class="line">        self.M = np.outer(self.y, self.y) * self.kernel(self.x, self.x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Learned parameters</span></span><br><span class="line">        self.alpha = np.ones(self.n)</span><br><span class="line">        self.w = np.zeros(self.d)</span><br><span class="line">        self.b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_alpha</span>(<span class="params">self, random_idx1, random_idx2</span>):</span></span><br><span class="line">        Zeta = -np.sum(self.alpha * self.y) + (</span><br><span class="line">            self.alpha * self.y)[random_idx1] + (self.alpha *</span><br><span class="line">                                                 self.y)[random_idx2]</span><br><span class="line">        self.alpha[random_idx1] = (Zeta - self.alpha[random_idx2] *</span><br><span class="line">                                   self.y[random_idx2]) * self.y[random_idx1]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dual_obj</span>(<span class="params">self, alpha</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.sum(alpha) - np.sum(<span class="number">0.5</span> * self.M * np.outer(alpha, alpha))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, iterations=<span class="number">200000</span></span>):</span></span><br><span class="line">        prev_val = self.alpha.copy()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(range(iterations)):</span><br><span class="line">            <span class="comment"># Select 2 alphas randomly</span></span><br><span class="line">            random_idx1, random_idx2 = np.random.choice(</span><br><span class="line">                np.arange(<span class="number">0</span>, self.n), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># The (quadratic w.r.t a2) function that scipy.optimize.minimize takes</span></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">optimizeWRTa2</span>(<span class="params">a2</span>):</span></span><br><span class="line">                self.alpha[random_idx2] = a2</span><br><span class="line">                self.update_alpha(random_idx1, random_idx2)</span><br><span class="line">                <span class="keyword">return</span> -self.dual_obj(self.alpha)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Solve optimization w.r.t a2</span></span><br><span class="line">            a2 = self.alpha[random_idx2]</span><br><span class="line">            res = minimize(optimizeWRTa2, a2, bounds=[(<span class="number">0</span>, self.C)])</span><br><span class="line">            a2 = res.x</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update a2</span></span><br><span class="line">            self.alpha[random_idx2] = a2</span><br><span class="line">            self.update_alpha(random_idx1, random_idx2)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Check convergence</span></span><br><span class="line">            <span class="keyword">if</span> (i % <span class="number">5</span> == <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> np.sum(np.abs(self.alpha - prev_val)) &lt; self.tol:</span><br><span class="line">                    print(</span><br><span class="line">                        <span class="string">f&quot;&gt;&gt; Optimized on the batch, step <span class="subst">&#123;i&#125;</span>. 5 steps Δalpha:<span class="subst">&#123;np.sum(np.abs(self.alpha - prev_val))&#125;</span>&quot;</span></span><br><span class="line">                    )</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> (i % <span class="number">5000</span> == <span class="number">1</span>):</span><br><span class="line">                        print(</span><br><span class="line">                            <span class="string">f&quot;&gt;&gt; Optimizing, step <span class="subst">&#123;i&#125;</span>. Δalpha:<span class="subst">&#123;np.sum(np.abs(self.alpha - prev_val))&#125;</span>&quot;</span></span><br><span class="line">                        )</span><br><span class="line">                    prev_val = self.alpha.copy()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Retrieve w and b</span></span><br><span class="line">        self.w = np.dot(self.alpha * self.y, self.phi_x)</span><br><span class="line">        <span class="comment"># The form of b changes when there exists Regularization C. So we simply cancel C here.</span></span><br><span class="line">        <span class="comment"># Look at P25 of CS229 - Note 3.</span></span><br><span class="line">        <span class="comment"># Form of b under regularization depends on alpha. (http://cs229.stanford.edu/materials/smo.pdf)</span></span><br><span class="line">        <span class="comment"># [Bias Term b in SVMs Again](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2004-11.pdf) gives a general representation</span></span><br><span class="line">        self.b = (np.max(np.dot(self.phi_x, self.w)[self.y == <span class="number">-1</span>]) +</span><br><span class="line">                  np.min(np.dot(self.phi_x, self.w)[self.y == <span class="number">1</span>])) * <span class="number">-0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, X_val</span>):</span></span><br><span class="line">        X_val_normed = X_val / np.linalg.norm(X_val, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> np.sign(</span><br><span class="line">            np.dot(self.kernel(X_val_normed, self.x), self.alpha * self.y) +</span><br><span class="line">            self.b)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span>(<span class="params">self, X_val, y_val</span>):</span></span><br><span class="line">        prediction = self.predict(X_val)</span><br><span class="line">        <span class="keyword">return</span> np.mean(prediction == y_val)</span><br></pre></td></tr></table></figure>
<h3 id="training-example-jupyter-notebook-file">Training Example: Jupyter notebook file</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"><span class="keyword">from</span> tqdm.notebook <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> SVM <span class="keyword">import</span> SVM</span><br><span class="line"><span class="keyword">from</span> Kernel_FeatureMaps <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<h4 id="data-extraction-from-mnist">"0"/"9" Data Extraction from MNIST</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,),(<span class="number">0.5</span>)),</span><br><span class="line">])</span><br><span class="line">trainset = datasets.MNIST(<span class="string">&#x27;PATH_TO_STORE_TRAINSET&#x27;</span>, download=<span class="literal">True</span>, train=<span class="literal">True</span>, transform=transform)</span><br><span class="line">valset = datasets.MNIST(<span class="string">&#x27;PATH_TO_STORE_TESTSET&#x27;</span>, download=<span class="literal">True</span>, train=<span class="literal">False</span>, transform=transform)</span><br><span class="line"></span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset)</span><br><span class="line">valloader = torch.utils.data.DataLoader(valset)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ZeroNineTrain = [i <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(trainset) <span class="keyword">if</span> (i[<span class="number">1</span>] == <span class="number">0</span>)|(i[<span class="number">1</span>] == <span class="number">9</span>)]</span><br><span class="line">X_train, y_train = zip(*ZeroNineTrain)</span><br><span class="line">X_train = torch.stack(X_train)</span><br><span class="line">y_train = torch.tensor(y_train)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ZeroNineVal = [i <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(valset) <span class="keyword">if</span> (i[<span class="number">1</span>] == <span class="number">0</span>)|(i[<span class="number">1</span>] == <span class="number">9</span>)]</span><br><span class="line">X_val, y_val = zip(*ZeroNineVal)</span><br><span class="line">X_val = torch.stack(X_val)</span><br><span class="line">y_val = torch.tensor(y_val)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(X_train[<span class="number">0</span>].squeeze(), cmap=<span class="string">&quot;gray_r&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line">y_train[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure>
<img src="/2020/11/28/Data-Mining-HW3/Tutorial_7_0.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<pre><code>tensor(0)</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.save(X_train, <span class="string">&quot;X_train.pt&quot;</span>)</span><br><span class="line">torch.save(y_train, <span class="string">&quot;y_train.pt&quot;</span>)</span><br><span class="line">torch.save(X_val, <span class="string">&quot;X_val.pt&quot;</span>)</span><br><span class="line">torch.save(y_val, <span class="string">&quot;y_val.pt&quot;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="load-train-val-data-and-preprocess">Load Train &amp; Val data and preprocess</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load Data</span></span><br><span class="line">X_train = torch.load(<span class="string">&quot;X_train.pt&quot;</span>)</span><br><span class="line">y_train = torch.load(<span class="string">&quot;y_train.pt&quot;</span>)</span><br><span class="line">X_val = torch.load(<span class="string">&quot;X_val.pt&quot;</span>)</span><br><span class="line">y_val = torch.load(<span class="string">&quot;y_val.pt&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Criteria: Label0: +1, Label9: -1</span></span><br><span class="line">y_train = (y_train==<span class="number">0</span>)*<span class="number">2</span><span class="number">-1</span></span><br><span class="line">y_val = (y_val==<span class="number">0</span>)*<span class="number">2</span><span class="number">-1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Formatting</span></span><br><span class="line">X_train = X_train.squeeze().view((X_train.shape[<span class="number">0</span>],<span class="number">-1</span>)).numpy()</span><br><span class="line">y_train = y_train.numpy()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_val = X_val.squeeze().view((X_val.shape[<span class="number">0</span>],<span class="number">-1</span>)).numpy()</span><br><span class="line">y_val = y_val.numpy()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">svm_linear = SVM(X_train, y_train, Kernel.Linear, FeatureMap.Linear, <span class="number">64</span>, np.inf, <span class="number">1e-4</span>)</span><br><span class="line">svm_linear.fit()</span><br><span class="line">svm_linear.score(X_train, y_train), svm_linear.score(X_val, y_val)</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(FloatProgress(value=0.0, max=200000.0), HTML(value=&#39;&#39;)))


&gt;&gt; Optimizing, step 1. Δalpha:6.0
&gt;&gt; Optimized on the batch, step 4446. 5 steps Δalpha:1.7763568394002505e-15

(0.9761623989218329, 0.9773755656108597)</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">svm_SM = SVM(X_train, y_train, Kernel.Softmax, FeatureMap.Softmax_Trigonometric, <span class="number">64</span>, np.inf, <span class="number">1e-4</span>)</span><br><span class="line">svm_SM.fit()</span><br><span class="line">svm_SM.score(X_train, y_train), svm_SM.score(X_val, y_val)</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(FloatProgress(value=0.0, max=200000.0), HTML(value=&#39;&#39;)))


&gt;&gt; Optimizing, step 1. Δalpha:15.273472888060688
&gt;&gt; Optimized on the batch, step 251. 5 steps Δalpha:0.0

(0.9834905660377359, 0.9803921568627451)</code></pre>
<h3 id="result">Result:</h3>
<p>Train Accuracy / Validation Accuracy:</p>
<ul>
<li>Linear SVM: (0.976, 0.977)</li>
<li>Softmax Kernel SVM: (0.983, 0.980)</li>
</ul>
<p>Note that the result varies over different trials because of the randomness in batch selection and ending condition.</p>
<h2 id="problem-2-kernel-ridge-regression-20-points">Problem 2: Kernel Ridge Regression [20 points]</h2>
<h3 id="general-ridge-regression">General Ridge Regression</h3>
<p><span class="math display">\[\hat{y} = X\theta\]</span></p>
<p><span class="math display">\[J(\theta) = \frac12(y-X\theta)^T(y-X\theta) + \frac{\lambda}2\|\theta\|^2\]</span></p>
<p>Let</p>
<p><span class="math display">\[\begin{aligned}
&amp; \nabla_{\theta}J(\theta) = 0 \\
\to \quad &amp; \theta = (X^TX + \lambda I_d)^{-1}X^Ty 
\end{aligned}\]</span></p>
<h3 id="method-1-approximate-kernel-with-random-features">Method 1: Approximate kernel with random features</h3>
<p>When using feature map, substituting <span class="math inline">\(X\)</span> with <span class="math inline">\(\phi = \phi(X)\)</span> gives us</p>
<p><span class="math display">\[\begin{aligned}
\theta &amp;= (\phi^T\phi + \lambda I_m)^{-1}\phi^Ty
\end{aligned}\]</span></p>
<p>When predicting,</p>
<p><span class="math display">\[\begin{aligned}
y_{new} &amp;= \phi(X_{new}) (\phi^T\phi + \lambda I_m)^{-1}\phi^Ty
\end{aligned}\]</span></p>
<p>The time complexity of computing <span class="math inline">\(\theta\)</span> is <span class="math inline">\(O(\max{(mn^2,m^3,m^2n)})\)</span>, considering the matrix inverse and the matrix multiplications.</p>
<h3 id="method-2-exact-kernel">Method 2: Exact kernel</h3>
<p>If we use matrix identity trick, we can change the dimensionality of computation.</p>
<p><span class="math display">\[\begin{aligned}
\theta &amp;= \phi^T(\phi\phi^T + \lambda I_n)^{-1}y \\
&amp;= \phi^T\big(K + \lambda I_n\big)^{-1}y
\end{aligned}\]</span></p>
<p><span class="math inline">\(K = K(X,X)\)</span> is <span class="math inline">\(n\times n\)</span> kernel matrix of the training data. This gives the closed form solution of the weight.</p>
<p>When predicting, <span class="math display">\[\begin{aligned}
y_{new} &amp;= \phi(X_{new})\phi(X)^T\big(K + \lambda I_n\big)^{-1}y \\
&amp;= K(X_{new}, X) \big(K + \lambda I_n\big)^{-1}y \\
&amp;= \kappa(X_{new}) \big(K + \lambda I_n\big)^{-1}y
\end{aligned}\]</span></p>
<p>The time complexity of computing <span class="math inline">\(\theta\)</span> is <span class="math inline">\(O(\max{(n^2d, n^3, mn^2)})\)</span>. Directly computing the kernel matrix takes <span class="math inline">\(O(n^2d)\)</span>, matrix inverse <span class="math inline">\(O(n^3)\)</span>, matrix multiplication <span class="math inline">\(O(mn^2), O(mn)\)</span></p>
<h3 id="comparison-time-complexity-of-training">Comparison: Time complexity of training</h3>
<p>When <span class="math inline">\(m \gg n\)</span>, approximate kernel takes</p>
<p><span class="math display">\[O(\max{(mn^2,m^3,m^2n)}) = O(m^3)\]</span></p>
<p>exact kernel method takes</p>
<p><span class="math display">\[O(\max{(n^2d, n^3, mn^2)})= O(mn^2)\]</span></p>
<p><span class="math display">\[\because m^3 \gg mn^2\]</span></p>
<p><span class="math display">\[\therefore \text{time complexity of approximate kernel method is higher than the exact kernel method}\]</span></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%AC%94%E8%AE%B0/" rel="tag"># 笔记</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/11/27/Machine-Learning-A-Probabilistic-Perspective-Kevin-P-Murphy/" rel="prev" title="Machine Learning: A Probabilistic Perspective, Kevin P. Murphy">
      <i class="fa fa-chevron-left"></i> Machine Learning: A Probabilistic Perspective, Kevin P. Murphy
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/12/03/Data-Mining-Lec13/" rel="next" title="Data-Mining-Lec13">
      Data-Mining-Lec13 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#hw3"><span class="nav-number">1.</span> <span class="nav-text">HW3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#problem-1-svm-algorithm-in-action-50-points"><span class="nav-number">1.1.</span> <span class="nav-text">Problem 1: SVM algorithm in action [50 points]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#description-of-the-algorithm"><span class="nav-number">1.1.1.</span> <span class="nav-text">Description of the algorithm:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#classes-kernel-and-svm"><span class="nav-number">1.1.2.</span> <span class="nav-text">Classes: Kernel and SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#kernel_featuremaps.py"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">Kernel_FeatureMaps.py</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#svm.py"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">SVM.py</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#training-example-jupyter-notebook-file"><span class="nav-number">1.1.3.</span> <span class="nav-text">Training Example: Jupyter notebook file</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#data-extraction-from-mnist"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">&quot;0&quot;&#x2F;&quot;9&quot; Data Extraction from MNIST</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#load-train-val-data-and-preprocess"><span class="nav-number">1.1.3.2.</span> <span class="nav-text">Load Train &amp; Val data and preprocess</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#result"><span class="nav-number">1.1.4.</span> <span class="nav-text">Result:</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#problem-2-kernel-ridge-regression-20-points"><span class="nav-number">1.2.</span> <span class="nav-text">Problem 2: Kernel Ridge Regression [20 points]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#general-ridge-regression"><span class="nav-number">1.2.1.</span> <span class="nav-text">General Ridge Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#method-1-approximate-kernel-with-random-features"><span class="nav-number">1.2.2.</span> <span class="nav-text">Method 1: Approximate kernel with random features</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#method-2-exact-kernel"><span class="nav-number">1.2.3.</span> <span class="nav-text">Method 2: Exact kernel</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#comparison-time-complexity-of-training"><span class="nav-number">1.2.4.</span> <span class="nav-text">Comparison: Time complexity of training</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="姚华(Hua Yao)"
      src="https://avatars2.githubusercontent.com/u/54726816?s=460&u=39a4b895b570f1c0a114d932d9eff85d49cbe399&v=4">
  <p class="site-author-name" itemprop="name">姚华(Hua Yao)</p>
  <div class="site-description" itemprop="description">hy2632@columbia.edu</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">64</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hy2632" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hy2632" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hy2632@columbia.edu" title="E-Mail → mailto:hy2632@columbia.edu" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.instagram.com/nocturnima1/" title="Instagram → https:&#x2F;&#x2F;www.instagram.com&#x2F;nocturnima1&#x2F;" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i>Instagram</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">姚华(Hua Yao)</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  




  
<script src="/js/local-search.js"></script>









<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://huo-meng-lou.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://hy2632.github.io/2020/11/28/Data-Mining-HW3/";
    this.page.identifier = "2020/11/28/Data-Mining-HW3/";
    this.page.title = "Data-Mining-HW3";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://huo-meng-lou.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
