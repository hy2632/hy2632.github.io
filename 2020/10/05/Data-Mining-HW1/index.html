<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.0.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"hy2632.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Description 1 Problem 1: Softmax-kernel approximators [50 points] The goal of this task is to test different random feature map based estimators of the softmax-kernel. Choose two feature vectors \(x,">
<meta property="og:type" content="article">
<meta property="og:title" content="Data-Mining-HW1">
<meta property="og:url" content="https://hy2632.github.io/2020/10/05/Data-Mining-HW1/index.html">
<meta property="og:site_name" content="豁蒙楼">
<meta property="og:description" content="Description 1 Problem 1: Softmax-kernel approximators [50 points] The goal of this task is to test different random feature map based estimators of the softmax-kernel. Choose two feature vectors \(x,">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=G%28i%2Cj%2C%5Ctheta%29+%3D++%5Cbegin%7Bbmatrix%7D+1+%26+%5Cdots+%26+0+%26+%5Cdots+%26+0+%26+%5Cdots+%26+0%5C%5C+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots%5C%5C+0+%26+%5Cdots+%26+cos%5Ctheta+%26+%5Cdots+%26+sin%5Ctheta+%26+%5Cdots+%26+0%5C%5C+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots%5C%5C+0+%26+%5Cdots+%26+-sin%5Ctheta+%26+%5Cdots+%26+cos%5Ctheta+%26+%5Cdots+%26+0%5C%5C+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots%5C%5C+0+%26+%5Cdots+%26+0+%26+%5Cdots+%26+0+%26+%5Cdots+%26+1%5C%5C+%5Cend%7Bbmatrix%7D+%5Ctag%7B8%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+i+%26%3C+j+%5C%5C+G_%7Bi%2Ci%7D+%26%3D+c%5C%5C+G_%7Bj%2Cj%7D+%26%3D+c%5C%5C+G_%7Bi%2Cj%7D+%26%3D+s%5C%5C+G_%7Bj%2Ci%7D+%26%3D+-s%5C%5C+G_%7Bk%2Ck%7D+%26%3D+1%2C%5C+for%5C+k%5Cne+i%5C+or%5C+j%5C%5C+G_%7Bt%2Cs%7D+%26%3D+0%2C%5C+otherwise+%5Cend%7Baligned%7D+%5Ctag%7B9%7D">
<meta property="og:image" content="https://hy2632.github.io/2020/10/05/Data-Mining-HW1/DataMining_HW1_39_0.png">
<meta property="article:published_time" content="2020-10-06T01:30:34.000Z">
<meta property="article:modified_time" content="2020-10-10T03:13:36.717Z">
<meta property="article:author" content="姚华(Hua Yao)">
<meta property="article:tag" content="笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.zhihu.com/equation?tex=G%28i%2Cj%2C%5Ctheta%29+%3D++%5Cbegin%7Bbmatrix%7D+1+%26+%5Cdots+%26+0+%26+%5Cdots+%26+0+%26+%5Cdots+%26+0%5C%5C+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots%5C%5C+0+%26+%5Cdots+%26+cos%5Ctheta+%26+%5Cdots+%26+sin%5Ctheta+%26+%5Cdots+%26+0%5C%5C+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots%5C%5C+0+%26+%5Cdots+%26+-sin%5Ctheta+%26+%5Cdots+%26+cos%5Ctheta+%26+%5Cdots+%26+0%5C%5C+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots%5C%5C+0+%26+%5Cdots+%26+0+%26+%5Cdots+%26+0+%26+%5Cdots+%26+1%5C%5C+%5Cend%7Bbmatrix%7D+%5Ctag%7B8%7D">

<link rel="canonical" href="https://hy2632.github.io/2020/10/05/Data-Mining-HW1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Data-Mining-HW1 | 豁蒙楼</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">豁蒙楼</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/hy2632" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hy2632.github.io/2020/10/05/Data-Mining-HW1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars2.githubusercontent.com/u/54726816?s=460&u=39a4b895b570f1c0a114d932d9eff85d49cbe399&v=4">
      <meta itemprop="name" content="姚华(Hua Yao)">
      <meta itemprop="description" content="hy2632@columbia.edu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="豁蒙楼">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Data-Mining-HW1
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-05 20:30:34" itemprop="dateCreated datePublished" datetime="2020-10-05T20:30:34-05:00">2020-10-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-10-09 22:13:36" itemprop="dateModified" datetime="2020-10-09T22:13:36-05:00">2020-10-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Data-Mining/" itemprop="url" rel="index"><span itemprop="name">Data Mining</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="description">Description</h1>
<h2 id="problem-1-softmax-kernel-approximators-50-points">1 Problem 1: Softmax-kernel approximators [50 points]</h2>
<p>The goal of this task is to test different random feature map based estimators of the softmax-kernel. Choose two feature vectors <span class="math inline">\(x, y \in \mathbb{R}^{20}\)</span> of unit <span class="math inline">\(L_2\)</span>-norm and with angle <span class="math inline">\(\frac{π}{3}\)</span> between them. Consider three estimators of the value of the softmax-kernel in these points: the regular one using independent sin/cos-random features (leveraging random feature maps for the Gaussian kernel) and its two modifications: the one using Givens random rotations and the one applying random Hadamard matrices (with three <strong>HD</strong>-blocks). Describe in detail each estimator. Then for the number of random projections <span class="math inline">\(m = 5, 10, 20, 30, 50, 100\)</span> and each estimator, compute its value as an average over <span class="math inline">\(s = 10\)</span> independent trials. Propose how to use those trials to compute empirical mean squared error for each estimator. Prepare plots where on <span class="math inline">\(x\)</span>-axis you put the number of random projections m used and on the <span class="math inline">\(y\)</span>-axis the average value of the estimator. Show also computed mean squared error in the form of the error-bars. Remark: If <span class="math inline">\(m &gt; d\)</span> the orthogonal trick can be still applied by constructing independent ensembles of samples such that within each ensemble samples are exactly orthogonal.</p>
<h2 id="problem-2-softmax-kernel-is-positive-semidefinite-30-points">Problem 2 Softmax-kernel is positive semidefinite [30 points]</h2>
<p><span class="math inline">\(K : \mathbb{R}^{d\times d} \to \mathbb{R}^{d}\)</span> is positive semidefinite (PSD) if:</p>
<p><span class="math display">\[v^T\kappa(X)v \geq 0\]</span></p>
<p>for any <span class="math inline">\(N&gt;0\)</span>, any set of feature vectors <span class="math inline">\(\chi = \{x_1, ..., x_N\}\subseteq \mathbb{R}^d\)</span>, matrix <span class="math inline">\(\kappa(X) := [K(x_i, x_j)]_{i,j=1,...,N} \in \mathbb{R}^{N\times N}\)</span> and any vector <span class="math inline">\(v \in \mathbb{R}^{N}\)</span>. Show that softmax-kernel is PSD.</p>
<h3 id="proof"><strong>Proof</strong>:</h3>
<!-- $$\kappa(X)_{ij} = K_{SM}(x_i, x_j) = e^{x_i^Tx_j}$$

$$v = \begin{bmatrix}v_1, v_2, ..., v_N\end{bmatrix} ,v_i \in \mathbb{R}$$

$$v^T\kappa(X)v = \begin{bmatrix}v_1, v_2, ..., v_N\end{bmatrix}\kappa(X) \begin{bmatrix}v_1\\v_2\\...\\v_N\end{bmatrix}, $$

$$ = \begin{bmatrix}e^{x_1^Tx_1}v_1 + e^{x_2^Tx_1}v_2+...+e^{x_N^Tx_1}v_N\\e^{x_1^Tx_2}v_1 + e^{x_2^Tx_2}v_2+...+e^{x_N^Tx_2}v_N\\...\\e^{x_1^Tx_N}v_1 + e^{x_2^Tx_N}v_2+...+e^{x_N^Tx_N}v_N\end{bmatrix}^T \begin{bmatrix}v_1\\v_2\\...\\v_N\end{bmatrix}$$

$$=\sum_{i=1}^{N}\sum_{j=1}^{N}{e^{x_j^Tx_i}v_iv_j}$$ -->
<p><span class="math display">\[X \in \mathbb{R}^{d\times N},\]</span> <span class="math display">\[\kappa(X)_{ij} = K_{SM}(x_i, x_j) = e^{x_i^Tx_j}\]</span> <span class="math display">\[ \kappa(X) = \exp{(X^TX)} \in \mathbb{R}^{N\times N}\]</span></p>
<h4 id="lemma1-xtx-in-mathbbrn-and-xxt-in-mathbbrd-are-psd">Lemma1: <span class="math inline">\(X^TX \in \mathbb{R}^N and \ XX^T \in \mathbb{R}^d\)</span> are PSD</h4>
<p>Proof:</p>
<p><span class="math display">\[v^T(X^TX)v = (Xv)^T(Xv) = y^Ty \geq 0\]</span></p>
<p><span class="math display">\[v \in \mathbb{R}^{N}\]</span></p>
<p><span class="math display">\[y = Xv \in \mathbb{R}^{d}\]</span></p>
<p>Similarly, $XX^T $ is PSD</p>
<h4 id="lemma2-xtxk-and-xxtk-k012...-are-psd.">Lemma2: <span class="math inline">\((X^TX)^k \ and \  (XX^T)^k, k=0,1,2,...\)</span> are PSD.</h4>
<p>Proof:</p>
<p>For <span class="math inline">\(k=0\)</span>, obviously <span class="math inline">\(I\)</span> is PSD.</p>
<p>From above, this holds when <span class="math inline">\(k=1\)</span>. Use mathematical induction, if this is true for <span class="math inline">\(k=n-1\)</span>, then for <span class="math inline">\(k=n\)</span> <span class="math display">\[ v^T(X^TX)^{n}v = (Xv)^T(XX^T)^{n-1}(Xv) = y^T(XX^T)^{n-1}y \geq 0\]</span> <span class="math display">\[ v\in \mathbb{R}^N, y = Xv \in \mathbb{R}^d\]</span> <span class="math display">\[ u^T(XX^T)^{n}u = (X^Tu)^T(X^TX)^{n-1}(X^Tu) = z^T(X^TX)^{n-1}z \geq 0,\  z\in \mathbb{R}^N \]</span> <span class="math display">\[ u\in \mathbb{R}^d, z=X^Tu\in \mathbb{R}^N\]</span></p>
<p>Therefore this holds for arbitrary <span class="math inline">\(k \in \{0,1,2,3...\}\)</span>.</p>
<h4 id="lemma3-matrix-exponential">Lemma3: Matrix exponential</h4>
<p>The exponential of a <span class="math inline">\(N\times N\)</span> matrix <span class="math inline">\(X\)</span> is: <span class="math display">\[e^{X} = \sum_{k=0}^{\infty}{\frac{1}{k!}X^k}\]</span></p>
<h4 id="using-results-above">Using results above,</h4>
<p><span class="math display">\[ \therefore \kappa(X) = \exp{(X^TX)} = \sum_{k=0}^{\infty}{\frac{1}{k!}(X^TX)^k}\]</span></p>
<p><span class="math display">\[ v^T\kappa(X)v = \sum_{k=0}^{\infty}{\frac{1}{k!}v^T(X^TX)^kv} \geq 0\]</span></p>
<p><span class="math inline">\(\therefore\)</span> Softmax-kernel is PSD.</p>
<h1 id="notebook-for-problem-1">Notebook for Problem 1</h1>
<h2 id="import-packages">Import packages</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> norm</span><br><span class="line"><span class="keyword">from</span> scipy.spatial.transform <span class="keyword">import</span> Rotation <span class="keyword">as</span> R</span><br><span class="line"><span class="keyword">import</span> scipy <span class="keyword">as</span> sp</span><br></pre></td></tr></table></figure>
<h2 id="problem-1-softmax-kernel-approximators-50-points-1">Problem 1: Softmax-kernel approximators [50 points]</h2>
<h3 id="choose-two-feature-vectors-x-y-in-mathbbr20-of-unit-l_2-norm-and-with-angle-fracπ3-between-them.">Choose two feature vectors <span class="math inline">\(x, y \in \mathbb{R}^{20}\)</span> of unit <span class="math inline">\(L_2\)</span>-norm and with angle <span class="math inline">\(\frac{π}{3}\)</span> between them.</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateXY</span>(<span class="params">d = <span class="number">20</span></span>):</span></span><br><span class="line">    <span class="comment"># init X</span></span><br><span class="line">    x = np.array([<span class="number">1</span>] + [<span class="number">0</span>]*(d<span class="number">-1</span>))</span><br><span class="line">    y = np.array([<span class="number">0.5</span>, <span class="number">3</span>**<span class="number">0.5</span>/<span class="number">2</span>] + [<span class="number">0</span>]*(d<span class="number">-2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Orthogonal Matrix Q</span></span><br><span class="line">    A = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, (d,d))</span><br><span class="line">    Q, _ = np.linalg.qr(A, mode = <span class="string">&#x27;complete&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># x^Ty=0.5 =&gt; (Qx)^T(Qy)=0.5, where Q is orthogonal</span></span><br><span class="line">    x = np.matmul(Q, x)</span><br><span class="line">    y = np.matmul(Q, y)</span><br><span class="line">    <span class="keyword">return</span> x,y</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x, y = generateXY(<span class="number">20</span>)</span><br><span class="line">x, y</span><br></pre></td></tr></table></figure>
<pre><code>(array([-0.29894506, -0.66638541, -0.18786531, -0.08032163,  0.01395138,
         0.002299  ,  0.1465464 ,  0.32123403, -0.13995845,  0.17208692,
        -0.30362139, -0.20299325, -0.00676445, -0.17788199, -0.00389046,
         0.11739096,  0.13262346, -0.07173246,  0.18018864, -0.12911171]),
 array([ 0.05091238, -0.50959723,  0.39335244, -0.36683427,  0.11539508,
        -0.07330203,  0.07384118,  0.13288575, -0.47794303,  0.09279718,
        -0.08177025,  0.01269871, -0.01592124,  0.1074321 , -0.0916396 ,
         0.05875871,  0.19641505, -0.27930029,  0.14967152,  0.01007921]))</code></pre>
<p><span class="math inline">\(K_{SM}(x,y) = e^{x^Ty} = e^{0.5}\)</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">true_value = np.exp(np.dot(x, y))</span><br><span class="line">true_value</span><br></pre></td></tr></table></figure>
<pre><code>1.6487212707001284</code></pre>
<h3 id="consider-three-estimators-of-the-value-of-the-softmax-kernel-in-these-points-describe-in-detail-each-estimator.">Consider three estimators of the value of the softmax-kernel in these points; Describe in detail each estimator.</h3>
<ul>
<li>the regular one using independent sin/cos-random features (leveraging random feature maps for the Gaussian kernel)</li>
<li>and its two modifications: the one using Givens random rotations</li>
<li>and the one applying random Hadamard matrices (with three <strong>HD</strong>-blocks).</li>
</ul>
<h4 id="answer">Answer</h4>
<ul>
<li>Regular one:
<ul>
<li>Generate matrix <span class="math inline">\(G:N(0,I_d) \in \mathbb{R}^{m\times d}\)</span></li>
<li>caculate the norms of each row(<span class="math inline">\(|w_i|^2\sim \chi^2(d)\)</span>)</li>
<li>Map through <span class="math inline">\(G\)</span>'s <span class="math inline">\(m\)</span> rows and orthogonalize <span class="math inline">\(G\)</span>'s each <span class="math inline">\(d\times d\)</span> block(or the remainder part <span class="math inline">\((m\%d) \times d\)</span>)</li>
<li>Renormalize the orthogonalized matrix with its original row norms. <span class="math display">\[ \phi_{SM}(x) = e^{\frac{||x||^2}{2}} \frac{1}{\sqrt{m}} \begin{bmatrix}\cos(w_1^Tx)\\...\\\cos(w_m^Tx)\\\sin(w_1^Tx)\\...\\\sin(w_m^Tx) \end{bmatrix} \]</span></li>
<li>Calculate <span class="math inline">\(\phi(x)\)</span> and <span class="math inline">\(\phi(y)\)</span>, and thus <span class="math inline">\(\phi(x)^T\phi(y)\)</span></li>
</ul></li>
<li>Givens random rotations:
<ul>
<li>Generate random Givens Matrices <span class="math inline">\(\{Giv_i \in \mathbb{R}^{d\times d}\}, i=1,...,k = d\log{d}\)</span></li>
<li>Generate orthogonal <span class="math inline">\(d\times d\)</span> matrix <span class="math inline">\(G_{ort} = Giv_1,...Giv_k\)</span></li>
<li>Generate diagnoal matrix <span class="math inline">\(S\)</span> with entries <span class="math inline">\(S_{ii}\)</span> such that <span class="math inline">\(S_{ii} \sim \chi(d)\)</span></li>
<li>Renormalize <span class="math inline">\(G_{ort}\)</span> by left multiplying <span class="math inline">\(S\)</span></li>
<li>Generate several such <span class="math inline">\(G_{ort} \in \mathbb{R}^{d\times d}\)</span> and concatenate to get <span class="math inline">\(G_{ort} \in \mathbb{R}^{m\times d}\)</span></li>
<li>Apply the softmax random feature function <span class="math inline">\(\phi_{SM}\)</span> like the regular one</li>
</ul></li>
<li>Hadamard Matrices
<ul>
<li>Calculate the padded size <span class="math inline">\(d&#39; = 2^T\)</span></li>
<li>Pad the input <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> to dimension of <span class="math inline">\(d&#39;\)</span></li>
<li>Generate Hadamard Matrix <span class="math inline">\(H: d&#39; \times d&#39;\)</span></li>
<li>Generate 3 random diagonal "sign-flipping" matrices <span class="math inline">\(D_i\)</span></li>
<li>Generate diagnoal matrix <span class="math inline">\(S\)</span> with entries <span class="math inline">\(S_{ii}\)</span> such that <span class="math inline">\(S_{ii} \sim \chi(d&#39;)\)</span></li>
<li><span class="math inline">\(G_{ort} = S(\frac{1}{\sqrt{d&#39;}}HD_1)(\frac{1}{\sqrt{d&#39;}}HD_2)(\frac{1}{\sqrt{d&#39;}}HD_3)\)</span></li>
<li>Generate several such <span class="math inline">\(G_{ort} \in \mathbb{R}^{d&#39;\times d&#39;}\)</span> and concatenate to get <span class="math inline">\(G_{ort} \in \mathbb{R}^{m\times d&#39;}\)</span></li>
<li>Apply the softmax random feature function <span class="math inline">\(\phi_{SM}\)</span> like the regular one</li>
</ul></li>
</ul>
<h3 id="then-for-the-number-of-random-projections-m-5-10-20-30-50-100-and-each-estimator-compute-its-value-as-an-average-over-s-10-independent-trials.">Then for the number of random projections <span class="math inline">\(m = 5, 10, 20, 30, 50, 100\)</span> and each estimator, compute its value as an average over <span class="math inline">\(s = 10\)</span> independent trials.</h3>
<h4 id="estimator1-regular-one">Estimator1: Regular one</h4>
<p><span class="math display">\[ \phi_{SM}(x) = e^{\frac{||x||^2}{2}}\phi_{Gauss}(x) = e^{\frac{||x||^2}{2}} \frac{1}{\sqrt{m}} \begin{bmatrix}\cos(w_1^Tx)\\...\\\cos(w_m^Tx)\\\sin(w_1^Tx)\\...\\\sin(w_m^Tx) \end{bmatrix} \]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_schmidt_columns</span>(<span class="params">X</span>):</span></span><br><span class="line">    Q, R = np.linalg.qr(X)</span><br><span class="line">    <span class="keyword">return</span> Q</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">orthgonalize</span>(<span class="params">V</span>):</span></span><br><span class="line">    N = V.shape[<span class="number">0</span>]</span><br><span class="line">    d = V.shape[<span class="number">1</span>]</span><br><span class="line">    turns = int(N/d)</span><br><span class="line">    remainder = N%d</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#V =  np.random.normal(size=[(turns+1)*d, d])</span></span><br><span class="line">    <span class="comment">#V =  np.random.normal(size =[N, d])</span></span><br><span class="line">    <span class="comment">#print(V.shape)</span></span><br><span class="line">    V_ = np.zeros_like(V)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(turns):</span><br><span class="line">        v = gram_schmidt_columns(V[i*d:(i+<span class="number">1</span>)*d, :].T).T</span><br><span class="line">        V_[i*d:(i+<span class="number">1</span>)*d, :] = v</span><br><span class="line">    <span class="keyword">if</span> remainder != <span class="number">0</span>:</span><br><span class="line">        V_[turns*d:,:] = gram_schmidt_columns(V[turns*d:,:].T).T</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> V_</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateGMatrix</span>(<span class="params">m = <span class="number">5</span>, d = <span class="number">20</span></span>) -&gt; np.array:</span></span><br><span class="line">    G = np.random.normal(<span class="number">0</span>,<span class="number">1</span>, (m,d))</span><br><span class="line">    <span class="comment"># Renormalize</span></span><br><span class="line">    norms = np.linalg.norm(G, axis=<span class="number">1</span>).reshape([m,<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> orthgonalize(G) * norms</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Softmax with sin/cos random features</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">baseline_SM</span>(<span class="params">x, G, m</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Calculate the result of baseline random feature mapping</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        x: array, dimension = d</span></span><br><span class="line"><span class="string">            The data point to input to the baseline mapping</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        G: matrix, dimension = m*d</span></span><br><span class="line"><span class="string">            The matrix in the baseline random feature mapping</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">        m: integer</span></span><br><span class="line"><span class="string">            The number of dimension that we want to reduce to</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    left = np.cos(np.dot(G, x).astype(np.float32))</span><br><span class="line">    right = np.sin(np.dot(G, x).astype(np.float32))</span><br><span class="line">    <span class="keyword">return</span> np.exp( (np.linalg.norm(x))**<span class="number">2</span> / <span class="number">2</span>) * ((<span class="number">1</span>/m)**<span class="number">0.5</span>) * np.append(left, right)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">baseline_SM_approximation</span>(<span class="params">x, y, m, d=<span class="number">20</span>, s=<span class="number">10</span></span>):</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(s):</span><br><span class="line">        G = generateGMatrix(m=m, d=d)</span><br><span class="line">        phi_x = baseline_SM(x, G, m)</span><br><span class="line">        phi_y = baseline_SM(y, G, m)</span><br><span class="line">        xTy = np.dot(phi_x, phi_y)</span><br><span class="line">        sum += xTy</span><br><span class="line">    <span class="keyword">return</span> sum / s</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr_m = [<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">50</span>, <span class="number">100</span>]</span><br><span class="line">arr_estimation = [baseline_SM_approximation(x, y, i, <span class="number">20</span>, <span class="number">10</span>) <span class="keyword">for</span> i <span class="keyword">in</span> arr_m]</span><br><span class="line">arr_estimation</span><br></pre></td></tr></table></figure>
<pre><code>[1.8072113811969757,
 1.69626122713089,
 1.6249037623405456,
 1.670891809463501,
 1.6728131294250488,
 1.640375828742981]</code></pre>
<h4 id="estimator2-givens-random-rotations">Estimator2: Givens random rotations</h4>
<p><span class="math inline">\(G_{ort} = Giv_1,...Giv_k, k=O(d\log{d})\)</span></p>
<p><span class="math inline">\(Giv_i \in \mathbb{R}^{d\times d}\)</span></p>
<p><img src="https://www.zhihu.com/equation?tex=G%28i%2Cj%2C%5Ctheta%29+%3D++%5Cbegin%7Bbmatrix%7D+1+%26+%5Cdots+%26+0+%26+%5Cdots+%26+0+%26+%5Cdots+%26+0%5C%5C+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots%5C%5C+0+%26+%5Cdots+%26+cos%5Ctheta+%26+%5Cdots+%26+sin%5Ctheta+%26+%5Cdots+%26+0%5C%5C+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots%5C%5C+0+%26+%5Cdots+%26+-sin%5Ctheta+%26+%5Cdots+%26+cos%5Ctheta+%26+%5Cdots+%26+0%5C%5C+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots%5C%5C+0+%26+%5Cdots+%26+0+%26+%5Cdots+%26+0+%26+%5Cdots+%26+1%5C%5C+%5Cend%7Bbmatrix%7D+%5Ctag%7B8%7D" /></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+i+%26%3C+j+%5C%5C+G_%7Bi%2Ci%7D+%26%3D+c%5C%5C+G_%7Bj%2Cj%7D+%26%3D+c%5C%5C+G_%7Bi%2Cj%7D+%26%3D+s%5C%5C+G_%7Bj%2Ci%7D+%26%3D+-s%5C%5C+G_%7Bk%2Ck%7D+%26%3D+1%2C%5C+for%5C+k%5Cne+i%5C+or%5C+j%5C%5C+G_%7Bt%2Cs%7D+%26%3D+0%2C%5C+otherwise+%5Cend%7Baligned%7D+%5Ctag%7B9%7D" /></p>
<p><a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/6246-orthogonal-random-features.pdf">Orthogonal Random Features</a></p>
<p>The idea of Orthogonal Random Features (ORF) is to impose orthogonality on the matrix on the linear transformation matrix <span class="math inline">\(G\)</span>. Note that one cannot achieve unbiased kernel estimation by simply replacing <span class="math inline">\(G\)</span> by an orthogonal matrix, since the norms of the rows of <span class="math inline">\(G\)</span> follow the -distribution, while rows of an orthogonal matrix have the unit norm. The linear transformation matrix of ORF has the following form <span class="math display">\[W_{ORF} = \frac{1}{\sigma}
SQ\]</span> where <span class="math inline">\(Q\)</span> is a uniformly distributed random orthogonal matrix. The set of rows of <span class="math inline">\(Q\)</span> forms a bases in <span class="math inline">\(R^d\)</span>. <span class="math inline">\(S\)</span> is a diagonal matrix, with diagonal entries sampled i.i.d. from the <span class="math inline">\(\chi\)</span>-distribution</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateGiv</span>(<span class="params">d</span>):</span></span><br><span class="line">    Giv = np.identity(d)</span><br><span class="line">    i = np.random.randint(<span class="number">0</span>, d<span class="number">-1</span>)</span><br><span class="line">    j = np.random.randint(i+<span class="number">1</span>, d)</span><br><span class="line">    theta = np.random.uniform(<span class="number">0</span>, <span class="number">2</span>*np.pi)</span><br><span class="line">    c = np.cos(theta)</span><br><span class="line">    s = np.sin(theta)</span><br><span class="line">    Giv[i,i] = c</span><br><span class="line">    Giv[j,j] = c</span><br><span class="line">    Giv[i,j] = s</span><br><span class="line">    Giv[j, i] = -s</span><br><span class="line">    <span class="keyword">return</span> Giv</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateS</span>(<span class="params">d</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.diag(np.sqrt(np.random.chisquare(d, (d))))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Givens_generateGort_dd</span>(<span class="params">d</span>):</span></span><br><span class="line">    k = int(d * np.log(d))</span><br><span class="line">    Gort_dd = generateGiv(d)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k<span class="number">-1</span>):</span><br><span class="line">        Gort_dd = np.matmul(Gort_dd, generateGiv(d))</span><br><span class="line">    <span class="keyword">return</span> np.matmul(generateS(d), Gort_dd)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Givens_generateGort_md</span>(<span class="params">m, d</span>):</span></span><br><span class="line">    arr_Gort_dd = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m//d + <span class="number">1</span>):</span><br><span class="line">        arr_Gort_dd.append(Givens_generateGort_dd(d)) <span class="comment"># (m//d+1, d, d)</span></span><br><span class="line">    Gort_md = np.concatenate(arr_Gort_dd, axis=<span class="number">0</span>)</span><br><span class="line">    Gort_md = Gort_md[:m, :]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Gort_md</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GivensRandomRotations_SM_approximation</span>(<span class="params">x, y, m, d=<span class="number">20</span>, s=<span class="number">10</span></span>):</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(s):</span><br><span class="line">        G = Givens_generateGort_md(m,<span class="number">20</span>)</span><br><span class="line">        phi_x = baseline_SM(x, G, m)</span><br><span class="line">        phi_y = baseline_SM(y, G, m)</span><br><span class="line">        xTy = np.dot(phi_x, phi_y)</span><br><span class="line">        sum += xTy</span><br><span class="line">    <span class="keyword">return</span> sum / s</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr_m = [<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">50</span>, <span class="number">100</span>]</span><br><span class="line">arr_estimation = [GivensRandomRotations_SM_approximation(x, y, i, <span class="number">20</span>, <span class="number">10</span>) <span class="keyword">for</span> i <span class="keyword">in</span> arr_m]</span><br><span class="line">arr_estimation</span><br></pre></td></tr></table></figure>
<pre><code>[1.3707373946905137,
 1.7438804149627685,
 1.6728084802627563,
 1.7161020755767822,
 1.689209222793579,
 1.6569807291030885]</code></pre>
<h4 id="estimator3-hadamard-matriceswith-3-hd-blocks">Estimator3: Hadamard matrices(with 3 HD-blocks)</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Hadamard</span>(<span class="params">d</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;return: Hadamard matrix of shape(2^d, 2^d)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> d==<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> np.ones((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ul = Hadamard(d<span class="number">-1</span>)</span><br><span class="line">        ur = Hadamard(d<span class="number">-1</span>)</span><br><span class="line">        bl = Hadamard(d<span class="number">-1</span>)</span><br><span class="line">        br = -Hadamard(d<span class="number">-1</span>)</span><br><span class="line">        u = np.concatenate((ul,ur),axis=<span class="number">1</span>)</span><br><span class="line">        b = np.concatenate((bl,br),axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> np.concatenate((u,b),axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalized_Hadamard</span>(<span class="params">d</span>):</span></span><br><span class="line">    H = Hadamard(d)</span><br><span class="line">    norms = np.linalg.norm(H, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> H / norms</span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateDi</span>(<span class="params">d</span>):</span></span><br><span class="line">    D_i = np.diag([np.random.choice([<span class="number">1</span>, <span class="number">-1</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, d+<span class="number">1</span>)])</span><br><span class="line">    <span class="keyword">return</span> D_i</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Uncomment to get Structured Orthogonal Random Features (SORF) solution</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Hadamard_generateGort_dd</span>(<span class="params">d</span>):</span> </span><br><span class="line">    t = np.ceil(np.log2(d)).astype(int)</span><br><span class="line">    d_ = <span class="number">2</span>**t</span><br><span class="line">    <span class="comment"># H = normalized_Hadamard(t)</span></span><br><span class="line">    H = Hadamard(t)</span><br><span class="line">    Gort_dd = np.identity(d_)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        <span class="comment"># Gort_dd = np.matmul(Gort_dd ,np.matmul(H, generateDi(d_)))</span></span><br><span class="line">        Gort_dd = np.matmul(Gort_dd ,np.matmul(<span class="number">1</span> / np.sqrt(d_) * H, generateDi(d_)))</span><br><span class="line">    <span class="comment"># return np.sqrt(d_)*Gort_dd</span></span><br><span class="line">    <span class="keyword">return</span> np.matmul(generateS(d_), Gort_dd)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Hadamard_generateGort_md</span>(<span class="params">m, d</span>):</span></span><br><span class="line">    t = np.floor(np.log2(d)).astype(int) + <span class="number">1</span></span><br><span class="line">    d_ = <span class="number">2</span>**t</span><br><span class="line">    arr_Gort_dd = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m//d_ + <span class="number">1</span>):</span><br><span class="line">        arr_Gort_dd.append(Hadamard_generateGort_dd(d))</span><br><span class="line">    Gort_md = np.concatenate(arr_Gort_dd, axis=<span class="number">0</span>)</span><br><span class="line">    Gort_md = Gort_md[:m, :]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Gort_md</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Hadamard_SM_approximation</span>(<span class="params">x, y, m, d=<span class="number">20</span>, s=<span class="number">10</span></span>):</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(s):</span><br><span class="line">        G = Hadamard_generateGort_md(m,<span class="number">20</span>)</span><br><span class="line">        d_ = G.shape[<span class="number">1</span>]</span><br><span class="line">        x_padded = np.pad(x, (<span class="number">0</span>, d_-d))</span><br><span class="line">        y_padded = np.pad(y, (<span class="number">0</span>, d_-d))</span><br><span class="line">        phi_x = baseline_SM(x_padded, G, m)</span><br><span class="line">        phi_y = baseline_SM(y_padded, G, m)</span><br><span class="line">        xTy = np.dot(phi_x, phi_y)</span><br><span class="line">        sum += xTy</span><br><span class="line">    <span class="keyword">return</span> sum / s</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr_m = [<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">50</span>, <span class="number">100</span>]</span><br><span class="line">arr_estimation = [Hadamard_SM_approximation(x, y, i, <span class="number">20</span>, <span class="number">10</span>) <span class="keyword">for</span> i <span class="keyword">in</span> arr_m]</span><br><span class="line">arr_estimation</span><br></pre></td></tr></table></figure>
<pre><code>[1.7086819410324097,
 1.617677903175354,
 1.6517545461654664,
 1.645971429347992,
 1.6519848704338074,
 1.6540889978408813]</code></pre>
<h3 id="propose-how-to-use-those-trials-to-compute-empirical-mean-squared-error-for-each-estimator.">Propose how to use those trials to compute empirical mean squared error for each estimator.</h3>
<h4 id="answer-1">Answer</h4>
<ul>
<li>The true value: <span class="math inline">\(K_{SM}(x,y) = e^{x^Ty} = e^{0.5}\)</span></li>
<li>For each <span class="math inline">\(m\)</span>, run 10 trials. <span class="math display">\[MSE = \frac{\sum_{i=1}^{10}{(est_i - e^{0.5}})^2}{10}\]</span></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MSE_iid</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">    SM_approximation=Hadamard_SM_approximation,</span></span></span><br><span class="line"><span class="function"><span class="params">    episode=<span class="number">1</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">    epoch=<span class="number">10</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">    arr_m=[<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">50</span>, <span class="number">100</span>],</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line"></span><br><span class="line">    estimation = np.zeros((len(arr_m), episode, epoch))</span><br><span class="line">    MSE_iid = []</span><br><span class="line">    list_of_samples = []</span><br><span class="line"></span><br><span class="line">    x, y = generateXY(<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># for e in tqdm(range(episode), position=0, leave=True):</span></span><br><span class="line">    <span class="comment">#     x, y = generateXY(20)</span></span><br><span class="line">    <span class="comment">#     true_value = np.exp(0.5)</span></span><br><span class="line">    <span class="comment">#     list_of_samples.append((x, y))</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(len(arr_m)):</span><br><span class="line">        mse_m = []</span><br><span class="line">        m = arr_m[n]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> e <span class="keyword">in</span> tqdm(range(episode), position=<span class="number">0</span>, leave=<span class="literal">True</span>):        </span><br><span class="line">            <span class="comment"># x = list_of_samples[e][0]</span></span><br><span class="line">            <span class="comment"># y = list_of_samples[e][1]</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</span><br><span class="line">                np.random.seed()</span><br><span class="line">                estimation[n, e, i] = SM_approximation(x, y, m, d=<span class="number">20</span>, s=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            mse_m.append(mean_squared_error(np.repeat(true_value, epoch), estimation[n, e])) <span class="comment"># compare 10 values</span></span><br><span class="line">        </span><br><span class="line">        MSE_iid.append(mse_m)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> estimation.mean(axis=(<span class="number">1</span>,<span class="number">2</span>)) ,MSE_iid</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">estimation_regular, MSE_iid_regular = MSE_iid(SM_approximation=baseline_SM_approximation)</span><br><span class="line">df_MSE_Regular = pd.DataFrame(MSE_iid_regular, index = arr_m, columns = [<span class="string">&quot;Regular&quot;</span>])</span><br><span class="line">df_estimation_Regular = pd.DataFrame(estimation_regular, index = arr_m, columns = [<span class="string">&quot;Regular&quot;</span>])</span><br></pre></td></tr></table></figure>
<pre><code>100%|██████████| 1/1 [00:00&lt;00:00, 101.45it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 119.49it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 95.60it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 112.89it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 89.43it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 74.54it/s]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">estimation_Givens, MSE_iid_Givens = MSE_iid(SM_approximation=GivensRandomRotations_SM_approximation)</span><br><span class="line">df_MSE_Givens = pd.DataFrame(MSE_iid_Givens, index = arr_m, columns = [<span class="string">&quot;GivensRotation&quot;</span>])</span><br><span class="line">df_estimation_Givens = pd.DataFrame(estimation_Givens, index = arr_m, columns = [<span class="string">&quot;GivensRotation&quot;</span>])</span><br></pre></td></tr></table></figure>
<pre><code>100%|██████████| 1/1 [00:00&lt;00:00, 25.62it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 30.17it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 13.01it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 16.68it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 14.39it/s]
100%|██████████| 1/1 [00:00&lt;00:00,  6.19it/s]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">estimation_Hadamard, MSE_iid_Hadamard = MSE_iid(SM_approximation=Hadamard_SM_approximation)</span><br><span class="line">df_MSE_Hadamard = pd.DataFrame(MSE_iid_Hadamard, index = arr_m, columns = [<span class="string">&quot;Hadamard&quot;</span>])</span><br><span class="line">df_estimation_Hadamard = pd.DataFrame(estimation_Hadamard, index = arr_m, columns = [<span class="string">&quot;Hadamard&quot;</span>])</span><br></pre></td></tr></table></figure>
<pre><code>100%|██████████| 1/1 [00:00&lt;00:00, 12.08it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 11.96it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 12.45it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 12.02it/s]
100%|██████████| 1/1 [00:00&lt;00:00,  7.46it/s]
100%|██████████| 1/1 [00:00&lt;00:00,  3.86it/s]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_MSE_total = pd.concat([df_MSE_Regular, df_MSE_Givens, df_MSE_Hadamard], axis=<span class="number">1</span>)</span><br><span class="line">df_estimation_total = pd.concat([df_estimation_Regular, df_estimation_Givens, df_estimation_Hadamard], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_MSE_total</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Regular
</th>
<th>
GivensRotation
</th>
<th>
Hadamard
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
5
</th>
<td>
0.172311
</td>
<td>
0.188588
</td>
<td>
0.162290
</td>
</tr>
<tr>
<th>
10
</th>
<td>
0.066876
</td>
<td>
0.065423
</td>
<td>
0.077146
</td>
</tr>
<tr>
<th>
20
</th>
<td>
0.007488
</td>
<td>
0.017989
</td>
<td>
0.032074
</td>
</tr>
<tr>
<th>
30
</th>
<td>
0.020978
</td>
<td>
0.027808
</td>
<td>
0.007083
</td>
</tr>
<tr>
<th>
50
</th>
<td>
0.011074
</td>
<td>
0.003390
</td>
<td>
0.004394
</td>
</tr>
<tr>
<th>
100
</th>
<td>
0.001614
</td>
<td>
0.001725
</td>
<td>
0.001746
</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_estimation_total</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Regular
</th>
<th>
GivensRotation
</th>
<th>
Hadamard
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
5
</th>
<td>
1.709679
</td>
<td>
1.735700
</td>
<td>
1.481327
</td>
</tr>
<tr>
<th>
10
</th>
<td>
1.560758
</td>
<td>
1.708040
</td>
<td>
1.657001
</td>
</tr>
<tr>
<th>
20
</th>
<td>
1.659077
</td>
<td>
1.617314
</td>
<td>
1.574311
</td>
</tr>
<tr>
<th>
30
</th>
<td>
1.615470
</td>
<td>
1.572253
</td>
<td>
1.635823
</td>
</tr>
<tr>
<th>
50
</th>
<td>
1.603345
</td>
<td>
1.656838
</td>
<td>
1.642844
</td>
</tr>
<tr>
<th>
100
</th>
<td>
1.617124
</td>
<td>
1.648726
</td>
<td>
1.638272
</td>
</tr>
</tbody>
</table>
</div>
<h3 id="prepare-plots-where-on-x-axis-you-put-the-number-of-random-projections-m-used-and-on-the-y-axis-the-average-value-of-the-estimator.-show-also-computed-mean-squared-error-in-the-form-of-the-error-bars">Prepare plots where on x-axis you put the number of random projections m used and on the y-axis the average value of the estimator. Show also computed mean squared error in the form of the error-bars</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sns.set_style(<span class="string">&quot;darkgrid&quot;</span>)    </span><br><span class="line">df_estimation_total.plot(</span><br><span class="line">    kind = <span class="string">&#x27;line&#x27;</span>, </span><br><span class="line">    title = <span class="string">&#x27;Softmax-kernel approximators comparison&#x27;</span>, </span><br><span class="line">    grid=<span class="literal">True</span>, </span><br><span class="line">    legend=[df_estimation_total.columns],</span><br><span class="line">    xlabel=<span class="string">&#x27;m&#x27;</span>, </span><br><span class="line">    ylabel=<span class="string">&#x27;Average of Estimation&#x27;</span>,</span><br><span class="line">    figsize = (<span class="number">12</span>,<span class="number">8</span>),</span><br><span class="line">    yerr=df_MSE_total,</span><br><span class="line">    )</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/10/05/Data-Mining-HW1/DataMining_HW1_39_0.png" /></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%AC%94%E8%AE%B0/" rel="tag"># 笔记</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/10/05/Data-Mining-Recitation1/" rel="prev" title="Data-Mining-Recitation1">
      <i class="fa fa-chevron-left"></i> Data-Mining-Recitation1
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/10/07/LaTex-%E9%80%9F%E6%9F%A5%E8%A1%A8/" rel="next" title="LaTex 速查表">
      LaTex 速查表 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#description"><span class="nav-number">1.</span> <span class="nav-text">Description</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#problem-1-softmax-kernel-approximators-50-points"><span class="nav-number">1.1.</span> <span class="nav-text">1 Problem 1: Softmax-kernel approximators [50 points]</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#problem-2-softmax-kernel-is-positive-semidefinite-30-points"><span class="nav-number">1.2.</span> <span class="nav-text">Problem 2 Softmax-kernel is positive semidefinite [30 points]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#proof"><span class="nav-number">1.2.1.</span> <span class="nav-text">Proof:</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#lemma1-xtx-in-mathbbrn-and-xxt-in-mathbbrd-are-psd"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">Lemma1: \(X^TX \in \mathbb{R}^N and \ XX^T \in \mathbb{R}^d\) are PSD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lemma2-xtxk-and-xxtk-k012...-are-psd."><span class="nav-number">1.2.1.2.</span> <span class="nav-text">Lemma2: \((X^TX)^k \ and \  (XX^T)^k, k&#x3D;0,1,2,...\) are PSD.</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lemma3-matrix-exponential"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">Lemma3: Matrix exponential</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#using-results-above"><span class="nav-number">1.2.1.4.</span> <span class="nav-text">Using results above,</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#notebook-for-problem-1"><span class="nav-number">2.</span> <span class="nav-text">Notebook for Problem 1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#import-packages"><span class="nav-number">2.1.</span> <span class="nav-text">Import packages</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#problem-1-softmax-kernel-approximators-50-points-1"><span class="nav-number">2.2.</span> <span class="nav-text">Problem 1: Softmax-kernel approximators [50 points]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#choose-two-feature-vectors-x-y-in-mathbbr20-of-unit-l_2-norm-and-with-angle-frac%CF%803-between-them."><span class="nav-number">2.2.1.</span> <span class="nav-text">Choose two feature vectors \(x, y \in \mathbb{R}^{20}\) of unit \(L_2\)-norm and with angle \(\frac{π}{3}\) between them.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#consider-three-estimators-of-the-value-of-the-softmax-kernel-in-these-points-describe-in-detail-each-estimator."><span class="nav-number">2.2.2.</span> <span class="nav-text">Consider three estimators of the value of the softmax-kernel in these points; Describe in detail each estimator.</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#answer"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">Answer</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#then-for-the-number-of-random-projections-m-5-10-20-30-50-100-and-each-estimator-compute-its-value-as-an-average-over-s-10-independent-trials."><span class="nav-number">2.2.3.</span> <span class="nav-text">Then for the number of random projections \(m &#x3D; 5, 10, 20, 30, 50, 100\) and each estimator, compute its value as an average over \(s &#x3D; 10\) independent trials.</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#estimator1-regular-one"><span class="nav-number">2.2.3.1.</span> <span class="nav-text">Estimator1: Regular one</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#estimator2-givens-random-rotations"><span class="nav-number">2.2.3.2.</span> <span class="nav-text">Estimator2: Givens random rotations</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#estimator3-hadamard-matriceswith-3-hd-blocks"><span class="nav-number">2.2.3.3.</span> <span class="nav-text">Estimator3: Hadamard matrices(with 3 HD-blocks)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#propose-how-to-use-those-trials-to-compute-empirical-mean-squared-error-for-each-estimator."><span class="nav-number">2.2.4.</span> <span class="nav-text">Propose how to use those trials to compute empirical mean squared error for each estimator.</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#answer-1"><span class="nav-number">2.2.4.1.</span> <span class="nav-text">Answer</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#prepare-plots-where-on-x-axis-you-put-the-number-of-random-projections-m-used-and-on-the-y-axis-the-average-value-of-the-estimator.-show-also-computed-mean-squared-error-in-the-form-of-the-error-bars"><span class="nav-number">2.2.5.</span> <span class="nav-text">Prepare plots where on x-axis you put the number of random projections m used and on the y-axis the average value of the estimator. Show also computed mean squared error in the form of the error-bars</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="姚华(Hua Yao)"
      src="https://avatars2.githubusercontent.com/u/54726816?s=460&u=39a4b895b570f1c0a114d932d9eff85d49cbe399&v=4">
  <p class="site-author-name" itemprop="name">姚华(Hua Yao)</p>
  <div class="site-description" itemprop="description">hy2632@columbia.edu</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">44</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hy2632" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hy2632" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hy2632@columbia.edu" title="E-Mail → mailto:hy2632@columbia.edu" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.instagram.com/nocturnima1/" title="Instagram → https:&#x2F;&#x2F;www.instagram.com&#x2F;nocturnima1&#x2F;" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i>Instagram</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">姚华(Hua Yao)</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  




  
<script src="/js/local-search.js"></script>









<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
