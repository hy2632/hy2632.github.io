<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>8/27OW分享会记录</title>
    <url>/2020/08/29/0827OW/</url>
    <content><![CDATA[<h2 id="basic-info">Basic Info</h2>
<p>Offices: BJ, SH, HK.</p>
<p>技能点：</p>
<ul>
<li>Big data</li>
<li>data mining</li>
<li>machine learning</li>
<li>analytical problem solving</li>
<li>co-creation</li>
<li>design thinking</li>
<li>change enablement</li>
<li>people effectiveness</li>
</ul>
<p>发展路径: 前三年跨行业</p>
<h2 id="project-span-industries-and-capabilities">Project span industries and capabilities</h2>
<p>行业：健康， 消费者工业和服务（教育 房地产）， 金融服务 <img src="https://raw.githubusercontent.com/hy2632/hy2632.github.io/master/2020/08/29/0827OW/span.jpg" alt="Image" /> 横向：不同division和三大类 纵向：不同行业</p>
<h2 id="recruiting">Recruiting</h2>
<p>digital interview: Oct 14, first round interview Oct 25, final round Oct 30. <img src="https://raw.githubusercontent.com/hy2632/hy2632.github.io/master/2020/08/29/0827OW/recruitinginfo.jpg" alt="Image" /></p>
<h2 id="breakout-room-life-as-a-new-consultant-digital-qa">Breakout room (Life as a new consultant + Digital), Q&amp;A</h2>
<figure>
<img src="https://raw.githubusercontent.com/hy2632/hy2632.github.io/master/2020/08/29/0827OW/breakoutrooms.jpg" alt="Image" /><figcaption aria-hidden="true">Image</figcaption>
</figure>
<h3 id="相对甲方战略咨询部的特点">相对甲方战略咨询部的特点</h3>
<ol type="1">
<li><p>行业选择</p></li>
<li><p>紧迫感，下一个30天、100天做什么</p></li>
<li><p>好的甲方的核心业务可能比咨询更优，因为有经验积淀</p></li>
<li><p>大厂有丰富的大数据</p></li>
<li><p>乙方咨询还有换赛道的机会，甲方从零开始</p></li>
<li><p>OW的Know:知识库 即插即用</p></li>
<li><p>甲方：战略部 -&gt; 策略部(市场策略)</p></li>
<li><p>咨询的生命周期可能是长期的，专注于商业本质，“无限游戏”</p></li>
<li><p>consultant和engineer合作更加紧密，有multinational的know平台，以及和客户ceo之类有很好的关系和实施的渠道</p></li>
</ol>
<h3 id="简要介绍不同的digital-transformation">简要介绍不同的digital transformation</h3>
<ol type="1">
<li>分行业分业务</li>
<li>例如保险 搬到线上 高净值人群需要线下服务，其实不是对象，应当优化大部分普通客户的流程。</li>
<li>digital products: 把社交媒体如微博的data变为insights，和客户一起探讨，确定客户在意的数据：例如被提到次数、提到内容、注意点。帮助识别emerging brand小众品牌，发现潜在competitors。</li>
<li>data product(server)/data government，组织形式，和各个公司有关</li>
<li>big data可以帮助做datadriven decision， 如促销机制，之前没有理论指导人为因素较大，利用大数据给出的解决方案包含数据分析和专家访谈，确定更换促销机制的周期以及更细致的客户targeting</li>
<li>总而言之可以高层次爷可接地气</li>
</ol>
<h3 id="需要编程技能">需要编程技能</h3>
<ol type="1">
<li>DE/DS/DC，DC除了技能还需要理解<strong>Business Implementation</strong></li>
<li>python必需，还要高效使用package；SQL可以现学</li>
<li>需要学过BA课程， 熟悉pandas/matplotlib等，可以看一些互联网PM的书籍，了解如何分析客户和产品，获得Business Insights</li>
<li><strong>Business implementation 不只是向客户摆事实，而应该告诉客户做什么，例如上新品类，好处的量化</strong></li>
</ol>
<h3 id="digital的发展路径">Digital的发展路径</h3>
<p>不是纵向而是横向，不局限于某个行业，和Operations/Strategy平行</p>
<h3 id="如果business-implementation的阶段性成果未达到预期ow的数字化转型团队有没有调整的机制和经验">如果Business Implementation的阶段性成果未达到预期，OW的数字化转型团队有没有调整的机制和经验</h3>
<ol type="1">
<li>举例: dashboard 的设计调整到更加 user-friendly</li>
<li>说明: 并非一个数字化团队，而是负责数字化的人员一起加入某项目。并非强调准确率，而是“赋能”，有很多qualitative的分析。有时引入新的方法或者数据量不够quant</li>
<li>不是做数据竞赛或做数据框架，而是服务于ceo的模糊想法，是一个工作方向而不是具体地帮助提高销量到某一个值</li>
<li>比如零售商想了解品牌商的想法，而咨询和品牌商有一定合作，可以提供信息</li>
</ol>
<h3 id="new-consultant的经验">new consultant的经验</h3>
<ol type="1">
<li>首先理解任务和clarify</li>
<li>时间紧</li>
<li>汇报要抓重点，不要interesting facts，而是business insights</li>
</ol>
<h3 id="如何知识点成系统">如何知识点成系统</h3>
<ol type="1">
<li>例如研究券商的core trade system</li>
<li>expert call,... 有自己的view，并逐渐完善， 除非manager带过类似项目</li>
<li>manager可以quick learning，需要学习这样的能力</li>
<li>项目完成后需要复盘</li>
</ol>
<h3 id="面试经历">面试经历</h3>
<ol type="1">
<li>第一轮笔试，数学题之类</li>
<li>读casebook， 做presentation</li>
<li>manager面 + 面， 两个digital问题</li>
<li><strong>怎么用ml帮netflix提升revenue？？</strong> 应用哪些方面？为什么？多久能达成结果？</li>
</ol>
<h3 id="fast-promotion">fast-promotion</h3>
<ol type="1">
<li>critical thinking, ownership</li>
<li>communication!!! upward and downward对上和对下</li>
<li>接受变化，疫情后医疗更多，很多公司询问疫情影响</li>
<li>学research，学写邮件，善于观察: 别的人怎么做？
<ul>
<li>写title方便locate subproject title</li>
<li>keyfinding 1 2 3, 然后细节定位</li>
<li>最后提需要review回复的时间，方便对方暂时搁置</li>
</ul></li>
<li>为上级考虑，节省上级时间</li>
<li>例如至少code不能有bug</li>
<li>组织events，增加曝光度 (120个同事)，建立工作之外关系</li>
</ol>
]]></content>
      <categories>
        <category>文章</category>
      </categories>
      <tags>
        <tag>Oliver Wyman</tag>
      </tags>
  </entry>
  <entry>
    <title>0922</title>
    <url>/2020/09/23/0922/</url>
    <content><![CDATA[<p>看到一篇讲SVM不错的文章 <a href="https://zhuanlan.zhihu.com/p/49331510">看了这篇文章你还不懂SVM你就来打我</a></p>
<p>google SVM 的 scaling，又看到了该作者的个人站点 <a href="https://tangshusen.me/">TangShusen</a>，同样是基于Hexo的next主题，感觉很值得借鉴。</p>
<p>看了下<a href="https://github.com/ShusenTang">GitHub</a>还有不少宝藏内容。关注了。</p>
]]></content>
      <categories>
        <category>misc</category>
      </categories>
  </entry>
  <entry>
    <title>CS224N-Lec15</title>
    <url>/2020/08/25/CS224N-Lec15/</url>
    <content><![CDATA[<h1 id="natural-language-generation">Natural Language Generation</h1>
<h2 id="nlg">NLG</h2>
<ul>
<li>subcomponent of
<ul>
<li>Machine Translation</li>
<li><strong>summarization</strong></li>
<li>dialogue</li>
<li>Freeform question answering(not only from the context)</li>
<li>Image Captioning</li>
</ul></li>
</ul>
<h2 id="recap">Recap</h2>
<ul>
<li><p>Language modeling? the task of predicting the next word: <span class="math display">\[P(y_t|y_1, ..., y_(t-1))\]</span></p></li>
<li><p>Language model</p></li>
<li><p>RNN-LM</p></li>
<li><p>Conditional Language Modeling <span class="math display">\[P(y_t|y_1, ..., y_(t-1), x)\]</span></p>
<ul>
<li>what is x? condition.</li>
<li>Examples:
<ul>
<li>Machine Translation (x = source sentence, y = target sentence)</li>
<li>Summarization (context and summarized)</li>
<li>Dialogue (dialogue history and next utterance)</li>
</ul></li>
</ul></li>
<li><p>training a RNN-LM? <span class="math display">\[J = \dfrac{1}{T}\sum\limits_{t=1}^T J_t\]</span></p>
<ul>
<li>"Teacher Forcing": always use the gold to feed into the decoder</li>
</ul></li>
<li><p><strong>decoding algorithms</strong></p>
<ul>
<li>Greedy decoding: argmax each step</li>
<li>Beam search: aims to find a high prob seq.
<ul>
<li>k most probable partial seqs (hypotheses)</li>
<li>k is the beam size (e.g. 2)</li>
<li>when reaching some stopping criterion, output</li>
<li>what's the effect of changing k?
<ul>
<li>k=1: greedy decoding</li>
<li>larger k: more hypotheses, computationaly expensive
<ul>
<li>for NMT, increasing k too much <strong>decreases</strong> BLEU, reason: producing shorter translations</li>
<li>for chit-chat dialogue, producing too generic responses</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>sampling-based decoding</strong>
<ul>
<li>pure sampling: randomly sample, instead of argmax in greedy</li>
<li>top-n sampling, randomly sample from top-n. truncate. n is another hyperparameter
<ul>
<li>increasing n, diverse and risky</li>
<li>decreasing n, generic and safe</li>
</ul></li>
</ul></li>
<li><strong>Softmax teperature</strong> -- not actually a decoding algorithm, but a technique applied at test time in conjunction with decoding algorithm
<ul>
<li>temperature hypparam <span class="math inline">\(\tau\)</span> to the softmax:</li>
<li>larger <span class="math inline">\(\tau\)</span>: <span class="math inline">\(P_t\)</span> becomes more uniform, more diverse output(probability is spread around vocab)</li>
</ul></li>
</ul></li>
<li><p>Decoding algorithms: summary</p>
<ul>
<li><u> Greedy </u></li>
<li><u> Beam search </u></li>
<li><u> Sampling methods </u></li>
<li><strong><em>Softmax temperature</em></strong></li>
</ul></li>
</ul>
<h2 id="section-2-nlg-tasks-and-neural-approaches">Section 2: NLG tasks and neural approaches</h2>
<h3 id="summarization">Summarization</h3>
<ul>
<li>definition: x -&gt; y, y is shorter and contains main info of x</li>
<li>examples:
<ul>
<li>Gigaword: headline -&gt; headline. sentence compression</li>
<li><strong>LCSTS</strong> (Chinese microblogging), paragraph -&gt; sentence summary</li>
<li>...</li>
</ul></li>
<li>Sentence simplification:
<ul>
<li>different but related</li>
<li>rewrite, simpler &amp; shorter</li>
<li>examples:
<ul>
<li>simple wikipedia</li>
<li>Newsela: news rewriting for children</li>
</ul></li>
</ul></li>
<li>summarization: 2 mains strategies
<ul>
<li>extractive: highlighter</li>
<li>abastractive: writing</li>
</ul></li>
<li>summarization evaluation: <strong>ROUGE</strong>
<ul>
<li><p>like <strong>BLEU</strong>, based on n-gram overlap</p></li>
<li><p>but no brevity penalty</p></li>
<li><p>ROUGE based on recall while BLEU based on precision</p></li>
<li><p>BLEU is a single number combining the precisions for n=1,2,3,4 n-grams</p></li>
<li><p>ROUGE: ROUGE-1/ROUGE-2/ROUGE-L(Largest common subseq overlap)</p></li>
</ul></li>
<li>Neural summarization:
<ul>
<li>seq2seq + attention NMT</li>
<li>Reinforcement learning</li>
</ul></li>
<li>neural: copy mechanism
<ul>
<li>probability of generation and probability of copying</li>
<li>Pgen: hard(0/1) or soft?</li>
<li>Problem:
<ul>
<li>copy too much: extractive to abstractive</li>
<li>bad at overall content selection, if input is long</li>
<li>no overall strategy for selecting content</li>
</ul></li>
</ul></li>
<li>better content selection
<ul>
<li>2 stages: content selection &amp; surface realization</li>
<li>seq2seq+att, mixed, word-level content selection(attention)</li>
<li>but no global content selection strategy</li>
<li>One solution: bottom-up summarization</li>
</ul></li>
<li>Bottom-up summarization
<ul>
<li>content selection stage: neural sequence tagging</li>
<li>masked, attention</li>
</ul></li>
<li>Neural summarization via RL
<ul>
<li>main idea: directly optimize Rouge-L</li>
<li>Better practice(both ROUGE &amp; human judgement): ML&amp;RL</li>
</ul></li>
</ul>
<h3 id="dialogue">Dialogue</h3>
]]></content>
      <categories>
        <category>CS224N</category>
      </categories>
      <tags>
        <tag>CS224N</tag>
      </tags>
  </entry>
  <entry>
    <title>09/06 论文笔记</title>
    <url>/2020/09/07/09-06-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="deep-visual-semantic-alignments-for-generating-image-descriptions"><a href="https://cs.stanford.edu/people/karpathy/cvpr2015.pdf"><strong>Deep Visual-Semantic Alignments for Generating Image Descriptions</strong></a></h2>
<p>摘要</p>
<ul>
<li>图像 CNN，文本 BiRNN，multimodal embedding，alignment model</li>
</ul>
<p>介绍部分</p>
<ul>
<li>先前：给定categories的labeling</li>
<li>目标：生成images的dense description</li>
<li>要求：模型同时推断内容和找出自然语言的表示，并且通过训练获得</li>
<li>数据集的challenge：image captioning的数据集并不包含图片中实体的定位</li>
<li>core insight: 句子作为weak labels，句子和图像的定位都未知 -&gt; 模型需要推断位置的 latent alignment</li>
</ul>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
  <entry>
    <title>CS229-LinearAlgebra Recap</title>
    <url>/2020/10/23/CS229-LinearAlgebra-Recap/</url>
    <content><![CDATA[<h2 id="link">Link</h2>
<p><a href="http://cs229.stanford.edu/summer2020/cs229-linalg.pdf">CS229: Review of Linear Algebra</a></p>
<h1 id="operations-and-properties">Operations and properties</h1>
<h2 id="norms">Norms</h2>
<p>Euclidean or <span class="math inline">\(l_2\)</span> norm;</p>
<p>More formally, a <strong>norm</strong> can be any function <span class="math inline">\(f: R^n \to R\)</span> that satisfies 4 properties:</p>
<ul>
<li>Non-negativity: <span class="math inline">\(f(x)\geq 0\)</span></li>
<li>Definiteness: <span class="math inline">\(f(x)=0\)</span> iff <span class="math inline">\(x=0\)</span></li>
<li>Homogeneity: <span class="math inline">\(f(tx) = |t|f(x)\)</span></li>
<li>Triangle inequality: <span class="math inline">\(f(x+y) \leq f(x) + f(y)\)</span></li>
</ul>
<p>Comment: According to the Bochner's Theorem, a <strong>kernel</strong> <span class="math inline">\(K(x,y): R^d \times R^d \to R\)</span> should have property that <span class="math inline">\(\kappa(X) = K(X^T, X)\)</span> is PSD.</p>
<p><span class="math inline">\(l_1\)</span> norm: <span class="math inline">\(\sum_{i}{|x_i|}\)</span></p>
<p><span class="math inline">\(l_{\infty}\)</span> norm: <span class="math inline">\(\max_i{x_i}\)</span>. This is because other dimensions diminishes.</p>
<ul>
<li>General form of <span class="math inline">\(l_p\)</span> norm</li>
</ul>
<p><span class="math display">\[||x||_p = \bigg(\sum_{i=1}^{n}{|x_i|^p}\bigg)^{\frac{1}{p}}\]</span></p>
<ul>
<li>Norm definition for Matrices: Frobenius norm</li>
</ul>
<p><span class="math display">\[||A||_F = \sqrt{\sum_{i,j}{A_{ij}^2} } = \sqrt{\textrm{tr}(A^TA)}\]</span></p>
<h2 id="linear-independence-and-rank">Linear independence and rank</h2>
<p>Linearly dependent:</p>
<p><span class="math display">\[ x_n = \sum_{i=1}^{n-1}{\alpha_ix_i}\]</span></p>
<p>Column rank: # of linearly independent columns of A</p>
<p>Row rank is similarly defined and equals to column rank. <a href="https://ocw.mit.edu/courses/mathematics/18-701-algebra-i-fall-2010/study-materials/MIT18_701F10_rrk_crk.pdf">Proof</a></p>
<p>Both called rank.</p>
<ul>
<li>Full rank if the <span class="math inline">\(=\min(m,n)\)</span></li>
<li>rank(<span class="math inline">\(A\)</span>) = rank(<span class="math inline">\(A^T\)</span>)</li>
<li>rank(<span class="math inline">\(AB\)</span>) <span class="math inline">\(\leq\)</span> min(rank(<span class="math inline">\(A\)</span>), rank(<span class="math inline">\(B\)</span>))</li>
<li>rank(<span class="math inline">\(A+B\)</span>) <span class="math inline">\(\leq\)</span> rank(<span class="math inline">\(A\)</span>) + rank(<span class="math inline">\(B\)</span>)</li>
</ul>
<p>Comment: <a href="https://yutsumura.com/the-rank-of-the-sum-of-two-matrices/">Proof</a> <span class="math display">\[rank(A) = dim(span(a_1, ..., a_n))\]</span> Use this to easily get the 4th property.</p>
<h2 id="orthogonal-matrices">Orthogonal Matrices</h2>
<p><span class="math display">\[U^TU = I = UU^T\]</span></p>
<p>Comment: This generally requires that, all the column vectors of <span class="math inline">\(U\)</span> has norm <span class="math inline">\(1\)</span> and orthogonal to any other columns. This property requires that <span class="math inline">\(n\leq m\)</span> because the dimension of the column space cannot surpass its dimensionality <span class="math inline">\(m\)</span>. And vice versa. Therefore only square matrices can satisfy both.</p>
<p>Property: Norm immutability</p>
<p><span class="math display">\[||Ux||_2 = ||x||_2\]</span></p>
<p>Comment: Expand by definition and find out that the cross quadratic terms equals 0.</p>
<h2 id="range-and-nullspace-of-a-matrix">Range and Nullspace of a Matrix</h2>
<p><strong>Span(线性生成空间)</strong>: all linear combinations of columns</p>
<p><strong>Projection</strong> of <span class="math inline">\(y\)</span> onto the span of <span class="math inline">\(\{x_1, ..., x_n\}\)</span>: minimizes the euclidean norm of the residual</p>
<p><span class="math display">\[Proj(y; {x_1,...,x_n}) = argmin_{v\in span(X)}||y-v||_2\]</span></p>
<p><strong>Range/Columnspace</strong>: span of columns of matrix</p>
<p><span class="math display">\[R(A) = \{ v \in \mathbb{R}^m: v = Ax, x\in \mathbb{R}^n \}\]</span></p>
<p>Projection of <span class="math inline">\(y\in \mathbb{R}^m\)</span> on to the range of <span class="math inline">\(A\)</span> is given by <span class="math display">\[Proj(y; A) = argmin_{v\in R(A)}||v-y||_2 = A(A^TA)^{-1}A^Ty\]</span></p>
<p>Proof: <a href="https://hy2632.github.io/2020/10/01/Data-Mining-%E6%96%BD%E5%AF%86%E7%89%B9%E6%AD%A3%E4%BA%A4%E5%8C%96/">最小二乘法与投影</a></p>
<p>When A is a column vector, the expression regenerates to <span class="math inline">\(\frac{aa^T}{a^Ta}y\)</span></p>
<p><strong>Nullspace</strong>:</p>
<p><span class="math display">\[N(A) = \{x\in\mathbb{R}^n: Ax=0\}\]</span></p>
<p>Comment: <span class="math inline">\(x\)</span> is a coordinate and <span class="math inline">\(A\)</span> is a set of column vectors. Certain <span class="math inline">\(x\)</span> could map to a zero vector.</p>
<p><strong>Orthogonal complements(正交补)</strong>:</p>
<p>Note that <span class="math inline">\(R(A)\subseteq \mathbb{R}^n\)</span> while <span class="math inline">\(N(A) \subseteq \mathbb{R}^m\)</span>. This is because, range is linear combination of <span class="math inline">\(n\)</span> <span class="math inline">\(\mathbb{R}^m\)</span> column vectors, while <span class="math inline">\(nullspace\)</span> consists of the coordinates <span class="math inline">\(x \in \mathbb{R}^n\)</span>.</p>
<p><span class="math inline">\(R(A^T)\)</span> and <span class="math inline">\(N(A)\)</span> are both in <span class="math inline">\(\mathbb{R}^n\)</span>. It can be proved that these two are disjoint subsets that together span the entire <span class="math inline">\(\mathbb{R}^n\)</span> space.</p>
<p><span class="math display">\[\{ w:w=u+v, u\in R(A^T), v\in N(A)\} = \mathbb{R}^n, \quad R(A^T)\cap N(A) = \{0\} \]</span></p>
<p>Comment:</p>
<p><span class="math display">\[w = u+v = A^Tx +y, \quad Ay = 0, \quad x \in \mathbb{R}^m, y,w \in \mathbb{R}^n \]</span></p>
<p><span class="math display">\[Aw = AA^Tx\]</span></p>
<p><span class="math display">\[rank(AA^T) = rank(A)\]</span></p>
<h2 id="determinantain-mathbbrntimes-n">Determinant(<span class="math inline">\(A\in \mathbb{R}^{n\times n}\)</span>)</h2>
<p>Set of points <span class="math inline">\(S \subset \mathbb{R}^n\)</span>: <span class="math display">\[S = \{ v\in \mathbb{R}^n: v = \sum_{i=1}^{n}{\alpha_ia_i}, \quad 0\leq \alpha_i \leq 1 \}\]</span></p>
<p>S is a restriction of span. |A| measures the <strong>"volume"</strong> of the set S.</p>
<p>Consider <span class="math display">\[\begin{bmatrix} a_1^T \\ a_2^T \end{bmatrix} = \begin{bmatrix} 1 \quad 3 \\ 3 \quad 2 \end{bmatrix}\]</span></p>
<p>The area of the parrelogram: <span class="math display">\[\sqrt{||a_1||^2||a_2||^2 - ||a_1\cdot a_2||^2}\]</span></p>
<ol type="1">
<li>The determinant of the identity is 1, |I| = 1. (Geometrically, the volume of a unit hypercube is 1).</li>
<li>Given a matrix <span class="math inline">\(A ∈ R^{n×n}\)</span>, if we multiply a single row in <span class="math inline">\(A\)</span> by a scalar <span class="math inline">\(t ∈ R\)</span>, then the determinant of the new matrix is <span class="math inline">\(t|A|\)</span>(Geometrically, multiplying one of the sides of the set <span class="math inline">\(S\)</span> by a factor <span class="math inline">\(t\)</span> causes the volume to increase by a factor <span class="math inline">\(t\)</span>.)</li>
<li>If we exchange any two rows of <span class="math inline">\(A\)</span>, then the determinant of the new matrix is <span class="math inline">\(−|A|\)</span></li>
</ol>
<p>For <span class="math inline">\(A,B\in \mathbb{R}^{n\times n}\)</span>, - <span class="math inline">\(|A| = |A^T|\)</span> - <span class="math inline">\(|AB| = |A||B|\)</span> - <span class="math inline">\(|A|=0\)</span> iff <span class="math inline">\(A\)</span> is singular(non-invertible). - For non-singular <span class="math inline">\(|A|^{-1} = 1/|A|\)</span></p>
<p><strong>Recursive formula for determinant</strong> <a href="https://en.wikipedia.org/wiki/Laplace_expansion">Laplace expansion</a> <strong>Complete expansion has <span class="math inline">\(n!\)</span> terms!</strong></p>
<p><span class="math display">\[|A| = \sum_{i=1}^{n}{a_{ij}|A_{\backslash i,\backslash j}|}, \forall j\]</span></p>
<p><strong>(Classical) adjoint</strong></p>
<p><span class="math display">\[(adj(A))_{ij} = (-1)^{i+j}|A_{\backslash j,\backslash i}|\]</span></p>
<p><span class="math display">\[A^{-1} = \frac{1}{|A|}adj(A)\]</span></p>
<h2 id="quadratic-forms-and-psd-matrices">Quadratic Forms and PSD matrices</h2>
<p><span class="math display">\[x^TAx = \sum_{i=1}^{n}\sum_{j=1}^{n}{A_{ij}x_ix_j}\]</span></p>
<p>Note that only the symmetric part contributes so generally we assume that <span class="math inline">\(A\)</span> is symmetric.</p>
<p><span class="math display">\[x^TAx = (x^TAx)^T = x^TA^Tx = x^T(\frac12A + \frac12A^T)x\]</span></p>
<p><strong>Property</strong>: PD/ND matrices are full rank and invertible.</p>
<p>Proof: if not, <span class="math inline">\(\exist x \neq 0, Ax=0\)</span>, then <span class="math inline">\(x^TAx=0\)</span></p>
<p><strong>Gram matrix</strong>: one type of PD matrix.</p>
<p><span class="math display">\[G = A^TA\]</span></p>
<p><span class="math inline">\(G\)</span> is PSD, and if <span class="math inline">\(m\geq n\)</span> and full rank, G is PD</p>
<h2 id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</h2>
<p><span class="math display">\[Ax = \lambda x, x\neq 0\]</span></p>
<p>if <span class="math inline">\(x\)</span> is a eigenvector to eigenvalue <span class="math inline">\(\lambda\)</span>, then <span class="math inline">\(cx\)</span> is also a eigenvector. So we can normalize the eigenvectors.</p>
<p><span class="math display">\[(\lambda I - A)x = 0, x\neq 0\]</span> This has non-zero solution iff <span class="math inline">\((\lambda I - A)\)</span> has a non-empty nullspace. <span class="math display">\[|\lambda I - A| = 0\]</span></p>
<p><strong>Properties of eigenvalues/eigenvectors</strong></p>
<ul>
<li>trace equals to sum of eigenvalues</li>
<li>determinant equals to product of eigenvalues</li>
<li>rank equals to # of non-zero eigenvalues</li>
<li>for one pair, <span class="math inline">\(A^{-1}x = (1/\lambda)x\)</span></li>
<li>diagonal matrix: diagonal are eigen values</li>
</ul>
<h2 id="eigenvalues-and-eigenvectors-for-symmetric-matrices">Eigenvalues and Eigenvectors for symmetric matrices</h2>
<p>If <span class="math inline">\(A\)</span> is symmetric, 1. All eigenvalues are real 2. exist unit and orthogonal eigenvectors</p>
<p>Let <span class="math display">\[U = [u_1, ..., u_n]\]</span></p>
<p><span class="math inline">\(U\)</span> is orthogonal so <span class="math inline">\(UU^T=I\)</span>.</p>
<p><span class="math display">\[\Lambda = diag(\lambda_1, ..., \lambda_n)\]</span></p>
<p><span class="math display">\[AU = [Au_1, ... Au_n] = [\lambda_1u_1, ... \lambda_nu_n] = U\Lambda\]</span></p>
<p><span class="math display">\[A = AUU^T = U\Lambda U^T\]</span></p>
<p>This is just the diagonalization of the symmetric matrix A.</p>
<p>Comment: When PD a.k.a all eigenvalues positive, <span class="math inline">\(A = (U\Lambda^{\frac12})(U\Lambda^{\frac12})^T\)</span></p>
<p><strong>Representing vector w.r.t. another basis</strong> Switching coordinate system to orghogonal</p>
<p><span class="math display">\[x = U\hat{x}\]</span></p>
<p><span class="math display">\[\hat{x} = U^Tx\]</span></p>
<p>This indicates <span class="math inline">\(U^Tx\)</span> is another representation of x.</p>
<p><strong>Diagonalizing matrix-vector multiplication</strong></p>
<p><span class="math display">\[\hat{z} = U^Tz = U^TAx = U^TU\Lambda U^Tx = \Lambda \hat{x}\]</span></p>
<p>Then matrix multiplication is simpler because it merely scales each coordinate by the corresponding eigenvalue.</p>
<p>Especially when you left multiply one <span class="math inline">\(A^n\)</span></p>
<p><span class="math display">\[\hat{q} = U^Tq = U^TAAAx = U^TU\Lambda U^TU\Lambda U^TU\Lambda U^Tx = \Lambda^3\hat{x} \]</span></p>
<p>Comment: Time complexity of diagonalization is <span class="math inline">\(O(N^3)\)</span></p>
<p><strong>Diagonalizing quadratic form</strong></p>
<p><span class="math display">\[x^TAx = x^TU\Lambda U^Tx = \hat{x}^T \Lambda \hat{x} = \sum_{i=1}^{n}{\lambda_i \hat{x}_i^2}\]</span></p>
<p>Then the dedfiniteness of A is decided by the sign of its eigenvalues.</p>
<p>W.r.t <strong>maximizing the quadratic form</strong>, <span class="math inline">\(s.t. ||\hat{x}||_2^2 = 1\)</span></p>
<p>Upper bound is all weights on the largest eigenvalue <span class="math inline">\(\lambda_1\)</span>. Then <span class="math inline">\(x=u_1\)</span>.</p>
<h1 id="matrix-calculus">Matrix Calculus</h1>
<h2 id="the-gradient">The Gradient</h2>
<p>matrix of partial derivatives,</p>
<p><span class="math display">\[(\nabla_{A}f(A))_{ij} = \frac{\partial f(A)}{\partial A_{ij}}\]</span></p>
<p>Keep the notation clear by declaring the differentiated variable (<span class="math inline">\(z\)</span> for the whole chunk)</p>
<h2 id="the-hessian">The Hessian</h2>
<p><span class="math display">\[(\nabla_x^2f(x))_{ij} = \frac{\partial^2f(x)}{\partial x_i\partial x_j} \]</span></p>
<p>Symmetric (<span class="math inline">\(\mathbb{R}^{n\times n}\)</span>, real valued)</p>
<p>Caveat:</p>
<p>We cannot see the Hessian as the gradient of the gradient, because this is <strong>undefined</strong>:</p>
<p><span class="math display">\[\nabla_x\nabla_xf(x) = \nabla_x
\begin{bmatrix}
\frac{\partial f(x)}{\partial x_1}\\
...\\
\frac{\partial f(x)}{\partial x_n}
\end{bmatrix}\]</span></p>
<p>However, the column of the Hessian can be defined as</p>
<p><span class="math display">\[\nabla_x\nabla \frac{\partial f(x)}{\partial x_i} = 
\begin{bmatrix}
\frac{\partial^2 f(x)}{\partial x_i \partial x_1}\\
...\\
\frac{\partial^2 f(x)}{\partial x_i \partial x_n}
\end{bmatrix}\]</span></p>
<p>Therefore, Hessian can be defined by (fix one and derive another)</p>
<p><span class="math display">\[\nabla_x^2f(x) = [\nabla_x(\nabla_xf(x))_1, ...\nabla_x(\nabla_xf(x))_n]\]</span></p>
<h2 id="gradients-and-hessians-of-quadratic-and-linear-forms">Gradients and Hessians of Quadratic and Linear Forms</h2>
<p>Analogous to scalars. Note that the multiplier is <strong>transposed</strong>.</p>
<p><span class="math display">\[\nabla_x b^Tx = b\]</span></p>
<h2 id="least-squares">Least Squares</h2>
<p>Before we derived the expression <a href="https://hy2632.github.io/2020/10/01/Data-Mining-%E6%96%BD%E5%AF%86%E7%89%B9%E6%AD%A3%E4%BA%A4%E5%8C%96/">最小二乘法与投影</a></p>
<p>We want to minimize the residual when doing projection. This time we use derivative.</p>
<p><span class="math display">\[||Ax-b||_2^2 = (Ax-b)^T(Ax-b) = x^TA^TAx-2b^TAx+b^2\]</span></p>
<p><span class="math display">\[\nabla_x(||Ax-b||_2^2) = 2A^TAx-2A^Tb\]</span></p>
<p><span class="math display">\[x = (A^TA)^{-1}A^Tb\]</span></p>
<h2 id="gradients-of-the-determinent">Gradients of the Determinent</h2>
<p>For symmetric <span class="math inline">\(A\)</span>,</p>
<p><span class="math display">\[\nabla_A log|A| = \frac{1}{|A|}\nabla_A|A| = A^{-1}\]</span></p>
<p>The proof uses the adjoint(伴随矩阵) and cofactor(代数余子式).</p>
<h2 id="eigenvalus-as-optimization">Eigenvalus as optimization</h2>
<p>Before in the eigenvalue and eigenvector part we discussed maximizing a quadratic form with restricted norm.</p>
<p><span class="math display">\[\max_{x\in\mathbb{R}^n} x^TAx, s.t. ||x||_2^2=1\]</span></p>
<p>From the angle of Lagrangian multiplier</p>
<p><span class="math display">\[L(x, \lambda) = x^TAx - \lambda(x^Tx-1)\]</span></p>
<p><span class="math display">\[\nabla_x L(x,\lambda) = \nabla_x(x^TAx - \lambda x^Tx) = 2Ax - 2\lambda x = 0\]</span></p>
<p><span class="math display">\[Ax = \lambda x\]</span></p>
<p>Lagrangian multiplier method ensures that the optimal solution of the original is within the solution of the Lagrangian function.<a href="https://zh.wikipedia.org/wiki/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0">拉格朗日乘数法所得的极点会包含原问题的所有极值点，但并不保证每个极值点都是原问题的极值点。</a></p>
<p>This indicates that the optimal solution <span class="math inline">\(x\)</span> of this problem must be one eigenvector of <span class="math inline">\(A\)</span>.</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224NA5</title>
    <url>/2020/08/20/CS224NA5/</url>
    <content><![CDATA[<h1 id="cs224n-assignment-5">CS224N Assignment #5</h1>
<p>(7.20)增加 Pylance 插件作为语言服务器. 打开 type checking mode(basic).</p>
<h2 id="文字题">文字题</h2>
<ul>
<li><ol type="a">
<li>We learned in class that recurrent neural architectures can operate over variable length input (i.e., the shape of the model parameters is independent of the length of the input sentence). Is the same true of convolutional architectures? Write one sentence to explain why or why not.</li>
</ol>
<p>window t ∈ {1, . . . , mword − k + 1}， mword 即最长单词的长度可变， xconv ∈ R^(eword×(mword−k+1))</p></li>
<li><p>(b)...if we use the kernel size k = 5, what will be the size of the padding (i.e. the additional number of zeros on each side) we need for the 1-dimensional convolution, such that there exists at least one window for all possible values of mword in our dataset?</p>
<p>极端情况 mword=1， 前后各 1 个 token，还需 padding=1.</p></li>
<li><ol start="3" type="a">
<li>In step 4, we introduce a Highway Network with <code>xhighway = xgate xproj + (1 − xgate) xconv out</code>. Since xgate is the result of the sigmoid function, it has the range (0, 1).Consider the two extreme cases. If xgate → 0, then xhighway → xconv out. When xgate → 1, then xhighway → xproj. This means the Highway layer is smoothly varying its behavior between that of normal linear layer (xproj) and that of a layer which simply passes its inputs (xconv out) through. Use one or two sentences to explain why this behavior is useful in character embeddings. Based on the definition of <code>xgate = σ(Wgatexconv out + bgate)</code>, do you think it is better to initialize bgate to be negative or positive? Explain your reason briefly. 原因： 所谓的 highway， x_gate=0 可以直接用 x_convout 的值。</li>
</ol>
<p>希望默认 x_gate 较小方便 highway，所以 b 取负。</p></li>
<li><ol start="4" type="a">
<li>In Lecture 10, we briefly introduced Transformers, a non-recurrent sequence (or sequence-to-sequence) model with a sequence of attention-based transformer blocks. Describe 2 advantages of a Transformer encoder over the LSTM-with-attention encoder in our NMT model</li>
</ol>
<p>可以看一下 &lt;<Attention is all you need>&gt;： "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence." 每一步都是句子里的所有单词之间建立联系。 主要用到三个矩阵 Key, Query, value, <code>Attention(Q,K,V) = softmax(QK.T/\sqrt(d_k))V</code> (包学包会，这些动图和代码让你一次读懂「自注意力」 - 机器之心的文章 - 知乎 https://zhuanlan.zhihu.com/p/96492170)</p>
<p>attention-based transformers的好处（P6 的 Part 4， Why self-attention）：</p>
<p>未采用RNN就可以避免梯度消失和梯度爆炸等问题, 从sequential computation 到实现parallelized computation, 更易学习到"long-range dependencies in the network", 更加interpretable.</p></li>
</ul>
<h1 id="character-based-convolutional-encoder-for-nmt-36-points">1. Character-based convolutional encoder for NMT (36 points)</h1>
<h2 id="vocab.py">Vocab.py</h2>
<ol type="1">
<li>这种写法很巧妙·</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(self.char_list):</span><br><span class="line">    self.char2id[c] = len(self.char2id)</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li><p>组合用法，类似 zip+enumerate <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> chain</span><br><span class="line">word_freq = Counter(chain(\*corpus))</span><br></pre></td></tr></table></figure></p></li>
<li><p>用字典辅助排序 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">valid_words = [w <span class="keyword">for</span> w, v <span class="keyword">in</span> word_freq.items() <span class="keyword">if</span> v &gt;= freq_cutoff]</span><br><span class="line">top_k_words = sorted(valid_words, key=<span class="keyword">lambda</span> w: word_freq[w], reverse=<span class="literal">True</span>[:size]</span><br></pre></td></tr></table></figure></p></li>
</ol>
<p>用到了<code>json.dump</code>，Vocab 也用此形式存储。</p>
<h2 id="e-implement-to_input_tensor_char-in-vocab.py">(e) Implement <code>to_input_tensor_char()</code> in <code>vocab.py</code></h2>
<p>字母 ∏ Û python 执行有问题，改成<code>&lt;pad&gt;</code>, <code>&lt;unk&gt;</code>。</p>
<h2 id="f-highway">(f) highway</h2>
<p>要求写一个 sanity_check, (f)本身实现很简单，只是一步处理，所以检查一下前后维度就可以。</p>
<h2 id="g-cnn.py-cnn">(g) cnn.py, CNN</h2>
<p>输入(sentence_length, batch_size, e_char, m_word)，前两维不动，对每个词 conv 完，后两维应该是 f 和窗口数，再经过 maxpool 所有窗口， 输出是(sentence_length, batch_size, f) torch 需要使用.contiguous().view(),因为 view 只能作用在 contiguous 的变量上 比较关键的一步。 <strong>07/25 更新</strong>： 果然后面还是出问题了。m_word 是 forward 函数中参数 x_reshaped 的维度属性，如果使用 max_pool layer，一开始并不知道输入的参数 m_word 是多少。所以不应该用 maxpool 层（因为不能对一个多维 tensor 的某一维更新）， 而应该在 forward 函数里直接调用 torch.max(dim=2)</p>
<h2 id="h-model_embeddings.">(h) Model_Embeddings.</h2>
<p>一个问题是 f=e_word, e_word 和 e_char 的关系到底如何？？ 题目假设 e_char=50, e_word 是初始化 model_embeddings 的参数 word_embedding_size, 默认值 21。</p>
<h2 id="j">(j)</h2>
<p>wdnmd， vocab.py 里的 sents_var 总是空的，查了半天发现 utils.pad_sents 忘了粘贴。 nmt_model.py 中 step()函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> enc*masks <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    e_t.data.masked_fill*(enc_masks.bool(), -float(<span class="string">&#x27;inf&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>显示'Tensor' object has no attribute 'bool' 原因应该是 torch 版本较低 事实也确实如此，local*env.yml 显示 pytorch=1.0.0，a4 作业就没有限定版本，估计是助教忘了更新。 解决方案：改成 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">e_t.data.masked_fill*(enc_masks==<span class="number">1</span>, -float(<span class="string">&#x27;inf&#x27;</span>))</span><br></pre></td></tr></table></figure></p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">epoch</span> <span class="number">100</span>, iter <span class="number">500</span>, cum. loss <span class="number">0</span>.<span class="number">30</span>, cum. ppl <span class="number">1</span>.<span class="number">01</span> cum. examples <span class="number">200</span></span><br><span class="line"><span class="attribute">validation</span>: iter <span class="number">500</span>, dev. ppl <span class="number">1</span>.<span class="number">001988</span></span><br><span class="line"><span class="attribute">Corpus</span> BLEU: <span class="number">99</span>.<span class="number">66941696422141</span></span><br></pre></td></tr></table></figure>
<p>达到题设要求。</p>
<h1 id="character-based-lstm-decoder-for-nmt-26-points">2. Character-based LSTM decoder for NMT (26 points)</h1>
<h2 id="b">(b)</h2>
<p>奇怪的点在于, char_decoder.py 中 train_forward 的 loss 计算，不 softmaxloss 才收敛。 题目要求仔细阅读 nn.CrossEntropyLoss，实际上 Pytorch 中 CrossEntropyLoss()函数将 softmax-log-NLLLoss 合并到一块。</p>
<p><code>This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.</code> loss 0.38, Corpus BLEU: 99.66941696422141</p>
<h2 id="c">(c)</h2>
<p>这部分思路很清晰，用到了一些技巧，比如(tensor,tensor)的 elementwise 的提取，char 拼接成 word 等，详见代码</p>
<h2 id="e">(e)</h2>
<p>在 VM 上训练。 注意 run.sh 可以进行修改，使得 train_local 也可使用 cuda，提高效率。 仍然遇到了环境问题。 “RuntimeError: Given input size: (256x1x12). Calculated output size: (256x1x0). Output size is too small” 于是只能在 VM 上配一个和本地相同的（过时的）环境。问题解决。 <strong>CNN.py 中存在问题，很久之前埋下的坑！！！</strong>：初始化时如果建立 maxpool 就需要提前知道 m_word 以确定 kernel_size。这个问题可以这样解决：避免 maxpool 层，在 forward 中使用 torch.max 函数，对某个维度进行 max。 对 cnn 和 sanity_check 都进行修改。由于默认使用了 sanitycheck 的值 m_word=21,实际上在写其他函数调用 CNN 类的时候没有定义 m_word 值，所以正好不需要改。 参考：Tessa Scott<a href="https://github.com/tessascott039/a5/blob/master/cnn.py" class="uri">https://github.com/tessascott039/a5/blob/master/cnn.py</a></p>
<p><a href="https://github.com/pytorch/pytorch/issues/4166" class="uri">https://github.com/pytorch/pytorch/issues/4166</a> <a href="https://stackoverflow.com/questions/56137869/is-it-possible-to-make-a-max-pooling-on-dynamic-length-sentences-without-padding" class="uri">https://stackoverflow.com/questions/56137869/is-it-possible-to-make-a-max-pooling-on-dynamic-length-sentences-without-padding</a> 探讨了 nn.MaxPool1d 能不能有一个动态的 kernel_size。</p>
<p>train 的结果： <figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">epoch</span> <span class="number">29</span>, iter <span class="number">196300</span>, avg. loss <span class="number">81</span>.<span class="number">60</span>, avg. ppl <span class="number">59</span>.<span class="number">75</span> cum. examples <span class="number">9600</span>, speed <span class="number">6086</span>.<span class="number">86</span> words/sec, time elapsed <span class="number">20580</span>.<span class="number">30</span> sec</span><br><span class="line"><span class="attribute">epoch</span> <span class="number">29</span>, iter <span class="number">196310</span>, avg. loss <span class="number">81</span>.<span class="number">10</span>, avg. ppl <span class="number">50</span>.<span class="number">14</span> cum. examples <span class="number">9920</span>, speed <span class="number">6458</span>.<span class="number">96</span> words/sec, time elapsed <span class="number">20581</span>.<span class="number">33</span> sec</span><br><span class="line"><span class="attribute">epoch</span> <span class="number">29</span>, iter <span class="number">196320</span>, avg. loss <span class="number">78</span>.<span class="number">58</span>, avg. ppl <span class="number">48</span>.<span class="number">75</span> cum. examples <span class="number">10240</span>, speed <span class="number">6548</span>.<span class="number">57</span> words/sec, time elapsed <span class="number">20582</span>.<span class="number">32</span> sec</span><br><span class="line"><span class="attribute">epoch</span> <span class="number">29</span>, iter <span class="number">196330</span>, avg. loss <span class="number">86</span>.<span class="number">52</span>, avg. ppl <span class="number">61</span>.<span class="number">24</span> cum. examples <span class="number">10537</span>, speed <span class="number">6019</span>.<span class="number">06</span> words/sec, time elapsed <span class="number">20583</span>.<span class="number">36</span> sec</span><br><span class="line"><span class="attribute">test</span>：Corpus BLEU: <span class="number">36</span>.<span class="number">395796664198</span></span><br></pre></td></tr></table></figure></p>
<h1 id="analyzing-nmt-systems-8-points">3. Analyzing NMT Systems (8 points)</h1>
<h2 id="a">(a)</h2>
<p>用 linux 的 grep 命令查找字符串 <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">(base)</span> <span class="string">hy2632_ubuntu20@DESKTOP-8LEIHPS:~/cs224n/a5_public\$</span> <span class="string">grep</span> <span class="string">tradu</span> <span class="string">vocab.json</span></span><br><span class="line"><span class="attr">&quot;traduciendo&quot;:</span> <span class="number">17349</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;tradujera&quot;:</span> <span class="number">32719</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;traduccin&quot;:</span> <span class="number">4562</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;traduzco&quot;:</span> <span class="number">40154</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;traduzcan&quot;:</span> <span class="number">23440</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;traductores&quot;:</span> <span class="number">19447</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;traducir&quot;:</span> <span class="number">4565</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;traducciones&quot;:</span> <span class="number">12054</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;traductor&quot;:</span> <span class="number">11809</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;traducirse&quot;:</span> <span class="number">36917</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;traducen&quot;:</span> <span class="number">19640</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;tradujo&quot;:</span> <span class="number">25176</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;traducido&quot;:</span> <span class="number">8515</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;traducimos&quot;:</span> <span class="number">18251</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;traduce&quot;:</span> <span class="number">7821</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;traducidas&quot;:</span> <span class="number">20336</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;traduzca&quot;:</span> <span class="number">44710</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;traducirlo&quot;:</span> <span class="number">19205</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;traductora&quot;:</span> <span class="number">13071</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;traduje&quot;:</span> <span class="number">23103</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;traducirlas&quot;:</span> <span class="number">35543</span><span class="string">,</span></span><br><span class="line"><span class="attr">&quot;traducida&quot;:</span> <span class="number">19350</span><span class="string">,</span></span><br></pre></td></tr></table></figure></p>
<p>traduces, traduzcas not in. 如果是 word-based NMT，将 spanish 翻译为 english， 如果句子中出现 traduces 就会判断为<code>&lt;unk&gt;</code>， 无法翻译；但如果是 character-aware NMT，别的类似 traducir(to translate)的动词可能有类似的性质(加 s，从 I 变成 you)，同时又恰好出现在训练集中，那么模型在遇到 traduces 的 s 时就能翻译出 you translate。</p>
<h2 id="b-1">(b)</h2>
<ol type="1">
<li><p>回顾 Word2Vec。<a href="https://projector.tensorflow.org/" class="uri">https://projector.tensorflow.org/</a> 可以查询 k-nearest words。</p>
<p>Markdown 表格生成<a href="https://www.tablesgenerator.com/markdown_tables" class="uri">https://www.tablesgenerator.com/markdown_tables</a></p></li>
</ol>
<table>
<thead>
<tr class="header">
<th>word</th>
<th>closest word</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>financial</td>
<td>economics</td>
</tr>
<tr class="even">
<td>neuron</td>
<td>nerve</td>
</tr>
<tr class="odd">
<td>Francisco</td>
<td>san</td>
</tr>
<tr class="even">
<td>naturally</td>
<td>occurring</td>
</tr>
<tr class="odd">
<td>expectation</td>
<td>norms</td>
</tr>
</tbody>
</table>
<ol start="2" type="1">
<li>也可以上传自己的 character-aware NMT model 的 embeddings 查找 nearest neighbors.</li>
</ol>
<figure>
<img src="https://raw.githubusercontent.com/hy2632/cs224n/master/a5_public/CS224n-A5-emb2-files/embedding_proj.jpg" alt="Image" /><figcaption aria-hidden="true">Image</figcaption>
</figure>
<table>
<thead>
<tr class="header">
<th>word</th>
<th>closest word</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>financial</td>
<td>vertical</td>
</tr>
<tr class="even">
<td>neuron</td>
<td>Newton</td>
</tr>
<tr class="odd">
<td>Francisco</td>
<td>France</td>
</tr>
<tr class="even">
<td>naturally</td>
<td>practically</td>
</tr>
<tr class="odd">
<td>expectation</td>
<td>exception</td>
</tr>
</tbody>
</table>
<ol start="3" type="1">
<li>分析 Word2Vec 和 CharCNN 的区别并解释。</li>
</ol>
<p>除了 naturally/practically 意思相近，CharCNN 的 embeddings 更多的还是按照字母组成（同时也包含一些 pos 和 ner）。Word2Vec 则更多地把握了词义的相似和关联性。原因就是模型本身。</p>
<p>word2vec: skip-grams &amp; CBOW(contiguous bag of words)，给定上下文此预测缺失的中心词 c，概率分布 P(C|W)。因而相近的词都是有较大概率作为中心词被代替。</p>
<p>CharCNN: 对某个单词的各个字母进行 charembedding，然后经过 CNN/Highway 等操作最后生成 wordembedding，较大程度上依赖于 charembedding，如果两个词有相同字母的 subset 则 wordembedding 可能相近。</p>
<h2 id="c-1">(c)</h2>
<p>(45)正确的例子： | Category | | | -------------- | -------------------------------------------------------------------------- | | ES | A medida que se derrite un tmpano, estoy respirando su atmsfera ancestral. | | Ref | As an eardrum melts, I am breathing in its ancient atmosphere. | | A4 translation | As a <code>&lt;unk&gt;</code> <code>&lt;unk&gt;</code> I'm breathing its atmosphere <code>&lt;unk&gt;</code> | | A5 translation | As it melts a iceberg, I'm breathing its ancestral atmosphere. |</p>
<p>tímpano，témpano 是同义词，但一个翻译为耳膜一个翻译为冰山。这里冰山显然更为合适。</p>
<p>(85)错误的例子： | Category | | | -------------- | ------------------------------------------------- | | ES | Es el sndrome de insensibilidad a los andrgenos. | | Ref | It is the syndrome of insensitivity to androgens. | | A4 translation | It's called <code>&lt;unk&gt;</code> <code>&lt;unk&gt;</code> | | A5 translation | It's the syndrome of insulin insulin. |</p>
<p>对于连续的<code>&lt;unk&gt;</code>，CharCNN 的表现并没有很好改善（重复出现的 insulin）。</p>
]]></content>
      <categories>
        <category>CS224N作业笔记</category>
      </categories>
      <tags>
        <tag>CS224N</tag>
      </tags>
  </entry>
  <entry>
    <title>CS229 ICA-Implementation</title>
    <url>/2020/11/01/CS229-ICA-Implementation/</url>
    <content><![CDATA[<p><strong><a href="https://towardsdatascience.com/separating-mixed-signals-with-independent-component-analysis-38205188f2f4">Link: Separating mixed signals with ICA</a></strong></p>
<h1 id="implementation-of-independent-components-analysis">Implementation of Independent Components Analysis</h1>
<h2 id="import-packages">Import packages</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> signal</span><br><span class="line"><span class="keyword">from</span> tqdm.notebook <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure>
<h2 id="generate-the-source-s">Generate the source <span class="math inline">\(S\)</span></h2>
<p><span class="math inline">\(S\in R^d\)</span></p>
<p>Here we choose 3 sources of signals: sine, sawtooth and noise.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">f = plt.figure(figsize = (<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line">ax = f.add_subplot(<span class="number">311</span>)</span><br><span class="line">ax2 = f.add_subplot(<span class="number">312</span>)</span><br><span class="line">ax3 = f.add_subplot(<span class="number">313</span>)</span><br><span class="line">t = np.linspace(<span class="number">0</span>, <span class="number">200</span> , <span class="number">1000</span>)</span><br><span class="line">sine = np.sin(t)</span><br><span class="line">sawtooth = signal.sawtooth(t*<span class="number">1.9</span>)</span><br><span class="line">noise = np.random.random(len(t))</span><br><span class="line"></span><br><span class="line">ax.plot(t, sine)</span><br><span class="line">ax2.plot(t, sawtooth, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">ax3.plot(t, noise, <span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/11/01/CS229-ICA-Implementation/ICA_5_0.png" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">S = np.vstack((sine, sawtooth, noise))</span><br><span class="line">S.shape</span><br></pre></td></tr></table></figure>
<pre><code>(3, 1000)</code></pre>
<h2 id="mixture-matrix-a-x-as">Mixture Matrix A, <span class="math inline">\(x = As\)</span></h2>
<p><span class="math inline">\(s_j^{(i)} = w_j^Tx^{(i)}\)</span></p>
<p><span class="math inline">\(x^{(i)} = As^{(i)}\)</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = np.array([</span><br><span class="line">  [<span class="number">0.5</span>,<span class="number">1</span>,<span class="number">0.2</span>],</span><br><span class="line">  [<span class="number">1</span>,<span class="number">0.5</span>,<span class="number">0.4</span>],</span><br><span class="line">  [<span class="number">0.5</span>,<span class="number">0.8</span>,<span class="number">1</span>]</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W = np.linalg.inv(A)</span><br><span class="line">W</span><br></pre></td></tr></table></figure>
<pre><code>array([[-0.3       ,  1.4       , -0.5       ],
       [ 1.33333333, -0.66666667,  0.        ],
       [-0.91666667, -0.16666667,  1.25      ]])</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = np.dot(A.T, S)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">3</span>))</span><br><span class="line">plt.plot(t[:<span class="number">100</span>], sine[:<span class="number">100</span>])</span><br><span class="line">plt.plot(t[:<span class="number">100</span>], sawtooth[:<span class="number">100</span>])</span><br><span class="line">plt.plot(t[:<span class="number">100</span>], noise[:<span class="number">100</span>])</span><br><span class="line">plt.legend(labels = [<span class="string">&quot;sine&quot;</span>, <span class="string">&quot;sawtooth&quot;</span>, <span class="string">&quot;noise&quot;</span>])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/11/01/CS229-ICA-Implementation/ICA_12_0.png" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">f = plt.figure(figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line">f.suptitle(<span class="string">&quot;Mixed Signals&quot;</span>)</span><br><span class="line">ax1 = f.add_subplot(<span class="number">311</span>)</span><br><span class="line">ax2 = f.add_subplot(<span class="number">312</span>)</span><br><span class="line">ax3 = f.add_subplot(<span class="number">313</span>)</span><br><span class="line"></span><br><span class="line">ax1.plot(t[:<span class="number">100</span>], X[<span class="number">0</span>][:<span class="number">100</span>])</span><br><span class="line">ax2.plot(t[:<span class="number">100</span>], X[<span class="number">1</span>][:<span class="number">100</span>])</span><br><span class="line">ax3.plot(t[:<span class="number">100</span>], X[<span class="number">2</span>][:<span class="number">100</span>])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/11/01/CS229-ICA-Implementation/ICA_13_0.png" /></p>
<h2 id="ica-algorithm">ICA Algorithm</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># https://towardsdatascience.com/separating-mixed-signals-with-independent-component-analysis-38205188f2f4</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">center</span>(<span class="params">x</span>):</span></span><br><span class="line">  <span class="keyword">return</span> x - np.mean(x, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>), np.mean(x, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">covariance</span>(<span class="params">x</span>):</span></span><br><span class="line">  mean = np.mean(x, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">  n = np.shape(x)[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">  m = x - mean</span><br><span class="line">  <span class="keyword">return</span> m.dot(m.T) / n</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># https://towardsdatascience.com/separating-mixed-signals-with-independent-component-analysis-38205188f2f4</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">whiten</span>(<span class="params">x</span>):</span></span><br><span class="line">  cov = covariance(x)</span><br><span class="line">  U,S,V = np.linalg.svd(cov)</span><br><span class="line">  d = np.diag(<span class="number">1</span> / np.sqrt(S))</span><br><span class="line">  whiteM = np.dot(U, np.dot(d, U.T))</span><br><span class="line">  Xw = np.dot(whiteM, X)</span><br><span class="line">  <span class="keyword">return</span> Xw, whiteM</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">x</span>):</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">W, xi, lr</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Stochastic gradient ascent, one point xi</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  gradient = np.dot((<span class="number">1</span> - <span class="number">2</span> * sigmoid(np.dot(W.T, xi))), xi.T) + np.linalg.inv(W.T)</span><br><span class="line">  diff = np.abs(gradient).sum()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> W + lr*gradient, diff</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span>(<span class="params">X, S, lr=<span class="number">0.1</span>, thresh=<span class="number">1e-2</span>, iterations=<span class="number">1000000</span></span>):</span></span><br><span class="line">  d, n = S.shape</span><br><span class="line">  W = np.random.random(size=(d,d))</span><br><span class="line">  <span class="keyword">for</span> c <span class="keyword">in</span> tqdm(range(iterations)):</span><br><span class="line">    xi = X[:,np.random.randint(<span class="number">0</span>,n<span class="number">-1</span>)]</span><br><span class="line">    W, diff = update(W, xi, lr)</span><br><span class="line">    <span class="keyword">if</span> (diff &lt; thresh):</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">  <span class="keyword">return</span> W</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Xc, meanX = center(X)</span><br><span class="line">Xw, whiteM = whiten(Xc)</span><br><span class="line"></span><br><span class="line">W_ = optimize(Xw,S)</span><br><span class="line">unMixed = Xw.T.dot(W_.T)</span><br><span class="line">unMixed = (unMixed.T - meanX)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">3</span>))</span><br><span class="line">plt.plot(t[:<span class="number">100</span>], unMixed[<span class="number">0</span>][:<span class="number">100</span>])</span><br><span class="line">plt.plot(t[:<span class="number">100</span>], unMixed[<span class="number">1</span>][:<span class="number">100</span>])</span><br><span class="line">plt.plot(t[:<span class="number">100</span>], unMixed[<span class="number">2</span>][:<span class="number">100</span>])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/11/01/CS229-ICA-Implementation/ICA_21_0.png" /></p>
<h2 id="remark">Remark</h2>
<p>The ICA ambiguities tells us that <strong>there is no way to recover the scaling of the wi's</strong></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224NA4</title>
    <url>/2020/08/20/CS224NA4/</url>
    <content><![CDATA[<h1 id="cs224n-a4-nmt-assignment">CS224N A4: NMT Assignment</h1>
<p>Note: Heavily inspired by the https://github.com/pcyin/pytorch_nmt repository</p>
<p>作业分为两部分， 第一部分代码实现 NMT with RNN， 第二部分文字题分析 NMT</p>
<h2 id="nmt-with-rnn">1. NMT with RNN</h2>
<ul>
<li>Bidirectional LSTM Encoder &amp; Unidirectional LSTM Decoder</li>
<li>勘误：
<ul>
<li>h 为(embedding size)</li>
<li><strong>(3), (4) 式中的下标 1 应改为 m</strong></li>
</ul></li>
<li><ol type="a">
<li>utils.py</li>
</ol>
<ul>
<li>要求每个 batch 里的句子有相同 length。utils.py 中实现 pad_sents(padding)</li>
</ul></li>
<li><ol start="2" type="a">
<li>model_embeddings.py</li>
</ol>
<ul>
<li>先去 vocab.py 里了解类的定义。</li>
<li>VocabEntry 类初始化参数 word2id(dict: words -&gt; indices), id2word 返回 idx 对应的 word 值，from_corpus 从 corpus 生成一个 VocabEntry 实例，from <strong>collections</strong> import <strong>Counter</strong>，Counter 可以直接查找出字符串中字母出现次数</li>
<li>Vocab 类包含 src 和 tgt 语言，初始化参数式两种语言的 VocabEntry， <span class="citation" data-cites="staticmethod">@staticmethod</span> 静态方法</li>
<li>VocabEntry.from_corpus 创建一个 vocab_entry 对象。Vocab.build 分别用 src_sents, tgt_sents 创建 src, tgt 两个 vocab_entry 并返回包含两者的 Vocab(src, tgt)</li>
<li>运用 nn.Embedding 初始化词嵌。</li>
</ul></li>
<li><ol start="3" type="a">
<li>nmt_model.py</li>
</ol>
<ul>
<li>按照 pdf 中的维度对各层初始化</li>
</ul></li>
<li><ol start="4" type="a">
<li>nmt_model.py 中 encode 方法实现</li>
</ol>
<ul>
<li><code>self.encoder</code> 是一个双向 lstm</li>
<li><code>encode</code> 方法传入两个参数：<code>source_padded, source_lengths</code>。前者是已经 pad 后(src_len, b)的 tensor，每一列是一个句子。后者是一个整数列表，表示每个句子实际多少词。</li>
<li>需要返回两个值：enc_hiddens = hencs(所有 1&lt;=i&lt;=m(句长),每一句中所有词，同时对于整个 batch 所有句子), dec_init_state = (hdec0, cdec0)</li>
<li>lstm 要求输入满足规范形状，所以需要<code>pad_packed_sequence</code> 和<code>packed_pad_sequence</code>进行变形</li>
<li>第一步用<code>self.model_embeddings</code>把 source_padded 转换为词嵌入</li>
</ul></li>
<li><ol start="5" type="a">
<li><code>decode</code>方法</li>
</ol>
<ul>
<li><code>self.decode</code>r 是<code>nn.LSTMCell</code>，返回值 h、c，但这部分包装在 step 里面，本 decode 方法里从<code>self.step</code>取得返回值<code>dec_state, combined_output, e_t</code></li>
<li>还是先用<code>model_embeddings</code>将 target_padded 转换为 Y，一个目标词嵌入，(tgt_len, b, e)</li>
<li>用<code>torch.split</code>方法， 将 Y 按第 0 维分成步长为 1 的步数，相当于逐词(t)操作。</li>
<li>(5)式表明了一个迭代过程，最后关心的<code>combined_outputs</code>是 o_t 集合</li>
<li><strong>07/23 勘误</strong> 做a5时发现dedcode忘记更新o_prev</li>
</ul></li>
<li><ol start="6" type="a">
<li><code>step</code>方法</li>
</ol>
<ul>
<li>step 方法具体处理(5)到(12)式。</li>
<li>第一部分，(5)-(7)，运用 bmm、(un)squeeze。bmm 需要注意第 0 维度是留给 batch_size 的，两个三维 tensor 的第一二维相乘，满足维度要求。常见的是在 dim=1/2 做 unsqueeze，乘完再 squeeze</li>
<li>注意到调换乘法次序+不同的变换维度方式会造成最终结果的精度损失。</li>
</ul></li>
<li><ol start="7" type="a">
<li>文字题：<code>generate_sent_masks()</code> 生成 <code>enc_masks(b, src_len)</code>标识 batch 中每个 sentence 每个词是否是 pad，这样做对 attention 计算的影响以及其必要性。</li>
</ol>
<ul>
<li><code>step</code>中，(8)式 α_t 进行了 softmax，后续 a_t 计算为确保 attention 不受 padding 影响要求 padding 处 α_t=0，即 e_t 设置为-∞。</li>
</ul></li>
<li><ol type="i">
<li></li>
</ol>
<ul>
<li><p>git 配置：git remote add origin https://github.com/hy2632/cs224n.git</p></li>
<li><p>git push origin master</p></li>
<li><p>..</p></li>
<li><p>Corpus BLEU: 31.892219171042335</p></li>
</ul></li>
<li><ol start="10" type="a">
<li><table>
<thead>
<tr class="header">
<th>Attention Type</th>
<th>Advantage</th>
<th>Disadvantage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Dot Product</td>
<td>不需要<code>self.att_projection</code>层</td>
<td>需要满足维度一致</td>
</tr>
<tr class="even">
<td>Multiplicative</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="odd">
<td>Additive</td>
<td>tanh 操作 normalize 了数值</td>
<td>两个参数矩阵，参数更多，空间复杂度大</td>
</tr>
</tbody>
</table></li>
</ol></li>
</ul>
<h2 id="analyzing-nmt-systems-30-points">2. Analyzing NMT Systems (30 points)</h2>
<ul>
<li><p>参考 lec8 slides P50-</p></li>
<li><p>https://www.skynettoday.com/editorials/state_of_nmt</p></li>
<li><p>Out-of-vocabulary words, Domain mismatch between src&amp;tgt, maintaining context over longer text, Low-resource language pairs.</p></li>
<li><ol type="a">
<li></li>
</ol>
<ul>
<li><ol type="i">
<li></li>
</ol>
<ul>
<li><ol type="1">
<li>error: favorite of my favorites,</li>
</ol></li>
<li><ol start="2" type="1">
<li>reason: tgt 库中缺乏 one of my favorites 这样的表达, "Low-resource language pairs"</li>
</ol></li>
<li><ol start="3" type="1">
<li>to fix: 添加训练数据</li>
</ol></li>
</ul></li>
<li><ol start="2" type="i">
<li></li>
</ol>
<ul>
<li><ol type="1">
<li>error: most read 译为了 more reading。</li>
</ol></li>
<li><ol start="2" type="1">
<li>reason: 使用 google translator 发现 ms ledo 被译为 read more， 而 ms ledo en los EEUU 被译为 most read in the US。西班牙语的特点？特定的语言构造。</li>
</ol></li>
<li><ol start="3" type="1">
<li>to fix: 需要让 ms ledo 和后面的定语建立更强的联系，从而把握语义理解。增大 hidden_size</li>
</ol></li>
</ul></li>
<li><ol start="3" type="i">
<li></li>
</ol>
<ul>
<li><ol type="1">
<li>error: "<unk>"</li>
</ol></li>
<li><ol start="2" type="1">
<li>reason: Out-of-vocabulary</li>
</ol></li>
<li><ol start="3" type="1">
<li>to fix: 添加到词表</li>
</ol></li>
</ul></li>
<li><ol start="4" type="i">
<li></li>
</ol>
<ul>
<li><ol type="1">
<li>error: block -&gt; apple</li>
</ol></li>
<li><ol start="2" type="1">
<li>reason: "manzana" 多义性</li>
</ol></li>
<li><ol start="3" type="1">
<li>to fix: 训练集添加 manzana 作为 block 含义的 phrase 数据，且大于“vuelta a la manzana”因为 google translator 仍将该句错译。</li>
</ol></li>
</ul></li>
<li><ol start="22" type="a">
<li></li>
</ol>
<ul>
<li><ol type="1">
<li>error: "la sala de profesores": "teacher's lounge" -&gt; "women's room",</li>
</ol></li>
<li><ol start="2" type="1">
<li>reason: "profesores"应该是复数，不包含性别，该句既错译又包含性别 bias。</li>
</ol></li>
<li><ol start="3" type="1">
<li>to fix: 增加“profesores”/profesor/profesora 的训练数据，平衡性别 bias 的同时也要将 teacher 翻译出来。</li>
</ol></li>
</ul></li>
<li><ol start="6" type="i">
<li></li>
</ol>
<ul>
<li><ol type="1">
<li>error: hectare -&gt; acre</li>
</ol></li>
<li><ol start="2" type="1">
<li>reason: 常识错误，涉及到单位转换</li>
</ol></li>
<li><ol start="3" type="1">
<li>to fix: 没想到好的方法。文章中写：General knowledge about the world is necessary for NMT systems to translate effectively. <strong>However, this knowledge is difficult to encode in its entirety and is not easily extractable from volumes of data. We need mechanisms to incorporate common sense and world knowledge into our neural networks.</strong></li>
</ol></li>
</ul></li>
</ul></li>
<li><ol start="2" type="a">
<li></li>
</ol>
<ul>
<li><p>88:</p>
<ul>
<li>When he 's born , the baby looks a little bit .</li>
<li>When the child is born, she looks like a girl.</li>
<li>Cuando nace, el beb tiene aspecto de nia.(Cuando nace, el bebé tiene aspecto de niña.)</li>
<li>错误：语意，原因：src 本身存在错误无法显示符号，解决方案：样本数据和测试数据的编码格式改一哈</li>
</ul></li>
<li><p>109：</p>
<ul>
<li>So , there are many of a lot of sex .</li>
<li>So sex can come in lots of different varieties.</li>
<li>Entonces, hay muchas variedades de sexo.</li>
<li>错误：语法（many a lot of)，variedades 没有翻译出 variety 的意思。解决方案：增加 muchas variedades 的数据</li>
</ul></li>
</ul></li>
<li><ol start="3" type="a">
<li>BLEU 的定义见 &lt;BLEU: a Method for Automatic Evaluation of Machine Translation&gt; Candidate c, Reference r, BLEU 包含两部分：reference 中出现 candidate 中 ngram phrase 的概率（注意有个 ceiling）和 candidate 太长导致的 brevity penalty。</li>
</ol></li>
<li><ol type="i">
<li></li>
</ol>
<ul>
<li>for c1,
<ul>
<li>p1 = (0 + 1 + 1 + 1 + 0)/5 = 3/5</li>
<li>p2 = (0 + 1 + 1 + 0) /4 = 1/2</li>
<li>len(c) = 5</li>
<li>len(r) = 6</li>
<li>BP = exp(1-6/5) = 0.819</li>
<li><code>BLEU = 0.819 * exp(0.5*log(3/5) + 0.5*log(1/2)) = 0.449</code></li>
</ul></li>
<li>for c2,
<ul>
<li>p1 = (1 + 1 + 0 + 1 + 1)/5 = 4/5</li>
<li>p2 = (1 + 0 + 0 + 1) /4 = 1/2</li>
<li>len(c) = 5</li>
<li>len(r) = 4</li>
<li>BP = 1</li>
<li><code>BLEU = 1 * exp(0.5*log(4/5) + 0.5*log(1/2)) = 0.632</code></li>
</ul></li>
<li>c2 更好。</li>
</ul></li>
<li><ol start="2" type="i">
<li></li>
</ol>
<ul>
<li>c1: p1 = 3/5, p2 = 1/2, BLEU 不变 0.449</li>
<li>c2: p1 = 2/5, p2 = 1/4, len(c) = 5, len(r) = 6, <code>BLEU = 0.819 * exp(0.5*log(2/5) + 0.5*log(1/4)) = 0.259</code></li>
<li>当前 c1 更好。</li>
</ul></li>
<li><ol start="3" type="i">
<li></li>
</ol>
<ul>
<li>单一 ref 产生类似 ii 的问题， 比如对于 r2，c2 可以说是非常好的翻译，如果没有 r2 仅用 r1 判断，c2 就比 c1 差很多。</li>
</ul></li>
<li><ol start="4" type="i">
<li></li>
</ol></li>
</ul>
<table>
<thead>
<tr class="header">
<th>BLEU vs Human</th>
<th>1</th>
<th>2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Pro</td>
<td>Fast</td>
<td>Language independent</td>
</tr>
<tr class="even">
<td>Con</td>
<td>Lack of Common Sense</td>
<td>Need Multiple References</td>
</tr>
</tbody>
</table>
]]></content>
      <categories>
        <category>CS224N作业笔记</category>
      </categories>
      <tags>
        <tag>CS224N</tag>
      </tags>
  </entry>
  <entry>
    <title>CS229 Notes10 Principal components analysis</title>
    <url>/2020/11/02/CS229-Notes10-Principal-components-analysis/</url>
    <content><![CDATA[<h2 id="remark">Remark</h2>
<p>(11/3) PCA中的centering的目的是？</p>
<ul>
<li>注意到我们对于协方差矩阵的表达式 <span class="math inline">\(\Sigma = XX^T, X\in \mathbb{R}^{N\times d}\)</span></li>
<li>实际上的协方差表达式 <span class="math inline">\(Cov(X) = E[XX^T] - E[X] (E[X])^T\)</span></li>
<li>所以需要centering各分量</li>
</ul>
<p>(11/3) PCA中特征值为什么对应方差贡献量？</p>
<ul>
<li>对<span class="math inline">\(\Sigma\)</span> 这个半正定对称矩阵的分解可以看成 <span class="math inline">\(\Sigma = U\Lambda U^T\)</span>，即U和V相等。而U的每个列向量组成标准(norm=1)正交基。最终的<span class="math inline">\(\Sigma\)</span>是各个分量按特征值加权再组合。因而这里的特征值可以反映高维椭圆各正交轴的长短。又因为每个<span class="math inline">\(\lambda uu^T\)</span>的量纲是二次，所以<span class="math inline">\(\lambda\)</span>可以反映方差贡献量。</li>
<li>SVD那张经典的示意图。正交阵的本质是换基（比如旋转变换），而<span class="math inline">\(\Lambda\)</span> 这种特征值对角矩阵就表示对各分量进行不同程度的伸缩。</li>
</ul>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/Singular-Value-Decomposition.svg/220px-Singular-Value-Decomposition.svg.png" /></p>
<h2 id="import-packages">Import Packages</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> tqdm.notebook <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure>
<p><strong><a href="https://scipy-lectures.org/packages/scikit-learn/auto_examples/plot_pca.html">Link: Plot PCA</a></strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.shape, y.shape</span><br></pre></td></tr></table></figure>
<pre><code>((150, 4), (150,))</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA(n_components=<span class="number">0.9</span>, whiten=<span class="literal">True</span>)</span><br><span class="line">pca.fit(X)</span><br></pre></td></tr></table></figure>
<pre><code>PCA(copy=True, iterated_power=&#39;auto&#39;, n_components=0.9, random_state=None,
    svd_solver=&#39;auto&#39;, tol=0.0, whiten=True)</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pca.explained_variance_ratio_</span><br></pre></td></tr></table></figure>
<pre><code>array([0.92461872])</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_pca = pca.transform(X)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_pca.shape</span><br></pre></td></tr></table></figure>
<pre><code>(150, 1)</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">target_ids = range(len(iris.target_names))</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i, c, label <span class="keyword">in</span> zip(target_ids, <span class="string">&#x27;rgbcmykw&#x27;</span>, iris.target_names):</span><br><span class="line">  plt.scatter(X_pca[y==i, <span class="number">0</span>], np.zeros(X_pca[y==i].shape), c=c, label=label)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/11/02/CS229-Notes10-Principal-components-analysis/CS229-Notes10-Principal-components-analysis_9_0.png" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pca = PCA(n_components=<span class="number">2</span>, whiten=<span class="literal">True</span>)</span><br><span class="line">pca.fit(X)</span><br><span class="line">X_pca = pca.transform(X)</span><br><span class="line">target_ids = range(len(iris.target_names))</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i, c, label <span class="keyword">in</span> zip(target_ids, <span class="string">&#x27;rgbcmykw&#x27;</span>, iris.target_names):</span><br><span class="line">  plt.scatter(X_pca[y==i, <span class="number">0</span>], X_pca[y==i, <span class="number">1</span>], c=c, label=label)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/11/02/CS229-Notes10-Principal-components-analysis/CS229-Notes10-Principal-components-analysis_10_0.png" /></p>
<h2 id="pca-implementation">PCA Implementation</h2>
<h3 id="construct-a-pca-function">Construct a PCA function</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myPCA</span>(<span class="params">X:np.ndarray, n_dimensions:int</span>):</span></span><br><span class="line">  <span class="comment"># N, d = X.shape</span></span><br><span class="line">  <span class="comment"># Centering</span></span><br><span class="line">  X_centered = X - X.mean(axis=<span class="number">0</span>) </span><br><span class="line"></span><br><span class="line">  <span class="comment"># Covariance Matrix of d*d</span></span><br><span class="line">  Sigma = np.dot(X_centered.T, X_centered)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># SVD</span></span><br><span class="line">  U, Lambda, V = np.linalg.svd(Sigma)</span><br><span class="line"></span><br><span class="line">  X_centered_PC = np.dot(U[:,:n_dimensions].T, X_centered.T).T</span><br><span class="line">  X_PC = X_centered_PC + X.mean(axis=<span class="number">0</span>)[:n_dimensions]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Purposely rescale and add negative sign to mimic Sklearn&#x27;s PCA</span></span><br><span class="line">  <span class="keyword">return</span> -(X_PC - X_PC.mean(axis=<span class="number">0</span>))/X_PC.std(axis=<span class="number">0</span>) </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="comparison-mypca-sklearn.decomposition.pca">Comparison: myPCA &amp; sklearn.decomposition.PCA</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_PC2 = myPCA(X, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i, c, label <span class="keyword">in</span> zip(target_ids, <span class="string">&#x27;rgbcmykw&#x27;</span>, iris.target_names):</span><br><span class="line">  plt.scatter(X_PC2[y==i, <span class="number">0</span>], X_PC2[y==i, <span class="number">1</span>], c=c, label=label)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/11/02/CS229-Notes10-Principal-components-analysis/CS229-Notes10-Principal-components-analysis_31_0.png" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pca = PCA(n_components=<span class="number">2</span>, whiten=<span class="literal">True</span>)</span><br><span class="line">pca.fit(X)</span><br><span class="line">X_pca = pca.transform(X)</span><br><span class="line">target_ids = range(len(iris.target_names))</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i, c, label <span class="keyword">in</span> zip(target_ids, <span class="string">&#x27;rgbcmykw&#x27;</span>, iris.target_names):</span><br><span class="line">  plt.scatter(X_pca[y==i, <span class="number">0</span>], X_pca[y==i, <span class="number">1</span>], c=c, label=label)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/11/02/CS229-Notes10-Principal-components-analysis/CS229-Notes10-Principal-components-analysis_32_0.png" /></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title>CS229 Notes1 Supervised Learning</title>
    <url>/2020/10/26/CS229-Notes1-Supervised-Learning/</url>
    <content><![CDATA[<h1 id="part-i-linear-regression">Part I Linear Regression</h1>
<p>Try to train the hypothesis function <span class="math inline">\(h\)</span>.</p>
<p>Letting <span class="math inline">\(x_0=1\)</span> gives the error term. <span class="math display">\[h(x) = \sum_{i=0}^d{\theta_ix_i}=\theta^T x\]</span></p>
<p>Cost function w.r.t parameter <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[J(\theta) = \frac12\sum_{i=1}^{n}{(h_\theta(x^{(i)}) - y^{(i)})^2}\]</span></p>
<h2 id="lms-algorithm">LMS algorithm</h2>
<p>Gradient descent: <span class="math display">\[\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j}J(\theta)\]</span></p>
<p><span class="math inline">\(\alpha\)</span> is the learning rate.</p>
<p>Here, <span class="math display">\[\frac{\partial}{\partial\theta_j}J(\theta) = (h_\theta(x) - y)x_j\]</span></p>
<p>For <strong>a single training example</strong>, the learning rule is <strong>LMS</strong>(least squared error) a.k.a <strong>Widrow-Hoff</strong></p>
<p><span class="math display">\[\theta_j := \theta_j + \alpha (y - h_\theta(x))x_j\]</span></p>
<p>Into vector level,</p>
<p><span class="math display">\[\theta := \theta + \alpha {(y^{(i)} - h_\theta(x^{(i)}))x^{(i)}}\]</span></p>
<p>One sample each time, we call this <strong>SGD</strong>(strochastic gradient descent).</p>
<p>About applying the rule to <strong>a training set</strong>, - SGD - <strong>batch gradient descent</strong></p>
<p><span class="math display">\[\theta := \theta + \alpha \sum_{i=1}^{n}{(y^{(i)} - h_\theta(x^{(i)}))x^{(i)}}\]</span></p>
<p>SGD vs BGD: Fast / Robust</p>
<h2 id="the-normal-equations">The normal equations</h2>
<h3 id="matrix-derivatives-elementwise">Matrix derivatives (elementwise)</h3>
<p>... ### Least squares revisited (vectorized)</p>
<p>The <strong>design matrix</strong> <span class="math inline">\(X\)</span></p>
<p><span class="math display">\[X = \begin{bmatrix} (x^{(1)})^T\\ (x^{(2)})^T\\...\\(x^{(n)})^T\end{bmatrix} \in \mathbb{R}^{n\times(d+1)}\]</span></p>
<p><span class="math display">\[y = \begin{bmatrix} y^{(1)}\\ y^{(2)}\\...\\y^{(n)}\end{bmatrix}\]</span></p>
<p><span class="math display">\[X\theta - y = \begin{bmatrix} h_\theta(x^{(1)})-y^{(1)}\\ h_\theta(x^{(2)})-y^{(2)}\\...\\h_\theta(x^{(n)})-y^{(n)}\end{bmatrix}\]</span></p>
<p><span class="math display">\[J(\theta) = \frac12(X\theta-y)^T(X\theta-y)\]</span></p>
<p><span class="math display">\[\nabla_\theta J(\theta) = X^TX\theta - X^Ty\]</span></p>
<p>To minimize <span class="math inline">\(J\)</span>, we set its derivatives to zero, and obtain <strong>normal equations</strong>:</p>
<p><span class="math display">\[X^TX\theta = X^Ty\]</span></p>
<p>Thus, the value of <span class="math inline">\(\theta\)</span> that minimizes the <span class="math inline">\(J(\theta)\)</span> is given in closed form by the equation</p>
<p><span class="math display">\[\theta = (X^TX)^{-1}X^Ty\]</span></p>
<h3 id="probablistic-interpretation">Probablistic interpretation</h3>
<p>Assume</p>
<p><span class="math display">\[y^{(i)} = \theta^Tx^{(i)} + \epsilon^{(i)}  \]</span></p>
<p>Assume that the error term <span class="math inline">\(\epsilon \sim N(0, \sigma^2)\)</span></p>
<p>The density function of $^{(i)} $ is given by</p>
<p><span class="math display">\[p(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(\epsilon^{(i)})^2}{2\sigma^2}}\]</span></p>
<p>Substitute <span class="math inline">\(\epsilon^{(i)}\)</span> with <span class="math inline">\(\theta^Tx^{(i)} -y^{(i)}\)</span></p>
<p><span class="math display">\[p(y^{(i)} | x^{(i)}; \theta) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(\theta^Tx^{(i)} -y^{(i)})^2}{2\sigma^2}}\]</span></p>
<p>Note that we are not conditioning on <span class="math inline">\(\theta\)</span>, because <span class="math inline">\(\theta\)</span> is not a random variable. The distribution of <span class="math inline">\(y^{(i)}\)</span> can also be written as <span class="math inline">\(y^{(i)}|x^{(i)}; \theta \sim N(0, \sigma^2)\)</span></p>
<p>By seeing the conditional distribution as a function of <span class="math inline">\(\theta\)</span>, we define the <strong>likelihood function</strong>:</p>
<p><span class="math display">\[L(\theta) = \prod_{i=1}^{n}{\frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(\theta^Tx^{(i)} -y^{(i)})^2}{2\sigma^2}}}\]</span></p>
<p>How to best guess the parameter <span class="math inline">\(\theta\)</span>? <strong>Maximum likelihood</strong> --- to make the data as high-probability as possible.</p>
<p><em>Comment: 频率学派(Frequentist), MLE, "Seeing is believing"</em></p>
<p>Maximize the <strong>log likelihood</strong> or minimize the negative log likelihood (NLL).</p>
<p><span class="math display">\[\begin{aligned}l(\theta) &amp;= \log{L(\theta)}\\
&amp;= n\log{\frac1{\sqrt{2\pi}\sigma}} - \frac1{\sigma^2}\cdot \frac12\sum_{i=1}^{n}(\theta^Tx^{(i)} -y^{(i)})^2
\end{aligned}\]</span></p>
<p>Hence it gives the same answer as minimizing the cost function.</p>
<p>Note that here the choice of <span class="math inline">\(\theta\)</span> does not rely on <span class="math inline">\(\sigma^2\)</span></p>
<h3 id="locally-weighted-linear-regression-lwr">Locally weighted linear regression (LWR)</h3>
<p>Instead of fitting <span class="math inline">\(\theta\)</span> to minimize <span class="math inline">\(\sum_i{(y^{(i)}-\theta^Tx^{(i)})^2}\)</span>, minimize</p>
<p><span class="math display">\[\sum_iw^{(i)}{(y^{(i)}-\theta^Tx^{(i)})^2}\]</span></p>
<p>The standard choice for the weights:</p>
<p><span class="math display">\[w^{(i)} = \exp\bigg( -\frac{(x^{(i)}-x)^2}{2\tau^2} \bigg)\]</span></p>
<p>Comment: part of pdf of <span class="math inline">\(N(x, \tau^2)\)</span>, but has nothing to do with gaussian. Just measures the distance, lower weights to distant data points. <span class="math inline">\(tau\)</span> is called the <strong>bandwidth</strong> parameter which controls how quickly the weight diminishes with distance increasing.</p>
<p>LWR is <strong>non-parametric</strong> algorithm. It means the model relies on the training data and scales with bigger size of the training dataset.</p>
<h1 id="part-ii-classification-and-logistic-regression">Part II Classification and logistic regression</h1>
<p>Problem: binary classification Keywords: positive/negative class, label</p>
<h2 id="logistic-regression">Logistic regression</h2>
<p>To limit the predicted value of <span class="math inline">\(y\)</span> to <span class="math inline">\([0,1]\)</span>, use <strong>sigmoid/logitstic</strong> function</p>
<p><span class="math display">\[h_\theta(x) = g(\theta^Tx) = \frac1{1 + e^{-\theta^Tx}}\]</span></p>
<p>Sigmoid's derivative has such property</p>
<p><span class="math display">\[g&#39;(z) = g(z)(1-g(z))\]</span></p>
<p>Probablistic assumptions:</p>
<p><span class="math display">\[P(y=1 | x;\theta) = h_\theta(x)\]</span></p>
<p><span class="math display">\[P(y=0 | x;\theta) = 1-h_\theta(x)\]</span></p>
<p>Or more compactly written:</p>
<p><span class="math display">\[P(y| x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y}\]</span></p>
<p>Then we can build a likelihood function for <span class="math inline">\(n\)</span> training examples</p>
<p><span class="math display">\[\begin{aligned}L(\theta) &amp;= p(y|X; \theta)\\
&amp;= \prod_{i=1}^{n}{(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}}
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}l(\theta) &amp;= \log{L(\theta)}\\
&amp;= \sum_{i=1}^{n}{y^{(i)}\log h(x^{(i)})+({1-y^{(i)}})\log(1-h(x^{(i)}))}
\end{aligned}\]</span></p>
<p>Optimize the LL by Stochastic Gradient Ascent(maximizing):</p>
<p><span class="math display">\[\begin{aligned}\frac{\partial}{\partial \theta_j}l(\theta) 
&amp;= \bigg(y\frac1{g(\theta^Tx)} + (1-y)\frac1{1-g(\theta^Tx)}\bigg)\frac{\partial}{\partial \theta_j}g(\theta^Tx)\\
&amp;= \bigg(y\frac1{g(\theta^Tx)} + (1-y)\frac1{1-g(\theta^Tx)}\bigg)g(\theta^Tx)(1-g(\theta^Tx))\frac{\partial}{\partial \theta_j}\theta^Tx\\
&amp;= \big(y(1-g(\theta^Tx)) - (1-y)g(\theta^Tx)\big)x_j\\
&amp;= (y-h_\theta(x))x_j
\end{aligned}\]</span></p>
<p>Update rule:</p>
<p><span class="math display">\[\theta_j := \theta_j + \alpha(y - h_\theta(x))x_j\]</span></p>
<p>This looks identical with LMS update rule but <span class="math inline">\(h_\theta(x)\)</span> here is different. This is actually general rule.</p>
<h2 id="digression-perceptron-learning-algorithm">Digression: perceptron learning algorithm</h2>
<p>change <span class="math inline">\(g(z)\)</span> to <span class="math inline">\(1\{z\geq 0\}\)</span>, still using the previous update rule.</p>
<h2 id="newtons-method-for-maximizing-ltheta">Newton's method for maximizing <span class="math inline">\(l(\theta)\)</span></h2>
<p>Newton's method for finding a zero of a function. <span class="math display">\[\theta := \theta - \frac{f(\theta)}{f&#39;(\theta)}\]</span></p>
<p>To maximize, we want the derivative <span class="math inline">\(l&#39;(\theta) = 0\)</span>. So we set <span class="math inline">\(f(\theta) = l&#39;(\theta)\)</span> <span class="math display">\[\theta := \theta - \frac{l&#39;(\theta)}{l&#39;&#39;(\theta)}\]</span></p>
<p>Generalize Newton's method to vector-valued/multidimensional setting (a.k.a Newton-Raphson Method):</p>
<p><span class="math display">\[\theta := \theta - H^{-1}\nabla_\theta l(\theta)\]</span></p>
<p>Complexity: <span class="math inline">\(O(N^2)\)</span> for inverting <span class="math inline">\(d\times d\)</span> Hessian. Faster than batch gradient descent.</p>
<p>When maximizing logistic regression, called <strong>Fisher scoring</strong>.</p>
<h1 id="part-iii-generalized-linear-modelsglms">Part III Generalized Linear Models(GLMs)</h1>
<p>In the Classification part, the derivative of sigmoid function is kind of relevant to Bernoulli distribution.</p>
<p>Both of Linear regression and calssification are special cases of a broader family of models in GLM family.</p>
<h2 id="the-exponential-family">The exponential family</h2>
<p><span class="math display">\[p(y; \eta) = b(y)\exp(\eta^TT(y) - a(\eta))\]</span></p>
<p><span class="math inline">\(\eta\)</span>: natural/canonical parameter</p>
<p><span class="math inline">\(T(y)\)</span>: sufficient statistic, often <span class="math inline">\(y\)</span></p>
<p><span class="math inline">\(a(\eta)\)</span>: log partition function. <span class="math inline">\(\exp{a(\eta)}\)</span> used to normalize so the distributino integrates into 1.</p>
<p><span class="math inline">\(T\)</span> and <span class="math inline">\(b\)</span> dedfines a <strong>family</strong> of distributions parametrized by <span class="math inline">\(\eta\)</span></p>
<p><strong>Bernoulli</strong> and <strong>Gaussian</strong> distributions are examples of <strong>exponential family distributions</strong>.</p>
<p><span class="math display">\[\begin{aligned}
p(y;\phi) 
&amp;= \phi^y (1-\phi)^{1-y}\\
&amp;= \exp(y\log\phi + (1-y)\log(1-\phi))\\
&amp;= \exp\bigg( \bigg( \log \bigg( \frac{\phi}{1-\phi} \bigg)y + \log(1-\phi) \bigg) \bigg)
\end{aligned}\]</span></p>
<p>Exponential family distribution notations:</p>
<p><span class="math display">\[\begin{aligned} 
\eta &amp;= \log(\phi/(1-\phi))\\
\phi &amp;=  \frac{1}{1+e^{-\eta}}\\
T(y) &amp;=  y \\
a(\eta) &amp;=  -log(1-\phi)\\
&amp;= \log (1+e^{\eta})\\
b(y) &amp;= 1
\end{aligned}\]</span></p>
<p>Notice that <span class="math inline">\(\phi\)</span> is the sigmoid function.</p>
<p>For Gaussian distribution with <span class="math inline">\(\sigma^2 = 1\)</span> <span class="math display">\[\begin{aligned}
p(y;\mu) &amp;= \frac1{\sqrt{2\pi}}\exp\bigg(-\frac12(y-\mu)^2  \bigg)\\
&amp;= \frac1{\sqrt{2\pi}}\exp\bigg(-\frac12y^2 + \mu y -\frac12\mu^2  \bigg) \\
&amp;= \frac1{\sqrt{2\pi}}\exp\bigg(-\frac12y^2\bigg) \cdot \exp\bigg(\mu y -\frac12\mu^2\bigg)
\end{aligned}\]</span></p>
<p>Compare with the exponential distribution family form <span class="math display">\[p(y; \eta) = b(y)\exp(\eta^TT(y) - a(\eta))\]</span></p>
<p>We get</p>
<p><span class="math display">\[\begin{aligned} 
b(y) &amp;= \frac1{\sqrt{2\pi}}\exp\bigg(-\frac12y^2\bigg)\\
\eta &amp;= \mu\\
T(y) &amp;=  y \\
a(\eta) &amp;=  \mu^2/2\\
\end{aligned}\]</span></p>
<h2 id="constructing-glms">Constructing GLMs</h2>
<p>"<strong>Poisson distribution</strong> usually gives a good model for numbers of visitors". (Counting Model)</p>
<p>Poisson is an exponential family distribution.</p>
<p>To derive a GLM, make the following <strong>3 assumptions</strong>, 1. <span class="math inline">\(y|x; \theta\)</span>~ExponentialFamily<span class="math inline">\((\eta)\)</span>. Given <span class="math inline">\(x\)</span> and <span class="math inline">\(\eta\)</span>, the distribution of <span class="math inline">\(y\)</span> follows some exponential family distribution with partameter <span class="math inline">\(\eta\)</span> 2. Given <span class="math inline">\(x\)</span>, our goal is to <strong>predict <span class="math inline">\(T(y)\)</span></strong>. In most cases <span class="math inline">\(T(y) = y\)</span>. We would like <span class="math inline">\(h_\theta(x) = E[T(y)|x;\theta]\)</span>. 3. Natural parameter <span class="math inline">\(\eta\)</span> is linearly related with the inputs <span class="math inline">\(x\)</span>: <span class="math inline">\(\eta = \theta^Tx\)</span>. This is a <strong>"design choice"</strong></p>
<h2 id="ols-revisited-using-glm-construction">OLS revisited using GLM construction</h2>
<p>target variable <span class="math inline">\(y\)</span> is also called <strong>reponse variable</strong> in GLM terminology.</p>
<p><span class="math display">\[y\sim N(\mu, \sigma^2)\]</span> where <span class="math inline">\(\mu\)</span> relies on <span class="math inline">\(x\)</span>.</p>
<p>Previously we have GLM for Gaussian, <span class="math inline">\(\mu = \eta\)</span></p>
<p><span class="math display">\[\begin{aligned} 
h_\theta(x) &amp;= E[y|x;\theta]\\
&amp;= \mu \\
&amp;= \eta \\
&amp;= \theta^Tx
\end{aligned}\]</span></p>
<p>First equality from <strong>assumption 2</strong>. Third equality from <strong>assumption 1</strong>. Last from <strong>assumption 3</strong>.</p>
<h2 id="softmax-regression">Softmax Regression</h2>
<p>Problem set - <strong>Multinomial distribution</strong>: multiple classification, <span class="math inline">\(y\in \{1, ..., k\}\)</span></p>
<p><span class="math inline">\(T(y)\)</span> is a <span class="math inline">\(d-1\)</span> vector with y-th entry 1.</p>
<p><span class="math display">\[T(y)_i = 1\{y = i\}\]</span></p>
<p><span class="math inline">\((T(y))_i\)</span> is the i-th element of <span class="math inline">\(T(y)\)</span></p>
<p><span class="math display">\[E[(T(y))_i] = \phi_i\]</span></p>
<p>The multinomial is a member of the exponential family, where</p>
<p><span class="math display">\[\begin{aligned} 
b(y) &amp;= 1\\
a(\eta) &amp;= -\log(\phi_k)\\
\eta &amp;= \begin{bmatrix}
\log(\phi_1/\phi_k)\\ \log(\phi_2/\phi_k) \\...\\\log(\phi_{k-1}/\phi_k)
\end{bmatrix}
\end{aligned}\]</span></p>
<p>The link function is given by <span class="math display">\[\eta_i = \log\frac{\phi_i}{\phi_k}\]</span></p>
<p>Invert the link function</p>
<p><span class="math display">\[e^{\eta_i} = \phi_i/\phi_k\]</span></p>
<p><span class="math display">\[\phi_ke^{\eta_i} = \phi_i\]</span></p>
<p><span class="math display">\[\phi_k\sum_{i=1}^k{e^{\eta_i}} = \sum_{i=1}^k{\phi_i}=1\]</span></p>
<p><span class="math display">\[\therefore \phi_k = \frac1{\sum_{i=1}^k{e^{\eta_i}}}\]</span></p>
<p><span class="math display">\[\therefore \phi_i = \frac{e^{\eta_i}}{\sum_{j=1}^k{e^{\eta_j}}}\]</span></p>
<p>This function mapping from <span class="math inline">\(\eta\)</span> to <span class="math inline">\(\phi\)</span> is called <strong>softmax</strong>.</p>
<p>From assumption 3, <span class="math inline">\(\eta_i = \theta_i^Tx\)</span>, and since we define <span class="math inline">\(\theta_k = 0\)</span>, <span class="math inline">\(\eta_k = 0\)</span></p>
<p><span class="math display">\[\begin{aligned} p(y=i|x; \theta) 
&amp;= \phi_i \\
&amp;= \frac{e^{\eta_i}}{\sum_{j=1}^k{e^{\eta_j}}} \\
&amp;= \frac{e^{\theta_i^Tx}}{\sum_{j=1}^k{e^{\theta_j^Tx}}}
\end{aligned}\]</span></p>
<p>This is the <strong>softmax regression</strong>. It's a generalization of logistic regression (binary classification).</p>
<p><span class="math display">\[\begin{aligned} h_\theta(x)
&amp;= E[T(y)|x; \theta] \\
&amp;= [\phi_1 ,.. \phi_{k-1}]^T\\
&amp;= \bigg[\frac{e^{\theta_1^Tx}}{\sum_{j=1}^k{e^{\theta_j^Tx}}}, ..., \frac{e^{\theta_{k-1}^Tx}}{\sum_{j=1}^k{e^{\theta_j^Tx}}}\bigg]^T\\
\end{aligned}\]</span></p>
<p>Comment: the output is the estimated probability for each class.</p>
<p>For parameter fitting, maximize the log likelihood using gradient ascent or Newton's method.</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title>CS229 Notes11 Independent Components Analysis</title>
    <url>/2020/10/31/CS229-Notes11-Independent-Components-Analysis/</url>
    <content><![CDATA[<p>The goal of Components Analysis is to find a new basis to represent the data.</p>
<h2 id="cocktail-party-problem">Cocktail party problem:</h2>
<ul>
<li>sources <span class="math inline">\(s\in R^d\)</span></li>
<li>microphone recordings(receiver): <span class="math inline">\(x\in R^d\)</span></li>
<li><strong>mixing matrix</strong> <span class="math inline">\(A\)</span>: <span class="math inline">\(x = As\)</span>, square matrix</li>
<li>unmixing matrix: <span class="math inline">\(W = A^{-1}\)</span>, <span class="math inline">\(s^{(i)} = Wx^{(i)}\)</span>, the superscript means time i.</li>
<li>j-th source at time i <span class="math inline">\(s_j^{(i)} = w_j^Tx^{(i)}\)</span>, <span class="math inline">\(W = [w_1^T, ...w_n^T]^T\)</span>, each row is a <span class="math inline">\(w_i^T\)</span></li>
</ul>
<h2 id="ica-ambiguities">ICA ambiguities</h2>
<h3 id="scale-of-a">scale of A:</h3>
<p><span class="math display">\[x = 2A(0.5s) = As\]</span></p>
<p>Imagine the coupling of volume and distance.</p>
<p><strong>There is no way to recover the correct scaling of the <span class="math inline">\(w_i\)</span>'s.</strong></p>
<h3 id="non-gaussian">Non-Gaussian</h3>
<p>Gaussian's <strong>Isotropy</strong> results in density rotationally symmetric.</p>
<p>If <span class="math inline">\(s \sim N(0,I)\)</span></p>
<p><span class="math display">\[E_s[x] = E[As] = AE[s] = 0\]</span></p>
<p><span class="math display">\[\begin{aligned}Cov[x] 
&amp; = E[xx^T] - (E[x])^2 \\
&amp; = E[xx^T]\\
&amp; = E[Ass^TA^T]\\
&amp; = A\cdot Cov[s] \cdot A^T\\
&amp; = AA^T
\end{aligned}\]</span></p>
<p>Basically, <span class="math display">\[x \sim N(0, AA^T)\]</span></p>
<p>Let <span class="math inline">\(R\)</span> be an arbitrary orthogonal(e.g. rotation/reflection) matrix, <span class="math inline">\(A&#39; = AR\)</span>, then <span class="math inline">\(x&#39; = A&#39;s\)</span>,</p>
<p>the distribution of <span class="math inline">\(x&#39;\)</span> is also gaussian <span class="math inline">\(x&#39;\sim N(0, AA^T)\)</span> because</p>
<p><span class="math display">\[A&#39;A&#39;^T = ARR^TA^T = AA^T\]</span></p>
<p>Then we could never figure out whether the mixing matrix is <span class="math inline">\(A\)</span> or <span class="math inline">\(A&#39;\)</span>.</p>
<p>However, so long as <span class="math inline">\(s\)</span> is distributed anisotropically, we could recover the mixing matrix.(Imagine you have 4 rotation-symmetrical sources, the n rotating these sources simulatenously does not influence the volume)</p>
<h2 id="densities-and-linear-transforms">Densities and linear transforms</h2>
<p><span class="math display">\[p_x(x) = p_s(Wx)\cdot|W|\]</span></p>
<p>E.g. uniform distribution.</p>
<p>Remember <strong>determinant</strong> measures the <strong>volume</strong> of the span. (See this in linalg recap). Imagine the density dillutes in an enlarged hyperspace.</p>
<h2 id="ica-algorithm">ICA Algorithm</h2>
<p>Joint distribution of sources <span class="math inline">\(s\)</span> (with the assumption that sources are independent):</p>
<p><span class="math display">\[p(s) = \prod_{j=1}^{d}{p_s(s_j)}\]</span></p>
<p>then transit to <span class="math inline">\(x\)</span>, j-th source at time i <span class="math inline">\(s_j^{(i)} = w_j^Tx^{(i)}\)</span></p>
<p><span class="math display">\[p(x) = \prod_{j=1}^{d}{p_s(w_j^Tx)\cdot |W|}\]</span></p>
<p>We purposely choose <span class="math inline">\(p_s(s) = g&#39;(s)\)</span>, i.e. choose the sigmoid function as CDF.</p>
<p>Given a training set, the log-likelihood</p>
<p><span class="math display">\[l(w) = \sum_{i=1}^n \bigg(\sum_{j=1}^d{\log g&#39;(w_j^Tx^{(i)}) + \log|W|}\bigg)\]</span></p>
<p><strong>Lemma</strong>:</p>
<p><span class="math display">\[\nabla_W|W = |W|(W^{-1})^T\]</span></p>
<p><span class="math display">\[g&#39;&#39; = \frac{\partial}{\partial x}g&#39; = \frac{\partial}{\partial x}\big(g(1-g)\big) = g&#39;(1-2g) \]</span></p>
<p>Then compute the derivative of <span class="math inline">\(l(w)\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
\therefore \frac{\partial}{\partial w} l(w) 
&amp;= \sum_{i=1}^n \bigg(\sum_{j=1}^d{\frac{g&#39;&#39;(w_j^Tx^{(i)}) \cdot x^{(i)T}}{g&#39;(w_j^Tx^{(i)})} + (W^{-1})^T}\bigg) \\
&amp;= \sum_{i=1}^n \bigg(\sum_{j=1}^d{(1-2g(w_j^Tx^{(i)}))x^{(i)T} + (W^{-1})^T}\bigg)
\end{aligned}\]</span></p>
<p>If we use stochastic gradient ascent,</p>
<p><span class="math display">\[\begin{aligned}
\frac{\partial}{\partial w} l(w) 
&amp;= \sum_{j=1}^d{(1-2g(w_j^Tx^{(i)}))x^{(i)T} + (W^{-1})^T}\\
&amp;= \begin{bmatrix}1-2g(w_1^Tx^{(i)})\\1-2g(w_2^Tx^{(i)}\\...\\1-2g(w_d^Tx^{(i)}\end{bmatrix}x^{(i)T} + (W^{-1})^T
\end{aligned}\]</span></p>
<p><strong>The updating rule</strong>:</p>
<p><span class="math display">\[W:= W + \alpha \bigg( \begin{bmatrix}1-2g(w_1^Tx^{(i)})\\1-2g(w_2^Tx^{(i)}\\...\\1-2g(w_d^Tx^{(i)}\end{bmatrix}x^{(i)T} + (W^{-1})^T\bigg)\]</span></p>
<p>Finally when converge, we take</p>
<p><span class="math display">\[s^{(i)} = Wx^{(i)}\]</span></p>
<p>to recover sources.</p>
<div class="pdfobject-container" data-target="./cs229-notes11.pdf" data-height="1000px"></div>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title>CS229 Notes3 SVM 实现</title>
    <url>/2020/11/23/CS229-Notes3-SVM-%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<h2 id="实现效果">实现效果</h2>
<p><img src="/2020/11/23/CS229-Notes3-SVM-%E5%AE%9E%E7%8E%B0/SVM_tutorial.jpg" /></p>
<h2 id="kernel_featuremaps.py"><a href="https://github.com/hy2632/cs229/blob/master/SVM/Kernel_FeatureMaps.py">Kernel_FeatureMaps.py</a></h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_schmidt_columns</span>(<span class="params">X</span>):</span></span><br><span class="line">    Q, R = np.linalg.qr(X)</span><br><span class="line">    <span class="keyword">return</span> Q</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">orthgonalize</span>(<span class="params">V</span>):</span></span><br><span class="line">    N = V.shape[<span class="number">0</span>]</span><br><span class="line">    d = V.shape[<span class="number">1</span>]</span><br><span class="line">    turns = int(N / d)</span><br><span class="line">    remainder = N % d</span><br><span class="line"></span><br><span class="line">    V_ = np.zeros_like(V)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(turns):</span><br><span class="line">        v = gram_schmidt_columns(V[i * d:(i + <span class="number">1</span>) * d, :].T).T</span><br><span class="line">        V_[i * d:(i + <span class="number">1</span>) * d, :] = v</span><br><span class="line">    <span class="keyword">if</span> remainder != <span class="number">0</span>:</span><br><span class="line">        V_[turns * d:, :] = gram_schmidt_columns(V[turns * d:, :].T).T</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> V_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate orthogonal normal weights (w1, ..., wm)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateGMatrix</span>(<span class="params">m, d</span>) -&gt; np.array:</span></span><br><span class="line">    G = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, (m, d))</span><br><span class="line">    <span class="comment"># Renormalize</span></span><br><span class="line">    norms = np.linalg.norm(G, axis=<span class="number">1</span>).reshape([m, <span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> orthgonalize(G) * norms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Softmax trignometric feature map Φ(x)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">baseline_SM</span>(<span class="params">x, G</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Calculate the result of softmax trigonometrix random feature mapping</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        x: array, dimension = n*d</span></span><br><span class="line"><span class="string">            Input to the baseline mapping</span></span><br><span class="line"><span class="string">            Required to be of norm 1 for i=1,...n</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        G: matrix, dimension = m*d</span></span><br><span class="line"><span class="string">            The matrix in the baseline random feature mapping</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m = G.shape[<span class="number">0</span>]</span><br><span class="line">    left = np.cos(np.dot(x, G.T).astype(np.float32))</span><br><span class="line">    right = np.sin(np.dot(x, G.T).astype(np.float32))</span><br><span class="line">    <span class="keyword">return</span> np.exp(<span class="number">0.5</span>) * ((<span class="number">1</span> / m)**<span class="number">0.5</span>) * np.hstack([left, right])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Kernel</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Kernels.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        x, y: array of (n_x, d), (n_y, d)</span></span><br><span class="line"><span class="string">        Return</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        K(x,y): kernel matrix of (n_x, n_y), K_ij = K(x_i, y_j)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Linear</span>(<span class="params">x, y</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.dot(x, y.T)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Softmax</span>(<span class="params">x, y</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.exp(np.dot(x, y.T))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureMap</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Feature mapping</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        x: array of (n, d)</span></span><br><span class="line"><span class="string">        Return</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        Φ(x): array of (n, m), where m is usually a higher dimensionality. Here we set m = 2*n</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Linear</span>(<span class="params">x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Softmax_Trigonometric</span>(<span class="params">x</span>):</span></span><br><span class="line">        n, d = x.shape</span><br><span class="line">        <span class="comment"># Increase dimensionality to 2x</span></span><br><span class="line">        G = generateGMatrix(<span class="number">2</span> * d, d)</span><br><span class="line">        phi_x = baseline_SM(x, G)</span><br><span class="line">        <span class="keyword">return</span> phi_x</span><br></pre></td></tr></table></figure>
<h2 id="svm.py"><a href="https://github.com/hy2632/cs229/blob/master/SVM/SVM.py">SVM.py</a></h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"><span class="keyword">from</span> tqdm.notebook <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> Kernel_FeatureMaps <span class="keyword">import</span> *;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVM</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">    ## Author:</span></span><br><span class="line"><span class="string">        Hua Yao (hy2632@columbia.edu)</span></span><br><span class="line"><span class="string">    ## Description:</span></span><br><span class="line"><span class="string">        SVM binary classifier, optimizing with the dual lagrangian program, trained on a sample batch. Uses the SMO algorithm.</span></span><br><span class="line"><span class="string">        Normalizes input X to adapt to kernelized version.</span></span><br><span class="line"><span class="string">        Regularization parameter C set to be `np.inf` for easy closed form solution of b.</span></span><br><span class="line"><span class="string">    ## Reference:</span></span><br><span class="line"><span class="string">        [CS229 - Kernel Methods and SVM](http://cs229.stanford.edu/notes2020fall/notes2020fall/cs229-notes3.pdf)</span></span><br><span class="line"><span class="string">    ---</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        X: (N, d)</span></span><br><span class="line"><span class="string">        Y: (N,)</span></span><br><span class="line"><span class="string">        kernel: kernel used, linear/softmax</span></span><br><span class="line"><span class="string">        featuremap: feature mapping corresponding to the kernel used</span></span><br><span class="line"><span class="string">        batch_size: int, also denoted as n</span></span><br><span class="line"><span class="string">        C: l1 regularization term for soft margin. alpha_i in [0, C]. Set as np.inf (no regularization), because the form of b is nasty under regularization.</span></span><br><span class="line"><span class="string">        tol = 1e-6: tolerance, deciding when to end training</span></span><br><span class="line"><span class="string">        Intermediate parameters:</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        x: (n, d), random batch of X</span></span><br><span class="line"><span class="string">        y: (n)</span></span><br><span class="line"><span class="string">        phi_x: (n, m), feature map(s) of x</span></span><br><span class="line"><span class="string">        M: (n, n), M[i,j] = y_iy_j * K(x_i,x_j), hadamard product of y^Ty and K</span></span><br><span class="line"><span class="string">        Learned parameters:</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        alpha: (n,)</span></span><br><span class="line"><span class="string">        w: (d,)</span></span><br><span class="line"><span class="string">        b: int</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 X,</span></span></span><br><span class="line"><span class="function"><span class="params">                 Y,</span></span></span><br><span class="line"><span class="function"><span class="params">                 kernel=Kernel.Linear,</span></span></span><br><span class="line"><span class="function"><span class="params">                 featuremap=FeatureMap.Linear,</span></span></span><br><span class="line"><span class="function"><span class="params">                 batch_size=<span class="number">64</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 C=np.inf,</span></span></span><br><span class="line"><span class="function"><span class="params">                 tol=<span class="number">1e-6</span></span>):</span></span><br><span class="line">        <span class="comment"># C set as np.inf here -- no regularization</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># X: (N,d), Y: (N,), x: (n,d), y:(n,)</span></span><br><span class="line">        <span class="comment"># Fixed values</span></span><br><span class="line">        self.N, self.d = X.shape</span><br><span class="line">        <span class="comment"># Normalize data</span></span><br><span class="line">        self.X = X</span><br><span class="line">        self.X = self.X / np.linalg.norm(self.X, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        self.Y = Y</span><br><span class="line">        self.kernel = kernel</span><br><span class="line">        self.featuremap = featuremap</span><br><span class="line">        self.n = batch_size</span><br><span class="line">        self.C = C</span><br><span class="line">        self.tol = tol</span><br><span class="line"></span><br><span class="line">        batch_indices = np.random.choice(np.arange(self.N), self.n)</span><br><span class="line">        self.x = self.X[batch_indices]</span><br><span class="line">        self.y = self.Y[batch_indices]</span><br><span class="line">        self.phi_x = self.featuremap(self.x)</span><br><span class="line">        self.M = np.outer(self.y, self.y) * self.kernel(self.x, self.x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Learned parameters</span></span><br><span class="line">        self.alpha = np.ones(self.n)</span><br><span class="line">        self.w = np.zeros(self.d)</span><br><span class="line">        self.b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_alpha</span>(<span class="params">self, random_idx1, random_idx2</span>):</span></span><br><span class="line">        Zeta = -np.sum(self.alpha * self.y) + (</span><br><span class="line">            self.alpha * self.y)[random_idx1] + (self.alpha *</span><br><span class="line">                                                 self.y)[random_idx2]</span><br><span class="line">        self.alpha[random_idx1] = (Zeta - self.alpha[random_idx2] *</span><br><span class="line">                                   self.y[random_idx2]) * self.y[random_idx1]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dual_obj</span>(<span class="params">self, alpha</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.sum(alpha) - np.sum(<span class="number">0.5</span> * self.M * np.outer(alpha, alpha))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, iterations=<span class="number">200000</span></span>):</span></span><br><span class="line">        prev_val = self.alpha.copy()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(range(iterations)):</span><br><span class="line">            <span class="comment"># Select 2 alphas randomly</span></span><br><span class="line">            random_idx1, random_idx2 = np.random.choice(</span><br><span class="line">                np.arange(<span class="number">0</span>, self.n), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># The (quadratic w.r.t a2) function that scipy.optimize.minimize takes</span></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">optimizeWRTa2</span>(<span class="params">a2</span>):</span></span><br><span class="line">                self.alpha[random_idx2] = a2</span><br><span class="line">                self.update_alpha(random_idx1, random_idx2)</span><br><span class="line">                <span class="keyword">return</span> -self.dual_obj(self.alpha)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Solve optimization w.r.t a2</span></span><br><span class="line">            a2 = self.alpha[random_idx2]</span><br><span class="line">            res = minimize(optimizeWRTa2, a2, bounds=[(<span class="number">0</span>, self.C)])</span><br><span class="line">            a2 = res.x</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update a2</span></span><br><span class="line">            self.alpha[random_idx2] = a2</span><br><span class="line">            self.update_alpha(random_idx1, random_idx2)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Check convergence</span></span><br><span class="line">            <span class="keyword">if</span> (i % <span class="number">5</span> == <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> np.sum(np.abs(self.alpha - prev_val)) &lt; self.tol:</span><br><span class="line">                    print(</span><br><span class="line">                        <span class="string">f&quot;&gt;&gt; Optimized on the batch, step <span class="subst">&#123;i&#125;</span>. 5 steps Δalpha:<span class="subst">&#123;np.sum(np.abs(self.alpha - prev_val))&#125;</span>&quot;</span></span><br><span class="line">                    )</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> (i % <span class="number">5000</span> == <span class="number">1</span>):</span><br><span class="line">                        print(</span><br><span class="line">                            <span class="string">f&quot;&gt;&gt; Optimizing, step <span class="subst">&#123;i&#125;</span>. Δalpha:<span class="subst">&#123;np.sum(np.abs(self.alpha - prev_val))&#125;</span>&quot;</span></span><br><span class="line">                        )</span><br><span class="line">                    prev_val = self.alpha.copy()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Retrieve w and b</span></span><br><span class="line">        self.w = np.dot(self.alpha * self.y, self.phi_x)</span><br><span class="line">        <span class="comment"># The form of b changes when there exists Regularization C. So we simply cancel C here.</span></span><br><span class="line">        <span class="comment"># Look at P25 of CS229 - Note 3.</span></span><br><span class="line">        <span class="comment"># Form of b under regularization depends on alpha. (http://cs229.stanford.edu/materials/smo.pdf)</span></span><br><span class="line">        <span class="comment"># [Bias Term b in SVMs Again](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2004-11.pdf) gives a general representation</span></span><br><span class="line">        self.b = (np.max(np.dot(self.phi_x, self.w)[self.y == <span class="number">-1</span>]) +</span><br><span class="line">                  np.min(np.dot(self.phi_x, self.w)[self.y == <span class="number">1</span>])) * <span class="number">-0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, X_val</span>):</span></span><br><span class="line">        X_val_normed = X_val / np.linalg.norm(X_val, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> np.sign(</span><br><span class="line">            np.dot(self.kernel(X_val_normed, self.x), self.alpha * self.y) +</span><br><span class="line">            self.b)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span>(<span class="params">self, X_val, y_val</span>):</span></span><br><span class="line">        prediction = self.predict(X_val)</span><br><span class="line">        <span class="keyword">return</span> np.mean(prediction == y_val)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title>CS229 Notes2 Generative Learning Algorithms</title>
    <url>/2020/10/27/CS229-Notes2-Generative-Learning-Algorithms/</url>
    <content><![CDATA[<p>Gaussian discriminant analysis - lemon的文章 - 知乎 https://zhuanlan.zhihu.com/p/22940577</p>
<p>Normal Equation如何实现一步求解最优参数及其对比梯度下降的特点是什么？ - 深度碎片的回答 - 知乎 https://www.zhihu.com/question/273799498/answer/370173526</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title>CS229 Notes12 Reinforcement Learning</title>
    <url>/2020/11/17/CS229-Notes12-Reinforcement-Learning/</url>
    <content><![CDATA[<h2 id="markov-decision-process-mdp">1 Markov decision process (MDP)</h2>
<ul>
<li><p>MDP is a tuple of <span class="math inline">\((S, A, \{P_{sa} \}, \gamma, R)\)</span></p>
<ul>
<li><span class="math inline">\(S\)</span>: states space</li>
<li><span class="math inline">\(A\)</span>: action space</li>
<li><span class="math inline">\(P_{sa}\)</span>: state transition probabilities. Distribution over the state space: Given state <span class="math inline">\(s\)</span>, if we take action <span class="math inline">\(a\)</span>, then the distribution of <span class="math inline">\(s&#39;\)</span></li>
<li><span class="math inline">\(\gamma\)</span> : discount factor, discount future rewards</li>
<li><span class="math inline">\(R\)</span>: reward function, <span class="math inline">\(S\times A \to \mathbb{R}\)</span>. Sometimes it is a function of state only, then <span class="math inline">\(S \to \mathbb{R}\)</span></li>
</ul></li>
<li><p><strong>Goal of RL: maximize the expectation of discounted future rewards</strong> <span class="math display">\[E[R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2)...]\]</span></p></li>
<li><p><strong>Policy</strong> function: <span class="math inline">\(\pi : S \to A\)</span>, action given certain state. <span class="math display">\[a = \pi(s)\]</span></p></li>
<li><p><strong>Value function</strong> for a policy function <span class="math inline">\(\pi\)</span>: <span class="math display">\[ V^\pi(s) = E[R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2)... | s_0=s, \pi] \]</span></p></li>
<li><p><strong>Bellman Equation</strong>: Discrete value case</p>
<ul>
<li>immediate Reward + the expected future sums <span class="math display">\[\begin{aligned}
V^\pi(s) &amp;= E[R(s_0) + \gamma V^\pi(s&#39;) |s_0=s, \pi] \\
&amp;= R(s) + \gamma E[V^\pi(s&#39;) |s_0=s, s_1=s&#39;, \pi] \\
&amp;= R(s) + \gamma E_{s&#39;\sim P_{s\pi(s)}}[V^\pi(s&#39;)] \\
&amp;= R(s) + \gamma\sum_{s&#39;} P_{s\pi(s)}(s&#39;)V^\pi(s&#39;) 
\end{aligned}\]</span></li>
<li>Finite-state MDP (<span class="math inline">\(|S| &lt; \infty\)</span>) has linear solution.</li>
</ul></li>
<li><p>Optimal value function <span class="math display">\[V^*(s) = \max_\pi V^\pi(s) \]</span></p>
<p><span class="math display">\[V^*(s) = R(s) + \max_{a\in A} {\gamma\sum_{s&#39;} P_{sa(s)}(s&#39;)V^*(s&#39;)}\]</span></p>
<ul>
<li><p>equivalently optimize the discounted future rewards</p></li>
<li><p>define the optimal policy <span class="math inline">\(\pi^*\)</span> <span class="math display">\[\pi^*(s) = \argmax_{a\in A}{\sum_{s&#39;} P_{sa(s)}(s&#39;)V^*(s&#39;)}\]</span></p></li>
<li><p><span class="math inline">\(V^*(s) = V^{\pi^*}(s)\)</span>, the optimal value function is the value function under the optimal policy. Note that the optimal policy does not depend on the initial state.</p></li>
</ul></li>
</ul>
<h2 id="value-iteration-and-policy-iteration">2 Value Iteration and Policy Iteration</h2>
<p>For finding the optimal policy and the corresponding value.</p>
<h3 id="value-iteration">Value Iteration</h3>
<p>Consider finite <span class="math inline">\(S, A\)</span> MDPs. Assume that we know the <span class="math inline">\(P_{sa}\)</span> and the reward function <span class="math inline">\(R\)</span>:</p>
<p>For each state <span class="math inline">\(s\)</span>, initialize <span class="math inline">\(V(s) = 0\)</span>;</p>
<p>Until convergence, for every state, update <span class="math inline">\(V(s)\)</span> with optimal policy. <span class="math display">\[V(s) := R(s) + \max_{a\in A}{\gamma \sum_{s&#39;}{P_{sa}(s&#39;)V(s&#39;)}}\]</span></p>
<p>Synchronous update: each time map all states and then update;</p>
<p>Asynchronous: ...</p>
<p>Finally the value function would converge to optimal and so does the policy.</p>
<h3 id="policy-iteration">Policy iteration</h3>
<p>Initialize <span class="math inline">\(\pi\)</span> randomly,<br />
for until convergence,</p>
<p>update value function with policy <span class="math inline">\(\pi\)</span> (linear solution by Bellman equations)<br />
for each state <span class="math inline">\(s\)</span>, greedily optimize the <span class="math inline">\(\pi(s)\)</span></p>
<h2 id="learning-a-model-for-an-mdp">3 Learning a model for an MDP</h2>
<h3 id="estimate-p_sa-r">Estimate <span class="math inline">\(P_{sa}, R\)</span></h3>
<p><span class="math inline">\(S, A, \gamma\)</span> are known, but <span class="math inline">\(P_{sa}, R\)</span> are unknown.</p>
<p>We sample and estimate (MLE).</p>
<p>The probability distribution of <span class="math inline">\(s&#39;\)</span>, <span class="math inline">\(P_{sa}(s&#39;)\)</span> is the average ratio of transition <span class="math inline">\(s\to s&#39;\)</span>, in the discrete and finite case.</p>
<p>The immediate reward <span class="math inline">\(R(s)\)</span> is the average of samples.</p>
<h3 id="algorithm-for-mdp-combining-value-iteration-and-p_sa-estimation">Algorithm for MDP: combining value iteration and <span class="math inline">\(P_{sa}\)</span> estimation</h3>
<p>...</p>
<p>The value function does not have to be initialized with 0 after updating the <span class="math inline">\(P_{sa}\)</span>.</p>
<h2 id="continuous-state-mdps">4 Continuous state MDPs</h2>
<p><span class="math inline">\(S = \mathbb{R}^d\)</span> can be infinite: high-dimensional real space.</p>
<h3 id="discretization">Discretization</h3>
<p>When the dimensionality is low, up to 4d or even 6d, discretize is feasible.</p>
<p><strong>curse of dimensionality</strong>: complexity <span class="math inline">\(|S|^d\)</span>. Action space is usually lower-dimensional and can be discretized.</p>
<h3 id="value-function-approximation">Value function approximation</h3>
<p>If we get the expression of value function?</p>
<h4 id="modelsimulator">Model/simulator</h4>
<p>Assume we have a <strong>model/simulator</strong> for MDP. It maps <span class="math inline">\(s_t, a_t: s_{t+1}\)</span>. - It can be some <strong>physics simulation engine</strong>. - Or it can be learned through <strong>monte carlo simulation</strong>. <span class="math inline">\(s_{t+1} = f(s_t, a_t)\)</span> - <strong>Linear form</strong>: <span class="math inline">\(s_{t+1} = As_t + Ba_t\)</span> - Optimize the parameters by minimizing l2 loss - <span class="math inline">\(\argmin_{A,B} \sum_{i=1}^n\sum_{t=0}^{T-1} {\big|\big| s_{t+1}^{(i)} - \hat{s}_{t+1}^{(i)}\big|\big|^2 }\)</span> - Non-Linear form: use non-linear feature mappings <span class="math inline">\(\phi_s(s_t), \phi_a(a_t)\)</span></p>
<h4 id="fitted-value-iteration">Fitted value iteration</h4>
<p>Estimate: <span class="math inline">\(q(a) \sim R(s^{(i)} + \gamma E[V(s&#39;)])\)</span>, through averaging the results of k samples (k=1 is okay if the simulator is deterministic. single sample)</p>
<p>set <span class="math inline">\(y^{(i)} = \max_aq(a)\)</span>, estimate of optimal value,</p>
<p>Fit the value function, <span class="math inline">\(V(S^{(i)}) \sim y^{(i)}\)</span> using supervised learning. Update parameters.</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title>CS229 Notes3 SVM 笔记</title>
    <url>/2020/11/25/CS229-Notes3-SVM-%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="functionalgeometric-margin">Functional/Geometric Margin</h2>
<p>两者区别在于 <span class="math inline">\(w\)</span> 是否被标准化。效用边际会受到参数scale的影响。如果标准化了则两者等效。</p>
<h2 id="optimal-margin-classifier-primal">Optimal Margin Classifier (Primal)</h2>
<p>经过转换，问题变为一个QP问题，可以用一般优化器优化。</p>
<p>Primal:</p>
<p><span class="math display">\[\min_{w,b} \frac12 \|w\|^2 \\
\text{s.t.\quad} y^{(i)}(w^Tx^{(i)}+b) \geq 1\]</span></p>
<p>关于代码实现，感谢小胖子的建议，如果使用 <code>scipy.optimize.minimize</code>，</p>
<ul>
<li>首先，最值必然位于某顶点处，这是凸优化的性质，因而必然有至少一个 <span class="math inline">\(x^{(j)}\)</span> 使得约束条件tight，即取等号。即至少有一个点到平面的向量是支持向量</li>
<li>则b与w的关系构成等式，b可以用w线性表示</li>
<li>假设任意点是支持向量，该优化问题必然有最优解</li>
<li>那么遍历 N 个数据点，取<code>min</code>和<code>argmin</code>即可</li>
</ul>
<p>具体来说，假设</p>
<p><span class="math display">\[y^{(j)}(w^Tx^{(j)}+b) = 1\]</span></p>
<p><span class="math display">\[b = y^{(j)} - w^Tx^{(j)}\]</span></p>
<p>对于剩余的 <span class="math inline">\(N-1\)</span> 个数据点，</p>
<p><span class="math display">\[ y^{(i)}(w^Tx^{(i)}+ y^{(j)} - w^Tx^{(j)}) \geq 1\]</span></p>
<p><span class="math display">\[\to y^{(i)}w^T(x^{(i)}-x^{(j)}) + y^{(i)}y^{(j)} \geq 1\]</span></p>
<p>可以写作线性的约束。</p>
<!-- $$\begin{cases} 
w^T(x^{(i)}-x^{(j)}) + y^{(j)} \geq 1, y^{(i)} = 1 \\

w^T(x^{(i)}-x^{(j)}) + y^{(j)} \leq -1, y^{(i)} = -1
\end{cases}$$ -->
<h2 id="dual">Dual</h2>
<p>使用拉格朗日对偶，需要注意如果使用软间隔（见CS229-Notes3 Part8），在存在 regularization C 时，b的表达式变化，不再是式 (25) 的形式。因而在代码实现时，偷懒不考虑C了。</p>
<h2 id="kernel-svm">Kernel SVM</h2>
<p>有以下注意点： 1. 要对输入数据 X 一开始就做标准化。否则用 softmax 这种核函数量级直接爆炸。比如本数据中 <span class="math inline">\(\|x\| \approx27\)</span>， 想象一下<span class="math inline">\(e^{x^Ty}有多大\)</span> 2. 相当于把线性的所有<code>self.x</code>替换为<code>self.phi_x</code>。K的存在确实替代了 featuremap 的显示表达，然而 b 的表达式里仍然是 <span class="math inline">\(\phi(x)\)</span>，所以还是不得不给出 featuremap 的表达式。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title>CS229 Notes7b GMM Implementation</title>
    <url>/2020/11/07/CS229-Notes7b-GMM-Implementation/</url>
    <content><![CDATA[<h1 id="import-packages-mypca-mykmeans-and-iris-dataset.">Import packages + MyPCA + MyKMeans and Iris dataset.</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> tqdm.notebook <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> scipy.stats <span class="keyword">as</span> spst</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_spd_matrix</span><br><span class="line"><span class="keyword">from</span> scipy.special <span class="keyword">import</span> softmax</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">os.getcwd()</span><br></pre></td></tr></table></figure>
<pre><code>&#39;/content/drive/My Drive/CS229/Notes7b-GMM&#39;</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%cd /content/drive/My Drive/CS229/Notes7b-GMM</span><br></pre></td></tr></table></figure>
<pre><code>/content/drive/My Drive/CS229/Notes7b-GMM</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">iris.target_names</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># PCA</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myPCA</span>(<span class="params">X:np.ndarray, n_dimensions:int</span>):</span></span><br><span class="line">  <span class="comment"># N, d = X.shape</span></span><br><span class="line">  <span class="comment"># Centering</span></span><br><span class="line">  X_centered = X - X.mean(axis=<span class="number">0</span>) </span><br><span class="line"></span><br><span class="line">  <span class="comment"># Covariance Matrix of d*d</span></span><br><span class="line">  Sigma = np.dot(X_centered.T, X_centered)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># SVD</span></span><br><span class="line">  U, Lambda, V = np.linalg.svd(Sigma)</span><br><span class="line"></span><br><span class="line">  X_centered_PC = np.dot(U[:,:n_dimensions].T, X_centered.T).T</span><br><span class="line">  X_PC = X_centered_PC + X.mean(axis=<span class="number">0</span>)[:n_dimensions]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Purposely rescale and add negative sign to mimic Sklearn&#x27;s PCA</span></span><br><span class="line">  <span class="keyword">return</span> -(X_PC - X_PC.mean(axis=<span class="number">0</span>))/X_PC.std(axis=<span class="number">0</span>) </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myKMeans</span> (<span class="params">X: np.ndarray, k: int, iterations=<span class="number">100</span>, tol=<span class="number">0.001</span></span>):</span></span><br><span class="line">  N, d = X.shape</span><br><span class="line">  mu = X[np.random.choice(range(N), size=k)]</span><br><span class="line">  c = np.zeros(N)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> step <span class="keyword">in</span> tqdm(range(iterations)):</span><br><span class="line">    prev_mu = mu.copy()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">      c[i] = np.argmin(np.linalg.norm(X[i] - mu, axis=<span class="number">1</span>)**<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">      mu[j] = np.mean(X[np.arange(N)[c==j]], axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (np.sum(np.linalg.norm(prev_mu - mu,axis=<span class="number">1</span>)) &lt; tol):</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">  distortion = np.sum([np.sum(np.linalg.norm(X[np.arange(N)[c==i]] - mu[i], axis=<span class="number">1</span>)**<span class="number">2</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(k)])</span><br><span class="line">  print(<span class="string">f&quot;distortion: <span class="subst">&#123;distortion&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> c, mu, distortion</span><br></pre></td></tr></table></figure>
<h1 id="gmm">GMM</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">E_step</span>(<span class="params">x, phi, mu, Sigma</span>):</span></span><br><span class="line">    <span class="comment"># x: (N, d)</span></span><br><span class="line">    <span class="comment"># phi: (k,)</span></span><br><span class="line">    <span class="comment"># mu: (k,d)</span></span><br><span class="line">    <span class="comment"># Sigma: (k, d, d)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># w = post_z: (N, k)</span></span><br><span class="line">    </span><br><span class="line">    N, d = x.shape</span><br><span class="line">    k = phi.shape[<span class="number">0</span>]</span><br><span class="line">    post_z = np.zeros((N,k))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">        numerator = [spst.multivariate_normal.pdf(x[i], mean=mu[j], cov=Sigma[j]) * phi[j] <span class="keyword">for</span> j <span class="keyword">in</span> range(k)]</span><br><span class="line">        sum = np.sum(numerator)</span><br><span class="line">        post_z[i] = numerator / sum</span><br><span class="line">    <span class="keyword">return</span> post_z</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">M_step</span>(<span class="params">x, w</span>):</span></span><br><span class="line">    <span class="comment"># phi: (k,)</span></span><br><span class="line">    <span class="comment"># mu: (k, d)</span></span><br><span class="line">    <span class="comment"># Sigma: (k, d, d) </span></span><br><span class="line"></span><br><span class="line">    N, d = x.shape</span><br><span class="line">    _, k = w.shape</span><br><span class="line">    phi = np.mean(w, axis=<span class="number">0</span>)</span><br><span class="line">    mu = np.dot(w.T, x) / np.expand_dims(np.sum(w, axis=<span class="number">0</span>),axis=<span class="number">1</span>)    </span><br><span class="line">    Sigma = np.zeros((k,d,d))</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">        Sigma[j] = np.sum([w[i][j] * np.outer((x[i]-mu[j]), (x[i]-mu[j]).T) <span class="keyword">for</span> i <span class="keyword">in</span> range(N)], axis=<span class="number">0</span>) / np.sum(w[:, j])</span><br><span class="line">    <span class="keyword">return</span> phi, mu, Sigma</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myGMM</span> (<span class="params">X: np.ndarray, k: int, iterations=<span class="number">100</span>, tol=<span class="number">0.00001</span></span>):</span></span><br><span class="line">    N, d = X.shape</span><br><span class="line">    X_sample = X[[np.random.choice(range(N)) <span class="keyword">for</span> i <span class="keyword">in</span> range(k)]]</span><br><span class="line"></span><br><span class="line">    phi = softmax(np.random.random(size=k))</span><br><span class="line">    mu = X_sample</span><br><span class="line">    Sigma = np.concatenate([make_spd_matrix(d) <span class="keyword">for</span> i <span class="keyword">in</span> range(k)], axis=<span class="number">0</span>).reshape(k,d,d)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> tqdm(range(iterations)):</span><br><span class="line">        prev_phi = phi</span><br><span class="line">        w = E_step(X, phi, mu, Sigma)</span><br><span class="line">        phi, mu, Sigma = M_step(X, w)</span><br><span class="line">        <span class="keyword">if</span> np.sum(np.square(phi-prev_phi)) &lt; tol:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> w</span><br></pre></td></tr></table></figure>
<h1 id="compare-with-ground-truth-k-means">Compare with Ground Truth &amp; K-Means</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Ground Truth Label</span></span><br><span class="line">X_PC2 = myPCA(X, <span class="number">2</span>)</span><br><span class="line">target_ids = range(len(iris.target_names))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i, c, label <span class="keyword">in</span> zip(target_ids, <span class="string">&#x27;rgbcmykw&#x27;</span>, iris.target_names):</span><br><span class="line">  plt.scatter(X_PC2[y==i, <span class="number">0</span>], X_PC2[y==i, <span class="number">1</span>], c=c, label=label)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/11/07/CS229-Notes7b-GMM-Implementation/GMM_12_0.png" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># K-means with 3 clusters</span></span><br><span class="line">cluster, centroids, distortion = myKMeans(X, <span class="number">3</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i, c, label <span class="keyword">in</span> zip(range(<span class="number">3</span>), <span class="string">&#x27;rgbcmykw&#x27;</span>, range(<span class="number">3</span>)):</span><br><span class="line">  plt.scatter(X_PC2[cluster==i, <span class="number">0</span>], X_PC2[cluster==i, <span class="number">1</span>], c=c, label=label)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(FloatProgress(value=0.0), HTML(value=&#39;&#39;)))


distortion: 78.85144142614601</code></pre>
<p><img src="/2020/11/07/CS229-Notes7b-GMM-Implementation/GMM_13_2.png" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># GMM with 3 clusters</span></span><br><span class="line">w = myGMM(X, <span class="number">3</span>)</span><br><span class="line">cluster_GMM = np.argmax(w,axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i, c, label <span class="keyword">in</span> zip(range(<span class="number">3</span>), <span class="string">&#x27;rgbcmykw&#x27;</span>, range(<span class="number">3</span>)):</span><br><span class="line">  plt.scatter(X_PC2[cluster_GMM==i, <span class="number">0</span>], X_PC2[cluster_GMM==i, <span class="number">1</span>], c=c, label=label)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(FloatProgress(value=0.0), HTML(value=&#39;&#39;)))</code></pre>
<p><img src="/2020/11/07/CS229-Notes7b-GMM-Implementation/GMM_14_2.png" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># GMM with 3 clusters</span></span><br><span class="line">w = myGMM(X, <span class="number">3</span>)</span><br><span class="line">cluster_GMM = np.argmax(w,axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i, c, label <span class="keyword">in</span> zip(range(<span class="number">3</span>), <span class="string">&#x27;rgbcmykw&#x27;</span>, range(<span class="number">3</span>)):</span><br><span class="line">  plt.scatter(X_PC2[cluster_GMM==i, <span class="number">0</span>], X_PC2[cluster_GMM==i, <span class="number">1</span>], c=c, label=label)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(FloatProgress(value=0.0), HTML(value=&#39;&#39;)))</code></pre>
<p><img src="/2020/11/07/CS229-Notes7b-GMM-Implementation/GMM_15_1.png" /></p>
<h1 id="remark">Remark</h1>
<p>Remark: GMM is more volatile than K-Means and the evaluation metric is said to be BIC. Let's deal with this later.</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title>CS229 Notes9 Lagrange Multipliers and Factor Analysis</title>
    <url>/2020/11/03/CS229-Notes9-Lagrange-Multipliers-and-Factor-Analysis/</url>
    <content><![CDATA[<h1 id="lagrange-multiplier">Lagrange Multiplier</h1>
<p>说明拉格朗日乘数函数的关键点同时也是原函数的关键点</p>
<p>Consider maximize a function with constraints <span class="math inline">\(f(x), s.t. Ax=b\)</span></p>
<p><span class="math inline">\(A: \mathbb{R}^{n\times d}, x: \mathbb{R}^d, b: \mathbb{R}^n, f: \mathbb{R}^d \to \mathbb{R}\)</span></p>
<h2 id="finding-the-space-of-solutions">Finding the space of solutions</h2>
<p>Assume <span class="math inline">\(n &gt;&gt; d\)</span>. If <span class="math inline">\(A\)</span> is column full rank, then <span class="math inline">\(x_0\)</span> is unique. Let <span class="math inline">\(k=d-r&gt;0\)</span>, the null space <span class="math inline">\(\mathbb{R}^{d\times k}\)</span> has basis</p>
<p><span class="math display">\[U = [u_1, ..., u_k] \in \mathbb{R}^{d\times k}\]</span></p>
<p>The solution to <span class="math inline">\(Ax=b\)</span> can be written with free parameters <span class="math display">\[x = x_0 + Uy\]</span></p>
<p>Consider the function <span class="math inline">\(g\)</span></p>
<p><span class="math display">\[g(y) = f(x_0+Uy), g:\mathbb{R}^k \to \mathbb{R}\]</span></p>
<p><span class="math display">\[\nabla_yg(y) = 0 \to U^T\nabla f(x_0+Uy) = \nabla f(x) = 0\]</span></p>
<p>Since <span class="math inline">\(U\)</span> is the null space of A, <span class="math inline">\(\nabla f(x)\)</span> should be in the row space of A.</p>
<p><span class="math display">\[\therefore \nabla f(x) + A^T \mu = 0, \mu \in \mathbb{R}^n\]</span></p>
<p>for a certain <span class="math inline">\(x\)</span> that <span class="math inline">\(Ax=b\)</span></p>
<h2 id="the-clever-langrangianshow-that-the-critical-points-of-the-constrained-function-f-are-critical-points-of-lx-mu">The Clever Langrangian(show that the critical points of the constrained function <span class="math inline">\(f\)</span> are critical points of <span class="math inline">\(L(x, \mu)\)</span>)</h2>
<p><span class="math display">\[L(x,\mu) = f(x) + \mu^T(Ax-b)\]</span></p>
<p>Normal equation:</p>
<p><span class="math display">\[\nabla_xL(x,\mu) = \nabla f(x) + A^T\mu = 0\]</span></p>
<p><span class="math display">\[\nabla_\mu L(x,\mu) = Ax-b = 0\]</span></p>
<p>First equation is equivalent to the normal equation for <span class="math inline">\(g\)</span>. The second one just says that the constraint be statisfied.</p>
<p>So if <span class="math inline">\(x\)</span> is critical point for <span class="math inline">\(f\)</span>, <span class="math inline">\((x,\mu)\)</span> is the critical point for <span class="math inline">\(L\)</span>.</p>
<h1 id="factor-analysis">Factor analysis</h1>
<p>稀疏数据的分布估计、EM</p>
<h2 id="marginals-and-conditions-of-sigma">marginals and conditions of <span class="math inline">\(\Sigma\)</span></h2>
<p>A random variable</p>
<p><span class="math display">\[x = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}\]</span></p>
<p>where <span class="math inline">\(x_1 \in \mathbb{R}^r, x_2 \in \mathbb{R}^s, x \in \mathbb{R}^{r+s}\)</span></p>
<p>Suppose <span class="math inline">\(x \sim N(\mu, \Sigma)\)</span>, where</p>
<p><span class="math display">\[\mu = \begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix}, \Sigma = \begin{bmatrix} \Sigma_{11} \quad \Sigma_{12} \\ \Sigma_{21} \quad \Sigma_{22}\end{bmatrix}\]</span></p>
<p><strong>The conditional distribution <span class="math inline">\(x_1|x_2\)</span></strong></p>
<p><span class="math display">\[\mu_{1|2} = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2)\]</span></p>
<p><span class="math display">\[\Sigma_{1|2} = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}\]</span></p>
<h2 id="the-factor-analysis-model">The Factor analysis model</h2>
<p><strong>Posit</strong> a joint distribution <span class="math inline">\((x,z)\)</span> with <span class="math inline">\(z\in \mathbb{R}^k\)</span> being the latent random variable</p>
<p><span class="math display">\[z \sim N(0,I)\]</span></p>
<p><span class="math display">\[x|z \sim N(\mu+\Lambda z, \Psi)\]</span></p>
<p>Here, <span class="math inline">\(\mu \in \mathbb{R}^{d}, \Lambda \in \mathbb{R}^{d\times k}, \Psi \in \mathbb{R}^{d\times d}\)</span>. Usually <span class="math inline">\(k&lt;d\)</span>, which means <span class="math inline">\(z\)</span> is a low-dimensional to <span class="math inline">\(x\)</span>.</p>
<p><strong>Define the factor analysis model</strong> according to:</p>
<p><span class="math display">\[z \sim N(0, I)\]</span></p>
<p><span class="math display">\[\epsilon \sim N(0, \Psi)\]</span></p>
<p><span class="math display">\[x = \mu + \Lambda z + \epsilon\]</span></p>
<p>Remark: Map k-dimension multivariate Gaussian <span class="math inline">\(z\)</span> to a d-dimensional affine space of <span class="math inline">\(\mathbb{R}^d\)</span></p>
<p><strong>Assertion: <span class="math inline">\((z,x)\)</span> have a joint Gaussian distribution:</strong></p>
<p><span class="math display">\[ \begin{bmatrix} z \\ x \end{bmatrix} \sim N(\mu_{zx}, \Sigma )\]</span></p>
<p>Then find <span class="math inline">\(\mu_{zx}\)</span> and <span class="math inline">\(\Sigma\)</span></p>
<p><span class="math display">\[E[x] = \mu\]</span></p>
<p><span class="math display">\[E[z] = 0\]</span></p>
<p><span class="math display">\[\therefore \mu_{zx} = \begin{bmatrix} \vec{0} \\ \mu \end{bmatrix}\]</span></p>
<p>To find <span class="math inline">\(\Sigma\)</span>, calculate its subcomponents <span class="math inline">\(\Sigma_{zz}, \Sigma_{xx}, \Sigma_{xz}\)</span></p>
<p><span class="math display">\[\Sigma_{zz} = cov(z) = I\]</span></p>
<p><span class="math display">\[\begin{aligned} \Sigma_{xz} &amp;= E[(z-Ez)(x-Ex)^T]\\
&amp;= E[z(\Lambda z + \epsilon)^T]\\
&amp;= E[zz^T]\Lambda^T + E[z\epsilon^T]\\
&amp;= \Lambda^T
\end{aligned}\]</span></p>
<p><span class="math display">\[\Sigma_{xx} = \Lambda\Lambda^T + \Psi \]</span></p>
<p>Putting everything together,</p>
<p><span class="math display">\[ \begin{bmatrix} z \\ x \end{bmatrix} \sim N(\begin{bmatrix} \vec{0} \\ \mu \end{bmatrix}, \begin{bmatrix} &amp;I \quad &amp;\Lambda^T \\ &amp;\Lambda \quad &amp;\Lambda\Lambda^T + \Psi  \end{bmatrix} )\]</span></p>
<p>The marginal distribution of x is <span class="math inline">\(x\sim N(\mu, \Lambda\Lambda^T+\Psi)\)</span>.</p>
<p>We can write out the log-likelihood of this multivariate gaussian. To optimize the MLE, we have to use EM.</p>
<p><span class="math display">\[l(\mu, \Lambda, \Psi) = \log \prod_{i=1}^{n} \frac1{(2\pi)^{d/2} |\Lambda\Lambda^T + \Psi|^{1/2} } \exp \bigg(  -\frac12 (x^{(i)}-\mu)^T(\Lambda\Lambda^T+\Psi)^{-1}(x^{(i)}-\mu)   \bigg)  \]</span></p>
<h2 id="em-for-factor-analysis">EM for factor analysis</h2>
<h3 id="the-e-step">The E-step</h3>
<p><span class="math display">\[Q_i(z^{(i)}) = p(z_i|x^{(i)}, \mu, \Lambda, \Psi)\]</span></p>
<p>Given the joint distribution of <span class="math inline">\((z,x)\)</span> and the conditional distribution formula of a gaussian,</p>
<p>太长略...</p>
<h2 id="讨论">讨论</h2>
<p>本篇notes开头讲的是 <span class="math inline">\(n&lt;d\)</span>时，数据集不足以估计多元高斯分布的协方差矩阵<span class="math inline">\(\Sigma\)</span>，即该矩阵奇异。</p>
<p>第一种方法限制 <span class="math inline">\(\Sigma\)</span> 的形式，使其为对角矩阵。因为数据不够填满整个协方差矩阵，就只关注各分量坐标的各自方差。这样只要<span class="math inline">\(n \geq d+1\)</span>， <span class="math inline">\(\Sigma\)</span> 就非奇异了。 这种方法完全忽略了各分量间相关性。</p>
<p>第二种方法Factor Analysis则是假设高维(d)高斯随机量 <span class="math inline">\(x\)</span> 是低维(k)高斯随机量 <span class="math inline">\(z\)</span> 投射到高维并加上 <span class="math inline">\(x\)</span> 的均值、随机噪声 <span class="math inline">\(\epsilon\)</span> 得到。同时 <span class="math inline">\(z,x\)</span> 的vertical stack 也是一个 (d+k) 高斯分布随机量。<strong>注意到这种形式和 OLS 的做法类似，这里的参数就是仿射矩阵 <span class="math inline">\(\Lambda\)</span> 和噪音 <span class="math inline">\(\epsilon\)</span>。</strong></p>
<p>如此，<span class="math inline">\(x\)</span> 的边际分布 <span class="math inline">\(x\sim N(\mu, \Lambda \Lambda^T, \Psi)\)</span>. 这样相比于原来的整个<span class="math inline">\(d\times d\)</span>协方差矩阵实现了降维，从而可能以更少的数据量实现估计。</p>
<p>本章一块比较重要的内容是， 两个高斯随机量vstack形成的一个更高维的高斯随机量的mean/var，以及条件分布<span class="math inline">\(x_1|x_2\)</span>也是个高斯分布，其mean/var的表达式。</p>
<p><span class="math display">\[\mu_{1|2} = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2)\]</span></p>
<p><span class="math display">\[\Sigma_{1|2} = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}\]</span></p>
<p>在EM for factor analysis中， E-step的 <span class="math inline">\(z|x\)</span> 的分布就是这么得出的。</p>
<p>M-step 的推导非常tricky(太长不看)。最有意思的一部分推导结果是，对于参数 仿射矩阵 <span class="math inline">\(\Lambda\)</span>， 可以直接normal equation一步求得，其表达式有与 OLS参数 <span class="math inline">\(\theta\)</span> 相似的形式。</p>
<p><em>于11月3日凌晨</em></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title>CS229-ProbabilityTheory Recap</title>
    <url>/2020/10/25/CS229-ProbabilityTheory-Recap/</url>
    <content><![CDATA[<h3 id="some-common-random-variables">Some common random variables</h3>
<ul>
<li>Discrete random variables
<ul>
<li>Bernoulli(p)</li>
<li>Binomial(n, p)</li>
<li>Geometric(p)</li>
<li>Poisson(<span class="math inline">\(\lambda\)</span>): <span class="math inline">\(p(x) = e^{-\lambda}\frac{\lambda^x}{x!}\)</span>, non-negative integers</li>
</ul></li>
<li>Continuous random variables
<ul>
<li>Uniform(a,b)</li>
<li>Exponential(<span class="math inline">\(\lambda\)</span>): <span class="math inline">\(f(x) = \lambda e^{-\lambda x}, x\geq 0\)</span>; <span class="math inline">\(F(x) = e^{-\lambda x}, x\geq 0\)</span></li>
<li>Normal(<span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma^2\)</span>)</li>
</ul></li>
</ul>
<p>Comment: some simulation methods 1. Inverse CDF technique : <span class="math inline">\(X = F^{-1}(U), U\sim unif(0,1)\)</span> 2. <a href="https://theclevermachine.wordpress.com/2012/09/11/sampling-from-the-normal-distribution-using-the-box-muller-transform/">Box Muller</a> method for generating Gaussian</p>
<h3 id="two-random-variables">Two random variables</h3>
<h4 id="expectation-covariance">Expectation &amp; Covariance</h4>
<p><span class="math display">\[Cov[x,y] = E[(x-E[x])(y-E[y])] = E[XY] - E[X]E[Y]\]</span></p>
<p>Properties: - Linearity of expectation - <span class="math inline">\(Var[X+Y] = Var[X] + Var[Y] + 2Cov[X,Y]\)</span> - If independent, <span class="math inline">\(Cov[X,Y]=0\)</span> - If Independet, <span class="math inline">\(E[XY] = E[X]E[Y]\)</span></p>
<h3 id="multiple-random-variables">Multiple random variables</h3>
<h4 id="random-vectors">Random vectors</h4>
<p>Vectorized denotation: <span class="math inline">\(X\)</span></p>
<p><strong>Covariance matrix</strong>: <strong>PSD</strong> and <strong>symmetric</strong> <span class="math display">\[\Sigma = E[XX^T] - E[X]E[X]^T = E[(X-E[X])(X-E[X])^T]\]</span></p>
<h4 id="multivariate-gaussian-distribution">Multivariate Gaussian distribution</h4>
<p><span class="math display">\[X\in \mathbb{R}^n \sim N(\mu, \Sigma)\]</span></p>
<p><span class="math display">\[f_X(X;\mu;\Sigma) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp{\bigg(-\frac12(X-\mu)^T\Sigma^{-1}(X-\mu)\bigg)}
\]</span></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title>CS229 Notes7a The k-means clustering algorithm</title>
    <url>/2020/11/04/CS229-Notes7a-The-k-means-clustering-algorithm/</url>
    <content><![CDATA[<h1 id="implement-k-means">Implement k-means</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> tqdm.notebook <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">iris.target_names</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"><span class="comment"># K clusters</span></span><br><span class="line">k = <span class="number">3</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># PCA</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myPCA</span>(<span class="params">X:np.ndarray, n_dimensions:int</span>):</span></span><br><span class="line">  <span class="comment"># N, d = X.shape</span></span><br><span class="line">  <span class="comment"># Centering</span></span><br><span class="line">  X_centered = X - X.mean(axis=<span class="number">0</span>) </span><br><span class="line"></span><br><span class="line">  <span class="comment"># Covariance Matrix of d*d</span></span><br><span class="line">  Sigma = np.dot(X_centered.T, X_centered)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># SVD</span></span><br><span class="line">  U, Lambda, V = np.linalg.svd(Sigma)</span><br><span class="line"></span><br><span class="line">  X_centered_PC = np.dot(U[:,:n_dimensions].T, X_centered.T).T</span><br><span class="line">  X_PC = X_centered_PC + X.mean(axis=<span class="number">0</span>)[:n_dimensions]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Purposely rescale and add negative sign to mimic Sklearn&#x27;s PCA</span></span><br><span class="line">  <span class="keyword">return</span> -(X_PC - X_PC.mean(axis=<span class="number">0</span>))/X_PC.std(axis=<span class="number">0</span>) </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myKMeans</span> (<span class="params">X: np.ndarray, k: int, iterations=<span class="number">100</span>, tol=<span class="number">0.001</span></span>):</span></span><br><span class="line">  N, d = X.shape</span><br><span class="line">  mu = X[np.random.choice(range(N), size=k)]</span><br><span class="line">  c = np.zeros(N)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> step <span class="keyword">in</span> tqdm(range(iterations)):</span><br><span class="line">    prev_mu = mu.copy()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">      c[i] = np.argmin(np.linalg.norm(X[i] - mu, axis=<span class="number">1</span>)**<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">      mu[j] = np.mean(X[np.arange(N)[c==j]], axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (np.sum(np.linalg.norm(prev_mu - mu,axis=<span class="number">1</span>)) &lt; tol):</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">  distortion = np.sum([np.sum(np.linalg.norm(X[np.arange(N)[c==i]] - mu[i], axis=<span class="number">1</span>)**<span class="number">2</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(k)])</span><br><span class="line">  print(<span class="string">f&quot;distortion: <span class="subst">&#123;distortion&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> c, mu, distortion</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Ground Truth Label</span></span><br><span class="line">X_PC2 = myPCA(X, <span class="number">2</span>)</span><br><span class="line">target_ids = range(len(iris.target_names))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i, c, label <span class="keyword">in</span> zip(target_ids, <span class="string">&#x27;rgbcmykw&#x27;</span>, iris.target_names):</span><br><span class="line">  plt.scatter(X_PC2[y==i, <span class="number">0</span>], X_PC2[y==i, <span class="number">1</span>], c=c, label=label)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/11/04/CS229-Notes7a-The-k-means-clustering-algorithm/KMeans_4_0.png" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># K-means with 3 clusters</span></span><br><span class="line">cluster, centroids, distortion = myKMeans(X, <span class="number">3</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i, c, label <span class="keyword">in</span> zip(range(<span class="number">3</span>), <span class="string">&#x27;rgbcmykw&#x27;</span>, range(<span class="number">3</span>)):</span><br><span class="line">  plt.scatter(X_PC2[cluster==i, <span class="number">0</span>], X_PC2[cluster==i, <span class="number">1</span>], c=c, label=label)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(FloatProgress(value=0.0), HTML(value=&#39;&#39;)))


distortion: 78.85566582597731</code></pre>
<p><img src="/2020/11/04/CS229-Notes7a-The-k-means-clustering-algorithm/KMeans_5_2.png" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cluster, centroids, distortion = myKMeans(X, <span class="number">4</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i, c, label <span class="keyword">in</span> zip(range(<span class="number">4</span>), <span class="string">&#x27;rgbcmykw&#x27;</span>, range(<span class="number">4</span>)):</span><br><span class="line">  plt.scatter(X_PC2[cluster==i, <span class="number">0</span>], X_PC2[cluster==i, <span class="number">1</span>], c=c, label=label)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(FloatProgress(value=0.0), HTML(value=&#39;&#39;)))


distortion: 57.25600931571816</code></pre>
<p><img src="/2020/11/04/CS229-Notes7a-The-k-means-clustering-algorithm/KMeans_6_2.png" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cluster, centroids, distortion = myKMeans(X, <span class="number">5</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i, c, label <span class="keyword">in</span> zip(range(<span class="number">5</span>), <span class="string">&#x27;rgbcmykw&#x27;</span>, range(<span class="number">5</span>)):</span><br><span class="line">  plt.scatter(X_PC2[cluster==i, <span class="number">0</span>], X_PC2[cluster==i, <span class="number">1</span>], c=c, label=label)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note that K-means is susceptible to local minima</span></span><br><span class="line"><span class="comment"># Select the result with minimal distortion.</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(FloatProgress(value=0.0), HTML(value=&#39;&#39;)))


distortion: 46.44618205128205</code></pre>
<p><img src="/2020/11/04/CS229-Notes7a-The-k-means-clustering-algorithm/KMeans_7_2.png" /></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title>Data-Mining-Final</title>
    <url>/2020/12/04/Data-Mining-Final/</url>
    <content><![CDATA[<h1 id="data-mining-final">Data Mining Final</h1>
<p>Hua Yao, UNI:hy2632</p>
<h2 id="problem-1-reducing-the-variance-of-the-monte-carlo-estimators-50-points">Problem 1: Reducing the variance of the Monte Carlo estimators [50 points]</h2>
<h3 id="proposed-estimator">Proposed estimator</h3>
<p>Here we propose an estimator using antithetic sampling for variance reduction.</p>
<p><span class="math display">\[\hat{X&#39;} = \frac1{m} \sum_{i=1}^{m}{ \frac{f(g_i^\top z) + f(-g_i^\top z)}2} \]</span></p>
<p>Still <span class="math inline">\(g_i \stackrel{\text{iid}}\sim \mathcal{N}(0, I_d)\)</span>.</p>
<h3 id="proof-unbiased">Proof: Unbiased</h3>
<p>The unbiasedness is evident.</p>
<p><span class="math display">\[\begin{aligned}
E[\hat{X&#39;}] &amp;= E\bigg[\frac1{m} \sum_{i=1}^{m}{ \frac{f(g_i^\top z) + f(-g_i^\top z)}2} \bigg] \\
&amp;=\frac12 \big(E_{g\sim \mathcal{N}(0, I_d)}[f(g^\top z) ]  + E_{g\sim \mathcal{N}(0, I_d)}[f(-g^\top z) ] \big) \\
&amp;=\frac12 \big(E_{g\sim \mathcal{N}(0, I_d)}[f(g^\top z) ]  + E_{g\sim \mathcal{N}(0, I_d)}[f(g^\top z) ] \big)  \quad\quad\quad\quad\quad\quad \because -g\sim \mathcal{N}(0, I_d) \\
&amp;= E_{g\sim \mathcal{N}(0, I_d)}[f(g^\top z) ] \\
&amp;= E[X]
\end{aligned}\]</span></p>
<h3 id="proof-strictly-lower-variance">Proof: Strictly lower variance</h3>
<p>Denote <span class="math inline">\(x = g^\top z \in \mathbb{R}\)</span>, <span class="math inline">\(X=f(x)\)</span>, <span class="math inline">\(x\)</span> has a normal distribution parameterized by <span class="math inline">\(z\)</span>, from the property of multivariate normal distribution.</p>
<p><span class="math display">\[x\sim N(0, \sum_{i=1}^d{|z_i|})\]</span></p>
<p>Then the expectation w.r.t <span class="math inline">\(g\)</span> can also be written as expectation w.r.t <span class="math inline">\(x\)</span>. For an arbitrary function <span class="math inline">\(F\)</span>,</p>
<p><span class="math display">\[E_g[F(g^\top z)] = E_x[F(x)]\]</span></p>
<p>The variance of estimators</p>
<p><span class="math display">\[\begin{aligned}
Var(\hat{X}) &amp;=  Var \bigg[  \frac1{m} \sum_{i=1}^{m}{ f(g_i^\top z) }  \bigg]\\
&amp;= \frac1{m} Var \big[ f(x) \big]  \\
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
Var(\hat{X&#39;}) &amp;= Var \bigg[  \frac1{m} \sum_{i=1}^{m}{ \frac{f(g_i^\top z) + f(-g_i^\top z)}2}  \bigg]\\
&amp;= \frac1{m} Var \bigg[ \frac{f(x) + f(-x)}2 \bigg] \\
&amp;= \frac1{4m} \bigg( Var\big[f(x)\big] + Var\big[f(-x)\big] + 2Cov\big[f(x), f(-x)\big]   \bigg)\\
&amp;= \frac1{2m}\bigg(Var\big[f(x)\big] + Cov\big[f(x), f(-x)\big]   \bigg)
\end{aligned}\]</span></p>
<p>To prove that <span class="math inline">\(Var(\hat{X&#39;}) &lt; Var(\hat{X})\)</span>, we only need to prove that <span class="math inline">\(Cov\big[f(x), f(-x)\big] &lt; Var\big[f(x)\big]\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
Cov\big[f(x), f(-x)\big] &amp;= E\big[f(x)f(-x)\big] - E\big[f(x)\big]E\big[f(-x)\big] \\
&amp;= E\big[f(x)f(-x)\big] - E\big[f(x)\big]^2\\
\end{aligned}\]</span></p>
<p>Here,</p>
<p><span class="math display">\[E\big[f(-x)\big] = E\big[f(x)\big]\]</span></p>
<p>because from above, <span class="math inline">\(x\sim N(0, \sum_{i=1}^d{|z_i|})\)</span>, <span class="math inline">\(-x\)</span> has the same distribution with <span class="math inline">\(x\)</span>.</p>
<p><span class="math display">\[Var\big[f(x)] = E\big[f^2(x)\big] - E\big[f(x)\big]^2\]</span> <span class="math display">\[\begin{aligned}
Cov\big[f(x), f(-x)\big] - Var\big[f(x)] &amp;= E\big[f(x)f(-x)\big] - E\big[f^2(x)\big] \\
&amp;=  E\big[f(x)f(-x) - f^2(x)\big]\\
&amp;= \int_{-\infty}^{\infty}{p(x)f(x)[f(-x)-f(x)]dx}
\end{aligned}\]</span></p>
<p>We know that <span class="math inline">\(f\)</span>'s taylor expansion has positive coefficients. Then it can be denoted as the sum of even and odd parts <span class="math display">\[f(x) = o(x) + e(x)\]</span></p>
<p><span class="math inline">\(p(x)\)</span> of normal distribution is even.</p>
<p><span class="math display">\[\begin{aligned}
\int_{-\infty}^{\infty}{p(x)f(x)[f(-x)-f(x)]dx} 
&amp;= \int_{-\infty}^{\infty}{p(x)[o(x) + e(x)][-2o(x)]dx} \\
&amp;= -2\int_{-\infty}^{\infty}{p(x)[o^2(x) + o(x)e(x)]dx} \\
&amp;= -2\int_{-\infty}^{\infty}{p(x)o^2(x) dx} \qquad \text{drop the odd part in integration} \\
&amp;&lt; 0 \qquad \text{strictly less than 0 because o(x) has positive coefficients} \\
\end{aligned}\]</span></p>
<p><span class="math display">\[\therefore Cov\big[f(x), f(-x)\big] &lt; Var\big[f(x)]\]</span> <span class="math display">\[\therefore Var(\hat{X&#39;})  &lt; Var(\hat{X}) \]</span></p>
<h2 id="problem-2-svm-with-dynamically-changing-labels-30-points">Problem 2: SVM with dynamically changing labels [30 points]</h2>
<h3 id="known-conditions">Known conditions</h3>
<p>For given time <span class="math inline">\(t\)</span>, from the derivation in the lagrangian program, we get</p>
<p><span class="math display">\[\begin{aligned}
w_t &amp;= \sum_{i=1}^{n}\alpha_{t}^{(i)}y_t^{(i)}x^{(i)}\\
&amp;= (\alpha_t * y_t) \cdot x
\end{aligned}\]</span></p>
<p><span class="math inline">\(w_t: (N,), \alpha_t : (N,), y_t: (N,), x: (N,d)\)</span></p>
<p>And</p>
<p><span class="math display">\[ \sum_{i=1}^{n}\alpha_{t}^{(i)}y_t^{(i)} = 0\]</span></p>
<h3 id="sufficient-condition-for-the-statement-to-hold">Sufficient condition for the statement to hold</h3>
<p>When <span class="math inline">\(t \in \{0,...N\}\)</span>, vectorized version</p>
<p><span class="math display">\[\begin{aligned}
\begin{bmatrix}w_1 \\...\\w_N \end{bmatrix}
&amp;= \begin{bmatrix}\alpha_1*y_1 \\...\\\alpha_N*y_N \end{bmatrix}\cdot x \\
&amp;=  \begin{bmatrix}\alpha_1^{(1)}y_1^{(1)}, ...,\alpha_1^{(N)}y_1^{(N)}  \\...\\\alpha_N^{(1)}y_N^{(1)}, ...,\alpha_N^{(N)}y_N^{(N)} \end{bmatrix}_{N\times N}\cdot \begin{bmatrix}x^{(1)} \\...\\x^{(N)} \end{bmatrix}
\end{aligned}\]</span></p>
<p>Denote the <span class="math inline">\(N\times N\)</span> matrix on the left as <span class="math inline">\(A\)</span>. Then</p>
<p><span class="math display">\[w_t = A_t\cdot x\]</span></p>
<p>where</p>
<p><span class="math display">\[x = \begin{bmatrix}x^{(1)} \\...\\x^{(N)} \end{bmatrix}_{N\times d}\]</span></p>
<p>if <span class="math inline">\(w_t\)</span> is the linear combination of all other <span class="math inline">\(w\)</span> vectors,</p>
<p><span class="math display">\[w_t = \sum_{j\in \{1,...N\}/t}{c_jw_j}\]</span></p>
<p><span class="math display">\[\Leftrightarrow A_tx = \sum_{j\in \{1,...N\}/t}{c_j A_jx}\]</span> <span class="math display">\[\Leftrightarrow (A_t-\sum_{j\in \{1,...N\}/t}{c_j A_j})x=0\]</span></p>
<p><span class="math inline">\(\therefore\)</span>The <strong>sufficient condition</strong> that <span class="math inline">\(w_t\)</span> can be the linear combination of all other <span class="math inline">\(w\)</span> vectors is</p>
<p><span class="math display">\[A_t = \sum_{j\in \{1,...N\}/t}{c_j A_j}\]</span></p>
<p>This property is namely the linearly dependence of <span class="math inline">\(A\)</span>'s rows, i.e. <strong><span class="math inline">\(A\)</span> is rank-deficient</strong>.</p>
<h3 id="proof-a-is-rank-deficient">Proof: <span class="math inline">\(A\)</span> is rank-deficient</h3>
<p>Using the fact that <span class="math inline">\(\sum_{i=1}^{n}\alpha_{t}^{(i)}y_t^{(i)} = 0\)</span>, we can rewrite the rightmost entry of each row of <span class="math inline">\(A\)</span> as negative the sum of all other entries in the row.</p>
<p><span class="math display">\[\begin{aligned}
\begin{bmatrix}\alpha_1^{(1)}y_1^{(1)}, ...,\alpha_1^{(N-1)}y_1^{(N-1)}, -\sum_{i=1}^{N-1}{\alpha_1^{(i)}y_1^{(i)}}  \\...\\\alpha_N^{(1)}y_N^{(1)}, ...,\alpha_N^{(N-1)}y_N^{(N-1)} , -\sum_{i=1}^{N-1}{\alpha_N^{(i)}y_N^{(i)}} \end{bmatrix}
\end{aligned}\]</span></p>
<p>Obviously, the rightmost column of <span class="math inline">\(A\)</span> is a linear combination of other columns, and the coefficients are all <span class="math inline">\(-1\)</span>,</p>
<p><span class="math display">\[A_{:, N} = -\sum_{j=1}^{N-1}{A_{:,j}}\]</span></p>
<p>Then the column rank of <span class="math inline">\(A\)</span> is strictly less than <span class="math inline">\(N\)</span>, A is rank-deficient.</p>
<p>Therefore, there exists <span class="math inline">\(w_t\)</span> which is a linear combination of all other vectors.</p>
<h2 id="problem-3-gradient-explodingvanishing-problems-in-neural-network-training-20-points">Problem 3: Gradient exploding/vanishing problems in neural network training [20 points]</h2>
<h3 id="explodingvanishing-gradients-and-why-it-affects-deep-nn">Exploding/vanishing gradients and why it affects deep NN</h3>
<p>In the simplest case where there are only linear transformations (without bias), denote the weight matrix between <span class="math inline">\(t\)</span>-th and <span class="math inline">\((t+1)\)</span>-th as <span class="math inline">\(A\)</span>.</p>
<p><span class="math display">\[\frac{\partial \ell}{\partial a_t} = A^T \frac{\partial \ell}{\partial a_{t+1}}\]</span></p>
<p>Then from the chain rule, partial derivative of the k-th hidden state and that of the loss satisfy:</p>
<p><span class="math display">\[\frac{\partial \ell}{\partial a_k} = \frac{\partial \ell}{\partial a_{L}} \cdot A_L^T A_{L-1}^T...A_K^T\]</span></p>
<p>The product of matrices causes exploding/vanishing gradients. If the eigenvalues of these matrices are not restricted to be <span class="math inline">\(\plusmn 1\)</span>, the product of these matrices might have unbounded eigenvalues, especially when the number of <span class="math inline">\(A\)</span>s is large (which means the early layers suffer more from vanishing/exploding gradient problem).</p>
<p>Then when we do backpropagation using the formula above, the gradient for updating the parameters could be 0 (vanishing gradient) or <span class="math inline">\(\infty\)</span> (exploding gradient). The NN actually loses track of the gradient information of the loss function and we cannot update the parameters effectively.</p>
<h3 id="methods-for-handling-this-problem">Methods for handling this problem</h3>
<h4 id="using-orthogonal-weight-matrices-in-linear-tranformations">Using orthogonal weight matrices in linear tranformations</h4>
<p>The eigenvalues of orthogonal matrix are all <span class="math inline">\(\plusmn 1\)</span>. The product of orthogonal matrices is also orthogonal and has <span class="math inline">\(\plusmn 1\)</span> eigenvalues and thus avoids exploding/vanishing gradient problem.</p>
<p>To parameterize the orthogonal matrix, we can use unrestricted parameterizations(<span class="math inline">\(|a_{ij}|\leq1\)</span>), Givens Rotations matrices, or Riemannion optimization.</p>
<h4 id="using-residual-network">Using Residual Network</h4>
<p>In many residual neural networks like ResNet, the shortcut skips some layers (especially linear layers). This help alleviate the problem of vanishing gradient.</p>
<h4 id="choice-in-activation-function">Choice in activation function</h4>
<p>Previously we mainly talked about the vanishing/exploding gradient problem in the setting of linear layers. In neural networks with activation functions, the chain rule formula also consists of the partial derivatives of these non-linear transformations.</p>
<p>Some activation functions might cause vanishing gradient by itself. For example, the partial derivative of sigmoid function is less than 0.25. Then such activations would make gradient smaller.</p>
<p>Therefore, ReLU and its variants (LeakyReLU, etc) is better in this respect.</p>
<h2 id="problem-4-epsilon-close-estimators-of-kernels-40-points">Problem 4: <span class="math inline">\(\epsilon\)</span>-close estimators of kernels [40 points]</h2>
<!-- From $F = \sup_{x\in\mathbb{R}}|f(x)| < \infty$, we know that $f(x): \mathbb{R} \to \mathbb{R}$ is a scalar function. The operation on a $\mathbb{R}^m$ vector is elementwise. -->
<p>Denote the value of the kernel function <span class="math inline">\(K(v,w) = X\)</span>. We use two estimators <span class="math inline">\(\hat{X_1}, \hat{X_2}\)</span> to approximate the exact value <span class="math inline">\(X\)</span>.</p>
<p>When <span class="math inline">\(v,w\)</span> fixed, denote the mean of two estimators <span class="math inline">\(\hat{X_1}, \hat{X_2}\)</span> as <span class="math inline">\(\mu_1, \mu_2\)</span>. Since this is one state in the whole set <span class="math inline">\(S\)</span>, we have</p>
<p><span class="math display">\[\|\mu_1 - \mu_2\| \leq \delta\]</span></p>
<p>Then,</p>
<p><span class="math display">\[ \begin{aligned}
MSE(\hat{X_1}) 
&amp;= E\big[(\hat{X_1} - \mu_1)^2 + (\mu_1 - X)^2 \big]  \\
&amp;= Var(\hat{X_1}) + bias(\hat{X_1}, X)^2 \\
&amp;= E\big[\hat{X_1^2}\big] - \mu_1^2 + E\big[(\mu_1 - X)^2 \big] 
\end{aligned}\]</span></p>
<p><span class="math display">\[ \begin{aligned}
MSE(\hat{X_1}) - MSE(\hat{X_2})
&amp;= E\big[\hat{X_1^2}\big] - E\big[\hat{X_2^2}\big] - \mu_1^2 + \mu_2^2 + E\big[(\mu_1 - X)^2 \big] -E\big[(\mu_2 - X)^2 \big]  \\
&amp;= E\big[\hat{X_1^2}\big] - E\big[\hat{X_2^2}\big] +2X(\mu_2 - \mu_1) 
\end{aligned}\]</span></p>
<p><span class="math display">\[ \begin{aligned}
|MSE(\hat{X_1}) - MSE(\hat{X_2})| &amp;= | E\big[\hat{X_1^2}\big] - E\big[\hat{X_2^2}\big] +2X(\mu_2 - \mu_1) |\\
&amp;\leq | E\big[\hat{X_1^2}\big] - E\big[\hat{X_2^2}\big]| + |2X(\mu_2 - \mu_1) | \\
\end{aligned}\]</span></p>
<p>Research the first part <span class="math inline">\(| E\big[\hat{X_1^2}\big] - E\big[\hat{X_2^2}\big]|\)</span>. Since <span class="math inline">\(f\)</span> is bounded by <span class="math inline">\(F\)</span>, <span class="math display">\[\begin{aligned}
|\hat{X_i}| &amp;= |\frac1mf(G_{i}v)f(G_{i}w)|\\ 
&amp;\leq \frac1m |f(G_{i}v)||f(G_{i}w)|\\
&amp;\leq \frac{F^2}m \\
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
\therefore 0 \leq \hat{X_i^2} \leq \frac{F^4}{m^2}
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
E[\hat{X_i^2}] &amp;= \int_{-\infty}^{\infty}{f(\hat{X_i}) \hat{X_i^2} d\hat{X_i}} \\
&amp;\leq \int_{-\infty}^{\infty}{f(\hat{X_i}) \frac{F^4}{m^2} d\hat{X_i}} \\
&amp;= \frac{F^4}{m^2}\int_{-\infty}^{\infty}{f(\hat{X_i})  d\hat{X_i}} \\
&amp;= \frac{F^4}{m^2}
\end{aligned}\]</span></p>
<p>And similarly,</p>
<p><span class="math display">\[E[\hat{X_i^2}] \geq0\]</span></p>
<p><span class="math display">\[\therefore |E\big[\hat{X_1^2}\big] - E\big[\hat{X_2^2}\big]| \leq \frac{2F^4}{m^2}\]</span></p>
<p>Then look at the second part <span class="math inline">\(|2X(\mu_2 - \mu_1)|\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
|2X(\mu_2 - \mu_1)| 
&amp;\leq |2X||\mu_2 - \mu_1| \\
&amp;\leq |2X|\delta
\end{aligned}\]</span></p>
<p>Here <span class="math inline">\(X\)</span> is the exact kernel value <span class="math inline">\(K(v, w)\)</span>. We already knew <span class="math inline">\(|\hat{X_i}|\)</span> is bounded by <span class="math inline">\(\frac{F^2}m\)</span>. It goes without saying that, <span class="math inline">\(|X|\)</span> should also be bounded by a value, which is even lower than <span class="math inline">\(\frac{F^2}m\)</span>. If not, <span class="math inline">\(E[\hat{X_i}]\)</span> cannot estimate <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[\therefore |X| \leq \frac{F^2}m\]</span></p>
<p><span class="math display">\[\begin{aligned}
|2X(\mu_2 - \mu_1)| 
&amp;\leq |2X|\delta \\
&amp;\leq \frac{2F^2}m\delta
\end{aligned}\]</span></p>
<p>To sum up, <span class="math display">\[ \begin{aligned}
|MSE(\hat{X_1}) - MSE(\hat{X_2})| &amp;= | E\big[\hat{X_1^2}\big] - E\big[\hat{X_2^2}\big] +2X(\mu_2 - \mu_1) |\\
&amp;\leq | E\big[\hat{X_1^2}\big] - E\big[\hat{X_2^2}\big]| + |2X(\mu_2 - \mu_1) | \\
&amp;\leq \frac{2F^4}{m^2} + \frac{2F^2}m\delta = \epsilon
\end{aligned}\]</span></p>
<h2 id="question-sheet">Question Sheet</h2>
<div class="pdfobject-container" data-target="./final_exam.pdf" data-height="1000px"></div>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Data-Mining-HW3</title>
    <url>/2020/11/28/Data-Mining-HW3/</url>
    <content><![CDATA[<h1 id="hw3">HW3</h1>
<p>Hua Yao, UNI:hy2632</p>
<h2 id="problem-1-svm-algorithm-in-action-50-points">Problem 1: SVM algorithm in action [50 points]</h2>
<h3 id="description-of-the-algorithm">Description of the algorithm:</h3>
<ol type="1">
<li><p>Used dual SVM</p></li>
<li><p>Did not consider regularization, a.k.a. set <span class="math inline">\(C=\infty\)</span>, because this problem (binary classification between 0/9) should be separable, and the representation of <span class="math inline">\(b\)</span> becomes nasty with regularization.</p></li>
<li><p>Used the SMO (sequential minimal optimization) algorithm. During optimization, within each iteration, randomly select <span class="math inline">\(\alpha_1, \alpha_2\)</span>, optimize the QP w.r.t. <span class="math inline">\(\alpha_2\)</span> and update <span class="math inline">\(\alpha_1\)</span> accordingly. Added constraint <span class="math inline">\(\alpha_2 \geq 0\)</span> to the <span class="math inline">\(\alpha_2\)</span> optimizer. This does not constrain <span class="math inline">\(\alpha_1\geq 0\)</span> directly. However, with the randomization within each iteration, <span class="math inline">\(\alpha_i \geq 0\)</span> is satisfied when the whole optimzation over <span class="math inline">\(\alpha\)</span> finally converges.</p></li>
<li><p>Provide 2 options: Linear Kernel (baseline SVM) or Softmax kernel</p>
<p><span class="math display">\[K_{SM}(x, y) = \exp{x^\top y}\]</span></p>
<p>To avoid explosion on scale, normalized the input <span class="math inline">\(x\)</span>.</p>
<p>Included the trigonometric feature map <span class="math inline">\(\phi(x)\)</span> of the softmax kernel for calculating <span class="math inline">\(b\)</span>, (not for <span class="math inline">\(w\)</span> because when making prediction, we use kernel function instead of <span class="math inline">\(w^\top \phi\)</span>).</p>
<p>Use exact kernel instead of approximating kernel with random feature map, because softmax's dimensionality is infinite. Directly compute the exponential in numpy is more efficient.</p>
<p>The prediction comes like this (vectorized version):</p>
<p><span class="math display">\[y_{new} = K(X_{new}, X)\cdot (\alpha * y) + b\]</span></p>
<p>Note that b is broadcasted to <span class="math inline">\(n&#39;\)</span> new data points. <span class="math inline">\(K(X_{new}, X)\)</span> is <span class="math inline">\(n&#39;  \times n\)</span> kernel matrix. <span class="math inline">\(\alpha*y\)</span> means the elementwise product of two vectors.</p></li>
<li><p>SVM is expensive when <span class="math inline">\(n\)</span> is large. Here in practice, we trained on a small batch (default=64). The randomness here influences the performance.</p></li>
<li><p>Have a few trial runs to get the model with best prediction on the training data. Then it should give good prediction on the validation data. You might also need to tune the hyperparameters a little bit, like <code>batch_size</code> and <code>tol</code>.</p></li>
</ol>
<h3 id="classes-kernel-and-svm">Classes: Kernel and SVM</h3>
<h4 id="kernel_featuremaps.py"><a href="https://github.com/hy2632/cs229/blob/master/SVM/Kernel_FeatureMaps.py">Kernel_FeatureMaps.py</a></h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_schmidt_columns</span>(<span class="params">X</span>):</span></span><br><span class="line">    Q, R = np.linalg.qr(X)</span><br><span class="line">    <span class="keyword">return</span> Q</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">orthgonalize</span>(<span class="params">V</span>):</span></span><br><span class="line">    N = V.shape[<span class="number">0</span>]</span><br><span class="line">    d = V.shape[<span class="number">1</span>]</span><br><span class="line">    turns = int(N / d)</span><br><span class="line">    remainder = N % d</span><br><span class="line"></span><br><span class="line">    V_ = np.zeros_like(V)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(turns):</span><br><span class="line">        v = gram_schmidt_columns(V[i * d:(i + <span class="number">1</span>) * d, :].T).T</span><br><span class="line">        V_[i * d:(i + <span class="number">1</span>) * d, :] = v</span><br><span class="line">    <span class="keyword">if</span> remainder != <span class="number">0</span>:</span><br><span class="line">        V_[turns * d:, :] = gram_schmidt_columns(V[turns * d:, :].T).T</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> V_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate orthogonal normal weights (w1, ..., wm)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateGMatrix</span>(<span class="params">m, d</span>) -&gt; np.array:</span></span><br><span class="line">    G = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, (m, d))</span><br><span class="line">    <span class="comment"># Renormalize</span></span><br><span class="line">    norms = np.linalg.norm(G, axis=<span class="number">1</span>).reshape([m, <span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> orthgonalize(G) * norms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Softmax trignometric feature map Φ(x)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">baseline_SM</span>(<span class="params">x, G</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Calculate the result of softmax trigonometrix random feature mapping</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        x: array, dimension = n*d</span></span><br><span class="line"><span class="string">            Input to the baseline mapping</span></span><br><span class="line"><span class="string">            Required to be of norm 1 for i=1,...n</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        G: matrix, dimension = m*d</span></span><br><span class="line"><span class="string">            The matrix in the baseline random feature mapping</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m = G.shape[<span class="number">0</span>]</span><br><span class="line">    left = np.cos(np.dot(x, G.T).astype(np.float32))</span><br><span class="line">    right = np.sin(np.dot(x, G.T).astype(np.float32))</span><br><span class="line">    <span class="keyword">return</span> np.exp(<span class="number">0.5</span>) * ((<span class="number">1</span> / m)**<span class="number">0.5</span>) * np.hstack([left, right])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Kernel</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Kernels.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        x, y: array of (n_x, d), (n_y, d)</span></span><br><span class="line"><span class="string">        Return</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        K(x,y): kernel matrix of (n_x, n_y), K_ij = K(x_i, y_j)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Linear</span>(<span class="params">x, y</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.dot(x, y.T)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Softmax</span>(<span class="params">x, y</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.exp(np.dot(x, y.T))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureMap</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Feature mapping</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        x: array of (n, d)</span></span><br><span class="line"><span class="string">        Return</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        Φ(x): array of (n, m), where m is usually a higher dimensionality. Here we set m = 2*n</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Linear</span>(<span class="params">x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Softmax_Trigonometric</span>(<span class="params">x</span>):</span></span><br><span class="line">        n, d = x.shape</span><br><span class="line">        <span class="comment"># Increase dimensionality to 2x</span></span><br><span class="line">        G = generateGMatrix(<span class="number">2</span> * d, d)</span><br><span class="line">        phi_x = baseline_SM(x, G)</span><br><span class="line">        <span class="keyword">return</span> phi_x</span><br></pre></td></tr></table></figure>
<h4 id="svm.py"><a href="https://github.com/hy2632/cs229/blob/master/SVM/SVM.py">SVM.py</a></h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"><span class="keyword">from</span> tqdm.notebook <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> Kernel_FeatureMaps <span class="keyword">import</span> *;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVM</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">    ## Author:</span></span><br><span class="line"><span class="string">        Hua Yao (hy2632@columbia.edu)</span></span><br><span class="line"><span class="string">    ## Description:</span></span><br><span class="line"><span class="string">        SVM binary classifier, optimizing with the dual lagrangian program, trained on a sample batch. Uses the SMO algorithm.</span></span><br><span class="line"><span class="string">        Normalizes input X to adapt to kernelized version.</span></span><br><span class="line"><span class="string">        Regularization parameter C set to be `np.inf` for easy closed form solution of b.</span></span><br><span class="line"><span class="string">    ## Reference:</span></span><br><span class="line"><span class="string">        [CS229 - Kernel Methods and SVM](http://cs229.stanford.edu/notes2020fall/notes2020fall/cs229-notes3.pdf)</span></span><br><span class="line"><span class="string">    ---</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        X: (N, d)</span></span><br><span class="line"><span class="string">        Y: (N,)</span></span><br><span class="line"><span class="string">        kernel: kernel used, linear/softmax</span></span><br><span class="line"><span class="string">        featuremap: feature mapping corresponding to the kernel used</span></span><br><span class="line"><span class="string">        batch_size: int, also denoted as n</span></span><br><span class="line"><span class="string">        C: l1 regularization term for soft margin. alpha_i in [0, C]. Set as np.inf (no regularization), because the form of b is nasty under regularization.</span></span><br><span class="line"><span class="string">        tol = 1e-6: tolerance, deciding when to end training</span></span><br><span class="line"><span class="string">        Intermediate parameters:</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        x: (n, d), random batch of X</span></span><br><span class="line"><span class="string">        y: (n)</span></span><br><span class="line"><span class="string">        phi_x: (n, m), feature map(s) of x</span></span><br><span class="line"><span class="string">        M: (n, n), M[i,j] = y_iy_j * K(x_i,x_j), hadamard product of y^Ty and K</span></span><br><span class="line"><span class="string">        Learned parameters:</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        alpha: (n,)</span></span><br><span class="line"><span class="string">        w: (d,)</span></span><br><span class="line"><span class="string">        b: int</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 X,</span></span></span><br><span class="line"><span class="function"><span class="params">                 Y,</span></span></span><br><span class="line"><span class="function"><span class="params">                 kernel=Kernel.Linear,</span></span></span><br><span class="line"><span class="function"><span class="params">                 featuremap=FeatureMap.Linear,</span></span></span><br><span class="line"><span class="function"><span class="params">                 batch_size=<span class="number">64</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 C=np.inf,</span></span></span><br><span class="line"><span class="function"><span class="params">                 tol=<span class="number">1e-6</span></span>):</span></span><br><span class="line">        <span class="comment"># C set as np.inf here -- no regularization</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># X: (N,d), Y: (N,), x: (n,d), y:(n,)</span></span><br><span class="line">        <span class="comment"># Fixed values</span></span><br><span class="line">        self.N, self.d = X.shape</span><br><span class="line">        <span class="comment"># Normalize data</span></span><br><span class="line">        self.X = X</span><br><span class="line">        self.X = self.X / np.linalg.norm(self.X, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        self.Y = Y</span><br><span class="line">        self.kernel = kernel</span><br><span class="line">        self.featuremap = featuremap</span><br><span class="line">        self.n = batch_size</span><br><span class="line">        self.C = C</span><br><span class="line">        self.tol = tol</span><br><span class="line"></span><br><span class="line">        batch_indices = np.random.choice(np.arange(self.N), self.n)</span><br><span class="line">        self.x = self.X[batch_indices]</span><br><span class="line">        self.y = self.Y[batch_indices]</span><br><span class="line">        self.phi_x = self.featuremap(self.x)</span><br><span class="line">        self.M = np.outer(self.y, self.y) * self.kernel(self.x, self.x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Learned parameters</span></span><br><span class="line">        self.alpha = np.ones(self.n)</span><br><span class="line">        self.w = np.zeros(self.d)</span><br><span class="line">        self.b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_alpha</span>(<span class="params">self, random_idx1, random_idx2</span>):</span></span><br><span class="line">        Zeta = -np.sum(self.alpha * self.y) + (</span><br><span class="line">            self.alpha * self.y)[random_idx1] + (self.alpha *</span><br><span class="line">                                                 self.y)[random_idx2]</span><br><span class="line">        self.alpha[random_idx1] = (Zeta - self.alpha[random_idx2] *</span><br><span class="line">                                   self.y[random_idx2]) * self.y[random_idx1]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dual_obj</span>(<span class="params">self, alpha</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.sum(alpha) - np.sum(<span class="number">0.5</span> * self.M * np.outer(alpha, alpha))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, iterations=<span class="number">200000</span></span>):</span></span><br><span class="line">        prev_val = self.alpha.copy()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(range(iterations)):</span><br><span class="line">            <span class="comment"># Select 2 alphas randomly</span></span><br><span class="line">            random_idx1, random_idx2 = np.random.choice(</span><br><span class="line">                np.arange(<span class="number">0</span>, self.n), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># The (quadratic w.r.t a2) function that scipy.optimize.minimize takes</span></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">optimizeWRTa2</span>(<span class="params">a2</span>):</span></span><br><span class="line">                self.alpha[random_idx2] = a2</span><br><span class="line">                self.update_alpha(random_idx1, random_idx2)</span><br><span class="line">                <span class="keyword">return</span> -self.dual_obj(self.alpha)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Solve optimization w.r.t a2</span></span><br><span class="line">            a2 = self.alpha[random_idx2]</span><br><span class="line">            res = minimize(optimizeWRTa2, a2, bounds=[(<span class="number">0</span>, self.C)])</span><br><span class="line">            a2 = res.x</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update a2</span></span><br><span class="line">            self.alpha[random_idx2] = a2</span><br><span class="line">            self.update_alpha(random_idx1, random_idx2)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Check convergence</span></span><br><span class="line">            <span class="keyword">if</span> (i % <span class="number">5</span> == <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> np.sum(np.abs(self.alpha - prev_val)) &lt; self.tol:</span><br><span class="line">                    print(</span><br><span class="line">                        <span class="string">f&quot;&gt;&gt; Optimized on the batch, step <span class="subst">&#123;i&#125;</span>. 5 steps Δalpha:<span class="subst">&#123;np.sum(np.abs(self.alpha - prev_val))&#125;</span>&quot;</span></span><br><span class="line">                    )</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> (i % <span class="number">5000</span> == <span class="number">1</span>):</span><br><span class="line">                        print(</span><br><span class="line">                            <span class="string">f&quot;&gt;&gt; Optimizing, step <span class="subst">&#123;i&#125;</span>. Δalpha:<span class="subst">&#123;np.sum(np.abs(self.alpha - prev_val))&#125;</span>&quot;</span></span><br><span class="line">                        )</span><br><span class="line">                    prev_val = self.alpha.copy()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Retrieve w and b</span></span><br><span class="line">        self.w = np.dot(self.alpha * self.y, self.phi_x)</span><br><span class="line">        <span class="comment"># The form of b changes when there exists Regularization C. So we simply cancel C here.</span></span><br><span class="line">        <span class="comment"># Look at P25 of CS229 - Note 3.</span></span><br><span class="line">        <span class="comment"># Form of b under regularization depends on alpha. (http://cs229.stanford.edu/materials/smo.pdf)</span></span><br><span class="line">        <span class="comment"># [Bias Term b in SVMs Again](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2004-11.pdf) gives a general representation</span></span><br><span class="line">        self.b = (np.max(np.dot(self.phi_x, self.w)[self.y == <span class="number">-1</span>]) +</span><br><span class="line">                  np.min(np.dot(self.phi_x, self.w)[self.y == <span class="number">1</span>])) * <span class="number">-0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, X_val</span>):</span></span><br><span class="line">        X_val_normed = X_val / np.linalg.norm(X_val, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> np.sign(</span><br><span class="line">            np.dot(self.kernel(X_val_normed, self.x), self.alpha * self.y) +</span><br><span class="line">            self.b)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span>(<span class="params">self, X_val, y_val</span>):</span></span><br><span class="line">        prediction = self.predict(X_val)</span><br><span class="line">        <span class="keyword">return</span> np.mean(prediction == y_val)</span><br></pre></td></tr></table></figure>
<h3 id="training-example-jupyter-notebook-file">Training Example: Jupyter notebook file</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"><span class="keyword">from</span> tqdm.notebook <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> SVM <span class="keyword">import</span> SVM</span><br><span class="line"><span class="keyword">from</span> Kernel_FeatureMaps <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<h4 id="data-extraction-from-mnist">"0"/"9" Data Extraction from MNIST</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,),(<span class="number">0.5</span>)),</span><br><span class="line">])</span><br><span class="line">trainset = datasets.MNIST(<span class="string">&#x27;PATH_TO_STORE_TRAINSET&#x27;</span>, download=<span class="literal">True</span>, train=<span class="literal">True</span>, transform=transform)</span><br><span class="line">valset = datasets.MNIST(<span class="string">&#x27;PATH_TO_STORE_TESTSET&#x27;</span>, download=<span class="literal">True</span>, train=<span class="literal">False</span>, transform=transform)</span><br><span class="line"></span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset)</span><br><span class="line">valloader = torch.utils.data.DataLoader(valset)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ZeroNineTrain = [i <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(trainset) <span class="keyword">if</span> (i[<span class="number">1</span>] == <span class="number">0</span>)|(i[<span class="number">1</span>] == <span class="number">9</span>)]</span><br><span class="line">X_train, y_train = zip(*ZeroNineTrain)</span><br><span class="line">X_train = torch.stack(X_train)</span><br><span class="line">y_train = torch.tensor(y_train)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ZeroNineVal = [i <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(valset) <span class="keyword">if</span> (i[<span class="number">1</span>] == <span class="number">0</span>)|(i[<span class="number">1</span>] == <span class="number">9</span>)]</span><br><span class="line">X_val, y_val = zip(*ZeroNineVal)</span><br><span class="line">X_val = torch.stack(X_val)</span><br><span class="line">y_val = torch.tensor(y_val)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.imshow(X_train[<span class="number">0</span>].squeeze(), cmap=<span class="string">&quot;gray_r&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line">y_train[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure>
<img src="/2020/11/28/Data-Mining-HW3/Tutorial_7_0.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<pre><code>tensor(0)</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(X_train, <span class="string">&quot;X_train.pt&quot;</span>)</span><br><span class="line">torch.save(y_train, <span class="string">&quot;y_train.pt&quot;</span>)</span><br><span class="line">torch.save(X_val, <span class="string">&quot;X_val.pt&quot;</span>)</span><br><span class="line">torch.save(y_val, <span class="string">&quot;y_val.pt&quot;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="load-train-val-data-and-preprocess">Load Train &amp; Val data and preprocess</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Load Data</span></span><br><span class="line">X_train = torch.load(<span class="string">&quot;X_train.pt&quot;</span>)</span><br><span class="line">y_train = torch.load(<span class="string">&quot;y_train.pt&quot;</span>)</span><br><span class="line">X_val = torch.load(<span class="string">&quot;X_val.pt&quot;</span>)</span><br><span class="line">y_val = torch.load(<span class="string">&quot;y_val.pt&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Criteria: Label0: +1, Label9: -1</span></span><br><span class="line">y_train = (y_train==<span class="number">0</span>)*<span class="number">2</span><span class="number">-1</span></span><br><span class="line">y_val = (y_val==<span class="number">0</span>)*<span class="number">2</span><span class="number">-1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Formatting</span></span><br><span class="line">X_train = X_train.squeeze().view((X_train.shape[<span class="number">0</span>],<span class="number">-1</span>)).numpy()</span><br><span class="line">y_train = y_train.numpy()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_val = X_val.squeeze().view((X_val.shape[<span class="number">0</span>],<span class="number">-1</span>)).numpy()</span><br><span class="line">y_val = y_val.numpy()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">svm_linear = SVM(X_train, y_train, Kernel.Linear, FeatureMap.Linear, <span class="number">64</span>, np.inf, <span class="number">1e-4</span>)</span><br><span class="line">svm_linear.fit()</span><br><span class="line">svm_linear.score(X_train, y_train), svm_linear.score(X_val, y_val)</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(FloatProgress(value=0.0, max=200000.0), HTML(value=&#39;&#39;)))


&gt;&gt; Optimizing, step 1. Δalpha:6.0
&gt;&gt; Optimized on the batch, step 4446. 5 steps Δalpha:1.7763568394002505e-15

(0.9761623989218329, 0.9773755656108597)</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">svm_SM = SVM(X_train, y_train, Kernel.Softmax, FeatureMap.Softmax_Trigonometric, <span class="number">64</span>, np.inf, <span class="number">1e-4</span>)</span><br><span class="line">svm_SM.fit()</span><br><span class="line">svm_SM.score(X_train, y_train), svm_SM.score(X_val, y_val)</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(FloatProgress(value=0.0, max=200000.0), HTML(value=&#39;&#39;)))


&gt;&gt; Optimizing, step 1. Δalpha:15.273472888060688
&gt;&gt; Optimized on the batch, step 251. 5 steps Δalpha:0.0

(0.9834905660377359, 0.9803921568627451)</code></pre>
<h3 id="result">Result:</h3>
<p>Train Accuracy / Validation Accuracy:</p>
<ul>
<li>Linear SVM: (0.976, 0.977)</li>
<li>Softmax Kernel SVM: (0.983, 0.980)</li>
</ul>
<p>Note that the result varies over different trials because of the randomness in batch selection and ending condition.</p>
<h2 id="problem-2-kernel-ridge-regression-20-points">Problem 2: Kernel Ridge Regression [20 points]</h2>
<h3 id="general-ridge-regression">General Ridge Regression</h3>
<p><span class="math display">\[\hat{y} = X\theta\]</span></p>
<p><span class="math display">\[J(\theta) = \frac12(y-X\theta)^T(y-X\theta) + \frac{\lambda}2\|\theta\|^2\]</span></p>
<p>Let</p>
<p><span class="math display">\[\begin{aligned}
&amp; \nabla_{\theta}J(\theta) = 0 \\
\to \quad &amp; \theta = (X^TX + \lambda I_d)^{-1}X^Ty 
\end{aligned}\]</span></p>
<h3 id="method-1-approximate-kernel-with-random-features">Method 1: Approximate kernel with random features</h3>
<p>When using feature map, substituting <span class="math inline">\(X\)</span> with <span class="math inline">\(\phi = \phi(X)\)</span> gives us</p>
<p><span class="math display">\[\begin{aligned}
\theta &amp;= (\phi^T\phi + \lambda I_m)^{-1}\phi^Ty
\end{aligned}\]</span></p>
<p>When predicting,</p>
<p><span class="math display">\[\begin{aligned}
y_{new} &amp;= \phi(X_{new}) (\phi^T\phi + \lambda I_m)^{-1}\phi^Ty
\end{aligned}\]</span></p>
<p>The time complexity of computing <span class="math inline">\(\theta\)</span> is <span class="math inline">\(O(\max{(mn^2,m^3,m^2n)})\)</span>, considering the matrix inverse and the matrix multiplications.</p>
<h3 id="method-2-exact-kernel">Method 2: Exact kernel</h3>
<p>If we use matrix identity trick, we can change the dimensionality of computation.</p>
<p><span class="math display">\[\begin{aligned}
\theta &amp;= \phi^T(\phi\phi^T + \lambda I_n)^{-1}y \\
&amp;= \phi^T\big(K + \lambda I_n\big)^{-1}y
\end{aligned}\]</span></p>
<p><span class="math inline">\(K = K(X,X)\)</span> is <span class="math inline">\(n\times n\)</span> kernel matrix of the training data. This gives the closed form solution of the weight.</p>
<p>When predicting, <span class="math display">\[\begin{aligned}
y_{new} &amp;= \phi(X_{new})\phi(X)^T\big(K + \lambda I_n\big)^{-1}y \\
&amp;= K(X_{new}, X) \big(K + \lambda I_n\big)^{-1}y \\
&amp;= \kappa(X_{new}) \big(K + \lambda I_n\big)^{-1}y
\end{aligned}\]</span></p>
<p>The time complexity of computing <span class="math inline">\(\theta\)</span> is <span class="math inline">\(O(\max{(n^2d, n^3, mn^2)})\)</span>. Directly computing the kernel matrix takes <span class="math inline">\(O(n^2d)\)</span>, matrix inverse <span class="math inline">\(O(n^3)\)</span>, matrix multiplication <span class="math inline">\(O(mn^2), O(mn)\)</span></p>
<h3 id="comparison-time-complexity-of-training">Comparison: Time complexity of training</h3>
<p>When <span class="math inline">\(m \gg n\)</span>, approximate kernel takes</p>
<p><span class="math display">\[O(\max{(mn^2,m^3,m^2n)}) = O(m^3)\]</span></p>
<p>exact kernel method takes</p>
<p><span class="math display">\[O(\max{(n^2d, n^3, mn^2)})= O(mn^2)\]</span></p>
<p><span class="math display">\[\because m^3 \gg mn^2\]</span></p>
<p><span class="math display">\[\therefore \text{time complexity of approximate kernel method is higher than the exact kernel method}\]</span></p>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Data-Mining-Lec11</title>
    <url>/2020/11/12/Data-Mining-Lec11/</url>
    <content><![CDATA[<h1 id="vanishingexploding-gradients">Vanishing/Exploding gradients</h1>
<p><span class="math display">\[\ell = \ell(\alpha_L)\]</span></p>
<p><span class="math display">\[\nabla_{w_j, b_j} \ell\]</span></p>
<p>Update parameter:</p>
<p><span class="math display">\[\theta_{i+1} := \theta_i - \eta \nabla_{w_j, b_j} \ell\]</span></p>
<p>activation <span class="math inline">\(a_t\)</span> &amp; <span class="math inline">\(a_{t+1}\)</span>, some matrix <span class="math inline">\(A\)</span> in between.</p>
<p>If linear transfromation,</p>
<p><span class="math display">\[a_{t+1} = Aa_t, A\in\mathbb{R}^{m\times m}\]</span></p>
<p>Chain rule:</p>
<p><span class="math display">\[\frac{\partial \ell}{\partial a_{t}} = \sum_{j=1}^{m}{\frac{\partial \ell}{\partial a_{i+1}^j} \cdot \frac{\partial a_{i+1}^j}{\partial a_i^j} }\]</span></p>
<p>And</p>
<p><span class="math display">\[a_{t+1}^j = \sum_{k=1}^{m}A_{jk} a_t^k\]</span></p>
<p><span class="math display">\[\frac{\partial \ell}{\partial a_t^i} =\sum_{j=1}^m{a_ji} \]</span></p>
<p><span class="math display">\[\frac{\partial \ell}{\partial a_t} = A^T \frac{\partial \ell}{\partial a_{t+1}}\]</span></p>
<p>Then</p>
<p><span class="math display">\[\frac{\partial \ell}{\partial a_k} = \frac{\partial \ell}{\partial a_{L}} \cdot A_L^T A_{L-1}^T...A_K^T\]</span></p>
<p>Gradient vanishing / exploding</p>
<p>We want the eigenvalues to be bounded.</p>
<p>One way is to assume all A are orthogonal matrix. Rows of norm 1.</p>
<p>Orthogonal neural networks(Orthogonal RNNs).</p>
<p>Problem: 1. Expressiveness: Only rotations/reflection 2. Init orthognalizing, but probably lose <strong>orthogonality</strong>, Invariant?? 1. Unrestricted parameterizations, <span class="math inline">\(|a_{ij}| \leq 1\)</span> 2. Givens Rotations: <span class="math display">\[A = \text{Giv}_1\text{Giv}_2...\text{Giv}_k\]</span></p>
<pre><code>    parameters of Givens rotations: $\text&#123;Giv&#125; = \text&#123;Giv&#125;(i, j, \theta)$</code></pre>
<ol start="3" type="1">
<li>Riemmanion optimization
<ol type="1">
<li>Make standard gradient step and project back onto orthogonal group</li>
<li>Compute the so-called Riemmanion gradient(Expensive)</li>
</ol></li>
</ol>
<h2 id="in-the-general-case">In the general case</h2>
<p>gradients' norms are preserved if <span class="math inline">\(A \in \mathbb{R}^{m\times m}\)</span> is orthogonal and <span class="math inline">\(\sigma :R\to R\)</span> satisfies : <span class="math inline">\(\sigma&#39;(x) = 1\)</span></p>
<p><span class="math display">\[ \bigg| \frac{\partial \ell}{\partial a_t}  \bigg|_2  \geq \frac tL \bigg|\frac{\partial \ell}{\partial a_L}\bigg|_2 \]</span></p>
<p><a href="https://arxiv.org/pdf/2008.07669.pdf">HiPPO: Recurrent Memory with Optimal Polynomial Projections</a></p>
<h1 id="svm">SVM</h1>
<p>Find the classifiers for a set of training data.</p>
<p>Margins:</p>
<p>Optimization: find a hyperplane H such that separtates points of label +1 from points of label -1 and margin optimized.</p>
<p>Feasibility</p>
<p>Define an orthogonal vector <span class="math inline">\(\omega\)</span> of the hyperplane, denoting the orientation of hyperplane.</p>
<p>Without loss of generality, we will assume that <span class="math inline">\(\omega\)</span> points to feature vectors with label +1.</p>
<p><span class="math display">\[H = \{ x\in \mathbb{R}^d,  \omega^Tx + b = 0   \}\]</span></p>
<p><span class="math inline">\(\omega\)</span> is the parametric form of the hyperplane in <span class="math inline">\(\mathbb{R}^d\)</span></p>
<h2 id="training-procedure">Training procedure</h2>
<p>Find <span class="math inline">\(\omega, b\)</span> such that - H is a separating hyperplane - <span class="math inline">\(d_l(X_{train})\)</span> is maximized given (1)</p>
<p>Optimization tricks: Convert to convex program</p>
<p><span class="math display">\[min \frac12 ||w||_2^2\]</span> <span class="math display">\[li(w^Tx_1+b)\geq 1, \text{for}\quad i=1,...N\]</span></p>
<h2 id="non-linear-classifier">Non-Linear classifier</h2>
<p><span class="math display">\[X_i \to \phi(x_i)\]</span></p>
<p>Some nonlinear transformation. Substitute x with the feature mapping.</p>
<p>Nonlinear classifier in the original space.</p>
<h2 id="lagrange-duality">Lagrange Duality</h2>
<p>Lagrangian;</p>
<p>KKT conditions;</p>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Data-Mining-HW2</title>
    <url>/2020/11/18/Data-Mining-HW2/</url>
    <content><![CDATA[<h1 id="hw2">HW2</h1>
<p>Hua Yao, UNI:hy2632</p>
<h2 id="problem-1-convolutional-neural-networks-20-points">Problem 1: Convolutional Neural Networks [20 points]</h2>
<p>The input RGB- image has 3 channels. Apply Conv2D to each channel to get 3 feature maps.</p>
<p>When there's no padding, the shape of 3 feature maps are <span class="math inline">\(125\times125\)</span>.</p>
<p><span class="math display">\[O = \frac{I-k}{s} + 1 = \frac{128-4}{1} +1 = 125\]</span></p>
<p>The number of parameters in the convolution layer is <span class="math inline">\(4^2 \times 3 = 48\)</span></p>
<p>The shape of 3 feature maps after max-pooling are <span class="math inline">\(61\times61\)</span> <span class="math display">\[O = \frac{I-k}{s} + 1 = \frac{125-5}{2} + 1 = 61\]</span></p>
<p>The number of parameters in maxpooling is <span class="math inline">\(0\)</span>.</p>
<p>After maxpooling, the fully connected layer maps <span class="math inline">\(I: 61\times61\times3 \to O:256\)</span>, which takes <span class="math inline">\(61\times61\times3\times256=2857728\)</span> parameters.</p>
<p>The output layer maps <span class="math inline">\(I:256 \to O:10\)</span> and then softmax. This step takes <span class="math inline">\(256\times10=2560\)</span> parameters.</p>
<ul>
<li>Answer:
<ul>
<li>Total number of parameters in the NN is <span class="math inline">\(48+0+2857728+2560=2860336\)</span>.</li>
<li>The shape of feature maps of Conv layer is <span class="math inline">\(125\times125\)</span>, that of max-pooling is <span class="math inline">\(61\times61\)</span>.</li>
<li>The max-pooling layer consists of 3 feature maps (same number of channels as in the Conv layer, because max-pooling is channel-wise)</li>
</ul></li>
</ul>
<h2 id="problem-2-deep-residual-networks-40-points">Problem 2: Deep Residual Networks [40 points]</h2>
<h3 id="form-of-partial-derivative">2.1 Form of partial derivative</h3>
<p><span class="math display">\[a_{i+1} = a_i + \frac1T\cdot|Wa_i|\]</span></p>
<p>Assume that <span class="math inline">\(W \in \mathbb{R}^{d\times d}, a_i \in \mathbb{R}^d\)</span>.</p>
<p><span class="math display">\[\begin{aligned}\because|Wa_i| &amp;= \sqrt{(Wa_i)^TWa_i}\\
&amp;=\sqrt{a_i^TW^TWa_i}\\
&amp;=\sqrt{a_i^Ta_i}\\
&amp;= |a_i|
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}\frac{\partial|a_i|}{\partial a_i} &amp;= \frac{\partial}{\partial a_i} \sqrt{a_i^Ta_i}\\
&amp;=\frac{2a_i}{2\sqrt{a_i^Ta_i}}\\
&amp;= \frac{a_i}{|a_i|}
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}\therefore \frac{\partial a_{i+1}}{\partial a_i} 
&amp;= I + \frac1T\frac{a_i}{|a_i|}  \text{\quad (broadcasting to each column)}
\end{aligned}\]</span></p>
<p>From chain rule,</p>
<p><span class="math display">\[\frac{\partial L}{\partial a_t} = \frac{\partial L}{\partial a_T}\cdot \frac{\partial a_T}{\partial a_T-1}...\frac{\partial a_{t+1}}{\partial a_t}\]</span></p>
<h3 id="upper-bound">2.2 Upper bound</h3>
<p>Reference: https://www.cs.utexas.edu/users/flame/laff/alaff-beta/chapter01-norms-matrix-submultiplicative.html</p>
<p>The 2-norm of matrix is defined by <span class="math display">\[||A||_2 = \sqrt{\lambda_1}\]</span> where <span class="math inline">\(\lambda_1\)</span> is the greatest eigenvalue of <span class="math inline">\(A^TA\)</span>.</p>
<p>The <strong>triangular inequality</strong> of matrix norm gives that</p>
<p><span class="math display">\[||x+y|| \leq ||x|| + ||y||\]</span></p>
<p><span class="math display">\[\begin{aligned}
\therefore \bigg|\bigg| \frac{\partial a_{i+1}}{\partial a_i}\bigg|\bigg| 
&amp;= \bigg|\bigg| I + \frac1T\frac{a_i}{|a_i|}   \bigg|\bigg| \\
&amp;\leq 1 + \frac1T \bigg|\bigg| \frac{a_i}{|a_i|} \bigg|\bigg| \\
&amp;= 1 + \frac1T
\end{aligned}\]</span></p>
<p>Note that the second term <span class="math inline">\(\frac1T\frac{a_i}{|a_i|}\)</span> should actually be broadcast to d columns to become a <span class="math inline">\(d\times d\)</span> matrix. But the norm of this matrix is the same as the norm of the vector before broadcasting.</p>
<p>Submultiplicative property of 2-norm gives: <span class="math display">\[||AB|| \leq ||A||\: ||B||\]</span></p>
<p>Matrix 2-norm is subordinate to vector 2-norm: <span class="math display">\[\| A x \| \leq \| A \| \| x \|\]</span></p>
<p><span class="math display">\[\begin{aligned}
\therefore \bigg|\bigg|\frac{\partial L}{\partial a_t}\bigg|\bigg| &amp;\leq \bigg|\bigg|\frac{\partial L}{\partial a_T} \bigg|\bigg| \:  \bigg|\bigg|\frac{\partial a_T}{\partial a_T-1}\bigg|\bigg|...\bigg|\bigg|\frac{\partial a_{t+1}}{\partial a_t}\bigg|\bigg|\\
&amp;\leq \bigg|\bigg|\frac{\partial L}{\partial a_T} \bigg|\bigg| \times (1+\frac1T)^{T-t}\\
&amp;= \bigg|\bigg|\frac{\partial L}{\partial a_T}\bigg|\bigg| \times (1+\frac1T)^{T} \times (1+\frac1T)^{-t}\\
&amp;\approx\bigg|\bigg|\frac{\partial L}{\partial a_T}\bigg|\bigg| \times e \times (1+\frac1T)^{-t} \text{\quad, T large enough}
\end{aligned}\]</span></p>
<p>This gives an upper bound on the ratio <span class="math display">\[r = \frac{\|\frac{\partial L}{\partial a_t}\|}{\|\frac{\partial L}{\partial a_T}\|} \leq e\times (1+\frac1T)^{-t}\]</span></p>
<h3 id="lower-bound">2.3 Lower bound</h3>
<p>Reference: https://ccse.lbl.gov/Publications/sepp/matrixLowerBound/LBNL-50635.pdf</p>
<p>The submultiplicative/subordinate property can be rewritten as</p>
<p><span class="math display">\[\|A\|\|B^{-1}\|^{-1} \leq \|AB\|\]</span></p>
<p>Denote <span class="math inline">\(B_i = I + \frac1T \frac{a_i}{\|a_i\|}\)</span>.</p>
<p>Solve the inverse of <span class="math inline">\(B_i\)</span> <span class="math display">\[\begin{aligned}
\because(I + \frac1T \frac{a_i}{\|a_i\|})(I - \frac1T \frac{a_i}{\|a_i\|}) 
&amp;= I - \frac1{T^2}I \\
&amp;= \frac{T^2-1}{T^2}I
\end{aligned}\]</span></p>
<p><span class="math display">\[\therefore B_i^{-1} = \frac{T^2}{T^2-1}(I - \frac1T \frac{a_i}{\|a_i\|})\]</span></p>
<p>Use the triangular inequality of matrix norm <span class="math inline">\(\|x-y\| \leq \|x\| + \|-y\| = \|x\| + \|y\|\)</span></p>
<p><span class="math display">\[\therefore \|B_i^{-1}\| \leq  \frac{T^2}{T^2-1}(1+\frac1T)=\frac T{T-1}\]</span></p>
<p><span class="math display">\[\therefore \|B_i^{-1}\|^{-1} \geq 1 - \frac{1}{T}\]</span></p>
<p><span class="math display">\[\begin{aligned}
\therefore r &amp;= \frac{\|\frac{\partial L}{\partial a_t}\|}{\|\frac{\partial L}{\partial a_T}\|} \\
&amp;\geq (\|B_{T-1}^{-1}\|...\|B_t^{-1}\|)^{-1}\\
&amp;\geq (1 - \frac{1}{T})^{T-t}\\
&amp;\approx \frac1e \times (1-\frac{1}{T})^{-t}
\end{aligned}\]</span></p>
<h3 id="conclusion">2.4 Conclusion</h3>
<p>From above we get</p>
<p><span class="math display">\[(1+\frac1T)^{T-t} \leq r \leq (1 - \frac{1}{T})^{T-t}\]</span></p>
<p>When <span class="math inline">\(T\)</span> is large,</p>
<p><span class="math display">\[(1+\frac1T)^{T}\approx e \]</span> <span class="math display">\[(1-\frac1T)^{T}\approx \frac1e \]</span></p>
<p><span class="math display">\[\frac1e \times (1-\frac{1}{T})^{-t} \leq r \leq e\times (1+\frac1T)^{-t}\]</span></p>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Data Mining Lec1</title>
    <url>/2020/09/13/Data-Mining-Lec1/</url>
    <content><![CDATA[<h3 id="课程信息-ieore4540">课程信息 IEORE4540</h3>
<p>Lecturer: <a href="https://research.google/people/KrzysztofChoromanski/">Krzysztof Choromanski</a></p>
<ul>
<li>Krzysztof Choromanski works on several aspects of machine learning and robotics. His current research interests include reinforcement learning and randomized methods such as nonlinear embeddings based on structured random feature maps and quasi-Monte-Carlo methods. He was also working on online nonparametric clustering for massive high-dimensional streams. Krzysztof is an author of several nonlinear embedding mechanisms based on structured matrices that can be used to speed up: neural network computations, kernel methods applying random feature maps, convex optimization solvers, quantization and soft clustering methods as well as several LSH-based algorithms. With his background in structural graph theory, he is also interested in applying graph theory and other combinatorial methods in machine learning.</li>
</ul>
<p>Contact: <a href="KMC2178@columbia.edu">KMC2178@columbia.edu</a>|<a href="CHOROMANSKI1@gmail.com">CHOROMANSKI1@gmail.com</a>|<a href="KCHORO@google.com">KCHORO@google.com</a></p>
<h3 id="提要">提要</h3>
<p>第一节课主要涉及几类核方法(Kernel)，random feature map，和MSE</p>
<h3 id="笔记正文">笔记正文</h3>
<div class="pdfobject-container" data-target="./Lec1.pdf" data-height="1000px"></div>
<h3 id="补充-核函数-kernels-与-svm">补充： 核函数 Kernels 与 SVM</h3>
<p><a href="https://www.youtube.com/watch?v=mTyT-oHoivA">cs229 Lecture 12.4 — Support Vector Machines | (Kernels-I) — [ Machine Learning | Andrew Ng]</a></p>
<h4 id="kernels">Kernels</h4>
<p>Kernel is aka similarity function. We set up <code>landmarks</code>: <span class="math inline">\(l_1, l_2, l_3\)</span> and we wanna know the similarity between our input <span class="math inline">\(x\)</span> and these landmarks.</p>
<p>Gaussian kernel <span class="math inline">\(K_{Gauss}\)</span> is</p>
<p><span class="math display">\[f_i = similarity(x, l^{(i)}) = exp(-\frac{||x-l^{(i)}||^2}{2\sigma^2})\]</span></p>
<p><span class="math inline">\(\sigma\)</span> is one hyperparameter which decides the shape of "contour graph" -- aka scaling the similarity</p>
<p>Then we are gonna use similarities <span class="math inline">\(f_i\)</span> to classify <span class="math inline">\(y\)</span>.</p>
<p>Predict <span class="math inline">\(y=1\)</span> if</p>
<p><span class="math display">\[\theta_0 + \theta_1f_1 + \theta_2f_2 + \theta_3f_3 \geq 0\]</span></p>
<p>Here, <span class="math inline">\(\theta\)</span> s are weights. If we wanna know whether <span class="math inline">\(x\)</span> is close to landmarks <span class="math inline">\(l_1\)</span> and <span class="math inline">\(l_2\)</span>, we could set <span class="math inline">\(\theta_1 = 1, \theta_2 = 1, \theta_3 = 0\)</span>.</p>
<h4 id="svm-with-kernels">SVM with Kernels</h4>
<p>Given <span class="math inline">\((x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})\)</span></p>
<p>we choose <code>landmarks</code> <span class="math inline">\(l^{(i)} = x^{(i)}\)</span></p>
<p><span class="math display">\[f_1 = similarity(x, l^{(1)})\]</span> <span class="math display">\[f_2 = similarity(x, l^{(2)})\]</span> <span class="math display">\[......\]</span> <span class="math display">\[f_m = similarity(x, l^{(m)})\]</span></p>
<p>Define <span class="math inline">\(f\)</span></p>
<p><span class="math display">\[f = \begin{bmatrix}f_0\\f_1\\f_2\\...\\f_m\end{bmatrix}\]</span> where <span class="math inline">\(f_0 = 1\)</span></p>
<p>for training example <span class="math display">\[f^{(i)} = \begin{bmatrix}f_0^{(i)}\\f_1^{(i)}\\f_2^{(i)}\\...\\f_m^{(i)}\end{bmatrix}\]</span></p>
<p><span class="math inline">\(f \in \large{R}^{m+1}\)</span>, predict <span class="math inline">\(y=1\)</span> if <span class="math inline">\(\theta^{T}f \geq 0\)</span></p>
<p><span class="math inline">\(\theta \in \large{R}^{m+1}\)</span></p>
<p>When training, the objective function changes w.r.t <span class="math inline">\(f\)</span></p>
<p><span class="math display">\[\min_{\theta} C\sum_{i=1}^{m}{y^{(i)}cost_1{(\theta^Tf^{(i)})} +  (1-y^{(i)})cost_0{(\theta^Tf^{(i)})}} + \frac{1}{2} \sum_{j=1}^{m}{\theta_j^2}\]</span></p>
<h3 id="wiki-kernel-method">Wiki: <a href="https://en.wikipedia.org/wiki/Kernel_method">Kernel Method</a></h3>
<p>使用内核函数，在高维隐式特征空间中操作，而无需计算该空间中数据的坐标，而是在特征空间中计算内积。</p>
<p>基于实例： 内核方法基于实例学习，并非训练一些固定数量的参数, 而是记住 <span class="math inline">\(i\)</span>-th training example <span class="math inline">\((\mathbf {x}_{i},\mathbf {y}_{i})\)</span> 并学习到对应的权重 <span class="math inline">\(w_{i}\)</span>.</p>
<p><span class="math display">\[\therefore w:\mathbf R^m, x:\mathbf R^m\]</span></p>
<h3 id="总结一下遇到的各种核函数"><a href="https://blog.csdn.net/wsj998689aa/article/details/47027365">总结一下遇到的各种核函数</a></h3>
<p>PCA在原始空间中的数学模型 <span class="math display">\[XX^Tw_i=\lambda_iw_i\]</span> 在高维空间中 <span class="math display">\[\Phi(X)\Phi(X)^Tw_i^{\Phi} = \lambda_iw_i^{\Phi}\]</span> 基向量 <span class="math inline">\(w_i^{\Phi}\)</span> 用训练样本线性表示(上文提到 <span class="math inline">\(w\)</span> 和 <span class="math inline">\(x\)</span> 关系,都在 <span class="math inline">\(N\times d\)</span> 维空间) <span class="math display">\[w_i^{\Phi} = \sum_{k=1}^{N}{\alpha_k\Phi(x_k)} = \Phi(X)\alpha_i \]</span> 代入 <span class="math display">\[\Phi(X)\Phi(X)^T\Phi(X)\alpha_i = \lambda_i\Phi(X)\alpha_i\]</span> <span class="math display">\[\Phi(X)^T\Phi(X)\Phi(X)^T\Phi(X)\alpha_i = \lambda_i\Phi(X)^T\Phi(X)\alpha_i\]</span> 某种<strong>核函数</strong>满足 <span class="math display">\[ k(x_i, x_j) = \Phi(x_i)^T\Phi(x_j) \]</span> 矩阵化表示 <span class="math display">\[ K(X,Y) = \Phi(X)^T\Phi(X) \]</span> 则 <span class="math display">\[ K^2\alpha_i = \lambda_iK\alpha_i \]</span> 可以看见高维映射被抹掉了。</p>
<p>那么核函数具有什么样的性质？</p>
<h3 id="mercers-theorem"><a href="https://en.wikipedia.org/wiki/Mercer%27s_theorem">Mercer's theorem</a></h3>
<p>Mercer's theorem: 任何半正定的函数都可以作为核函数。(充分不必要)</p>
<p><span class="math display">\[X = (x_1,x_2,...x_n)\]</span></p>
<p>我们定义矩阵的元素 <span class="math display">\[a_{ij} = f(x_i,x_j)\]</span></p>
<p>如果这个 <span class="math inline">\(n*n\)</span> 的矩阵是半正定的，那么 <span class="math inline">\(f(x_i,x_j)\)</span> 就称为半正定的函数。</p>
<h3 id="常见核函数">常见核函数</h3>
<h4 id="linear-kernel">Linear Kernel</h4>
<p><span class="math display">\[ k(x,y) = x^Ty \]</span></p>
<h4 id="gaussian-kernel">Gaussian Kernel</h4>
<p><span class="math display">\[ k_{Gauss}(x,y) = \exp(-\frac{||x-y||^2}{2\sigma^2}) \]</span></p>
<h4 id="exponential-kernel">Exponential Kernel</h4>
<p>将高斯核的 L2 距离调整为 L1 距离</p>
<p><span class="math display">\[ k(x,y) = \exp(-\frac{||x-y||}{2\sigma^2}) \]</span></p>
<h4 id="laplacian-kernel">Laplacian Kernel</h4>
<p>等价于指数核</p>
<p><span class="math display">\[ k(x,y) = \exp(-\frac{||x-y||}{\sigma}) \]</span></p>
<h4 id="softmax-kernel">Softmax Kernel</h4>
<p><span class="math display">\[ K_{SM}(x,y) = e^{x^Ty} \]</span> <span class="math display">\[ = e^{\frac{||x||^2}{2}}  e^{\frac{||y||^2}{2}}  K_{Gauss}(x,y) \]</span></p>
<h4 id="angular-kernel">Angular Kernel</h4>
<p><span class="math display">\[ K_{Ang}(x,y) = 1 - \frac{2\theta_{x,y}}{\pi} \]</span></p>
<h3 id="feature-map-phi-的结构">feature map <span class="math inline">\(\Phi\)</span> 的结构</h3>
<p><span class="math display">\[ \Phi(x) = \frac{1}{\sqrt{m}} \begin{bmatrix}f_1(w_1^Tx),...,f_1(w_m^Tx)\\...\\f_l(w_1^Tx),...,f_l(w_m^Tx)\end{bmatrix}^T \]</span></p>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Data-Mining-Lec12</title>
    <url>/2020/11/19/Data-Mining-Lec12/</url>
    <content><![CDATA[<h2 id="recap">Recap</h2>
<p>KKT, Lagrangian</p>
<p><span class="math display">\[L(w, b, \alpha, \beta) = f(w,b) + \sum_{i=1}^{N}{\alpha_ig_i(w,b)} + \sum_{i=1}^{N}{\beta_ih_i(w,b)}\]</span></p>
<p><span class="math display">\[\theta(w,b) = \max_{\alpha, \beta} L(w, b, \alpha, \beta)  \]</span></p>
<p><span class="math display">\[\theta(w,b) = \begin{cases} f(w,b) \text{\: if feasible} \\ \infty \text{\: otherwise} \end{cases}\]</span></p>
<p><span class="math display">\[\min_{w,b}\theta(w,b) = \min_{w,b}\max_{\alpha, \beta, \alpha_1 \geq 0} L(w, b, \alpha, \beta) \]</span></p>
<p>if <span class="math inline">\(f, g\)</span> are convex, ?affine</p>
<p><span class="math display">\[ \min_{w,b}\max_{\alpha, \beta, \alpha_1 \geq 0} L(w, b, \alpha, \beta) = \max_{w,b}\min_{\alpha, \beta, \alpha_i \geq 0} L(w, b, \alpha, \beta) \]</span></p>
<hr />
<p>Constraints, partial derivatives of <span class="math inline">\(w, b, \beta_i, \alpha_i\)</span> are all <span class="math inline">\(0\)</span>.</p>
<h2 id="svm-primal">SVM-primal</h2>
<p><span class="math display">\[\min_{w,b} \frac12 \|w\|^2 \\
\text{s.t. \quad} l^i(w^Tx_i+b) \geq 1 \text{\quad for i=1, ...N}\]</span></p>
<p><span class="math display">\[g_i(w,b) = 1-l^i(w^Tx_i+b) \leq 0\\
\to L(w, b, \alpha) = \frac12 \|w\|^2 - \sum_{i=1}^m{\alpha_i (l^i(w^Tx_i+b-1))}
\]</span></p>
<p><span class="math display">\[\nabla_wL(w, b, \alpha) = w - \sum_{i=1}^{m}\alpha_il^ix^i=0 \\
\to w = \sum_{i=1}^{m}\alpha_il^ix^i\]</span></p>
<p><span class="math display">\[\nabla_bL(w, b, \alpha) = \sum_{i=1}^{m}\alpha_il^i = 0 \]</span></p>
<p>Put back to lagrangian,</p>
<p><span class="math display">\[ \begin{aligned}
L(w,b,\alpha) &amp;= \frac12 \|\sum_{i=1}^{m}\alpha_il^ix^i\|^2 - \sum_{i=1}^m{\alpha_i (l^i(w^Tx_i+b-1))} \\
&amp;= -\frac12 \|\sum_{i=1}^{m}\alpha_il^ix^i\|^2  + \sum_{i=1}^m{\alpha_i} 
\end{aligned}\]</span></p>
<p>Then the goal is</p>
<p><span class="math display">\[\max -\frac12 \|\sum_{i=1}^{m}\alpha_il^ix^i\|^2  + \sum_{i=1}^m{\alpha_i} \\
\alpha_i \geq 0, \quad \sum_{i=1}^N{\alpha_il^i}=0\]</span></p>
<p>Without loss of generality, let us assume that in the <span class="math inline">\(i^{th}\)</span> iteration of the optimization procedure for the DUAL-SVM, we optimize over: <span class="math inline">\(\alpha_1, \alpha_2\)</span></p>
<p><span class="math display">\[\alpha_1l^1 + \alpha_2l^2 = -\sum_{i=3}^N{\alpha_il^i}\]</span></p>
<p><span class="math display">\[\alpha_2 = \frac{-\sum_{i=3}^N{\alpha_il^i} - \alpha_1l^1 }{l^2} \]</span></p>
<p>Each step freeze 2, optimize others. (Heuristic)</p>
<p><img src="/2020/11/19/Data-Mining-Lec12/1.jpg" /></p>
<p><img src="/2020/11/19/Data-Mining-Lec12/2.jpg" /></p>
<h2 id="kernel-ridge-regression">Kernel Ridge Regression</h2>
<p><span class="math inline">\(\{x_i\}, \{y_i\}\)</span></p>
<p>Cost function (linear) <span class="math display">\[ C(w) = \frac12 \sum_{i=1}^N{(y_i - w^Tx_i)^2} \]</span></p>
<p><span class="math display">\[H: w^Tx + b = 0\]</span></p>
<p><span class="math inline">\(x_i&#39; = x_i \text{\: concating 1}\)</span></p>
<p>Optimization for standard linear regression is just minimize <span class="math inline">\(C(w)\)</span></p>
<p>In ridge regression,</p>
<p><span class="math display">\[\min_w C(w) = \frac12 \sum_{i=1}^N{(y_i - w^Tx_i)^2} + \frac12 \lambda \|w\|_2^2\]</span></p>
<p><span class="math display">\[\nabla_{w^*}C(w^*) = 0\]</span></p>
<p><span class="math display">\[w^* = \big(\lambda I + \sum_{i=1}^N{x_ix_i^T}\big) ^{-1}\big(\sum_{j=1}^N{y_jx_j^T}\big)\]</span></p>
<ul>
<li>Time complexity:
<ul>
<li>each <span class="math inline">\(x_ix_i^T\)</span> is a matrix in <span class="math inline">\(\mathbb{R}^{d\times d}\)</span>,</li>
<li><span class="math inline">\(\sum_N x_ix_i^T\)</span> takes <span class="math inline">\(Nd^2\)</span></li>
<li><span class="math inline">\(\big(\lambda I + \sum_{i=1}^N{x_ix_i^T}\big) ^{-1}\)</span> takese <span class="math inline">\(d^3\)</span></li>
</ul></li>
</ul>
<p><strong>Approximate kernel Method</strong> <span class="math display">\[x_i \to \phi(x_i)\]</span></p>
<p><span class="math display">\[w = \big(\lambda I + \sum_{i=1}^N{\phi(x_i)\phi(x_i)^T}\big) ^{-1}\big(\sum_{j=1}^N{y_j\phi(x_j)^T}\big)\]</span></p>
<hr />
<h2 id="matrix-identity">Matrix Identity</h2>
<p>P, B, R</p>
<p><span class="math display">\[(P^{-1} + B^TR^{-1}B)^{-1}B^TR^{-1} = PB^T(BPB^T+R)^{-1}\]</span></p>
<p><span class="math inline">\(B\in\mathbb{R}^{r\times p}\)</span></p>
<p><span class="math display">\[B^TR^{-1} = (P^{-1} + B^TR^{-1}B)PB^T(BPB^T+R)^{-1}\]</span></p>
<p><span class="math display">\[B^TR^{-1}(BPB^T+R) = (P^{-1}+B^TR^{-1}-B)PB^T\]</span></p>
<p><span class="math display">\[w = (\lambda I +\phi\phi^{-1})\phi_y = \phi(\phi^T\phi + \lambda I)^{-1}y\]</span></p>
<p><span class="math display">\[\phi(X)^T\phi(X) \in \mathbb{R}^{N\times N}\]</span></p>
<h2 id="predictions">Predictions</h2>
<p><span class="math display">\[x\in\mathbb{R}^d, y=w^T\phi(x)\]</span></p>
<p>number of random features <span class="math display">\[m = \theta(d\log d)\]</span></p>
<p>G: Gaussian unstructured matrix <span class="math inline">\(\in \mathbb{R}^{d\times d}\)</span></p>
<ul>
<li>Get Gaussian Orthogonal Matrix
<ul>
<li>Gram-schmidt orthogonalization approach:
<ul>
<li>rows of l2-norm 1</li>
<li>Apply sample scalars: s1, ..., sd independently from <span class="math inline">\(\chi(d)\)</span> distribution and apply them to renormalize rows (Or simply <span class="math inline">\(\sqrt{d}\)</span>, but biased)</li>
</ul></li>
<li>random hadamard matrices
<ul>
<li><span class="math inline">\(\frac1{\sqrt{d}} HD_1 ... \frac1{\sqrt{d}} HD_L\)</span></li>
<li>renormalize</li>
</ul></li>
<li>Givens rotations
<ul>
<li>also renormalize</li>
</ul></li>
</ul></li>
</ul>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Data-Mining-Lec13</title>
    <url>/2020/12/03/Data-Mining-Lec13/</url>
    <content><![CDATA[<h1 id="evolutional-strategy">Evolutional Strategy</h1>
<h2 id="rl-policies-optimization">RL Policies Optimization</h2>
<p>State, Action</p>
<p>Policy: Deterministic / Randomized</p>
<p>Function <span class="math inline">\(F: \mathbb{R}^d \to \mathbb{R}\)</span>, reward from vector to scalar.</p>
<p>Transition is a blackbox: decided by the environment. Dense reward better than sparse reward. Simulators are used to get the transitions (Physics engine?).</p>
<p>Now assume that the environment is blackbox. We get partial observation. Environment can be randomized.</p>
<h2 id="rfd-fancy-finite-difference-replacing-backprop">RFD: Fancy Finite difference replacing backprop</h2>
<p>Random finite difference</p>
<p><span class="math inline">\(F: \mathbb{R}^d \to \mathbb{R}\)</span></p>
<p>Towards smooth relations</p>
<p><span class="math display">\[\max_{\mu\in P(\mathbb{R}^d)} {\mathbb{E}_{\theta \sim \mu}{[F(\theta)]}}\]</span></p>
<p>Gaussian smoothings</p>
<p><span class="math display">\[\max_{\theta\in \mathbb{R}^d}{J(\theta)} =  \mathbb{E}_{\phi \sim N(\theta, \sigma^2I)}{[F(\phi)]}\]</span></p>
<p><span class="math display">\[\nabla J(\theta) = \frac1\sigma \mathbb{E}_{\epsilon \sim N(0, I)}{[F(\theta+\sigma\epsilon)\epsilon]}\]</span></p>
<p>Saliman(2017)</p>
<h2 id="gradient-sensing-as-a-mc-estimation">Gradient sensing as a MC estimation</h2>
<p>ES-style vanilla gradient estimator: <span class="math display">\[\hat{\nabla}_NJ(\theta) = \frac{1}{N\sigma}\sum{F(\theta+\sigma\epsilon_i)\epsilon_i}\]</span></p>
<p>gradient estimator with <strong>antithetic pairs</strong></p>
<p><span class="math display">\[\hat{\nabla}_NJ(\theta) =\frac{1}{2N\sigma}\sum{(F(\theta+\sigma\epsilon_i)\epsilon_i - F(\theta-\sigma\epsilon_i)\epsilon_i)}  \]</span></p>
<p>FD style, randomized</p>
<p><span class="math display">\[\hat{\nabla}_NJ(\theta) =\frac{1}{N\sigma}\sum{(F(\theta+\sigma\epsilon_i)\epsilon_i - F(\theta)\epsilon_i)}\]</span></p>
<ul>
<li>all 3 estimators are unbiased</li>
<li>none dominates</li>
</ul>
<p>It can be proved that the orthogonal(structured) gradient estimator <span class="math inline">\(\hat{\nabla}_N^{ort}J(\theta)\)</span> is unbiased and yields lower MSE than the unstructured estimator.</p>
<h2 id="coupled-anithetic-pairs-for-mc-estimation">Coupled anithetic pairs for MC estimation</h2>
<p><span class="math display">\[\hat{\nabla}_NJ(\theta) =\frac{1}{2N\sigma}\sum{(F(\theta+\sigma\epsilon_i)\epsilon_i - F(\theta-\sigma\epsilon_i&#39;)\epsilon_i&#39;)}  \]</span></p>
<p><span class="math inline">\(\epsilon_i&#39;\)</span> inverse length.</p>
<p>Or more complex formulation.</p>
<h2 id="rbo-robust-blackbox-optimization">RBO-Robust Blackbox Optimization</h2>
<p><span class="math display">\[\hat{\nabla}_{RBO}^{ort}J(\theta) = \text{arg} \min_{v\in R^d} \frac1{2N} \|y-Zv\|^p_p + \alpha  \|v\|^q_q\]</span></p>
<p>re-used perturbations</p>
<h2 id="efficiency-of-the-orthogonal-exploration-for-gradient-sensing---structured-nns.">Efficiency of the orthogonal exploration for gradient sensing - structured NNs.</h2>
<p>Structurized Matrices: Toeplitz, <span class="math inline">\(n\times n\)</span> with <span class="math inline">\(2n-1\)</span> parameters. constant diagonal values.</p>
<p><span class="math inline">\(O(n\log n)\)</span> complexity</p>
<h2 id="rbo-noisy-measurements">RBO &amp; noisy measurements</h2>
<p>under noise 0.4, the MC still manages to reach the same accuracy</p>
<p>[Neuroevolution of self-interpretation]</p>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Data-Mining-Lec2</title>
    <url>/2020/09/17/Data-Mining-Lec2/</url>
    <content><![CDATA[<h2 id="lemmas">(09/20)Lemmas</h2>
<h3 id="eulers-formula-欧拉公式">Euler's formula 欧拉公式</h3>
<p><span class="math display">\[ e^{ix} = \cos{x} + i\sin{x} \]</span></p>
<h3 id="fourier-transform-傅里叶变换">Fourier transform 傅里叶变换</h3>
<p><a href="https://www.zhihu.com/question/19714540/answer/1119070975">如何理解傅里叶变换公式？ - 苗华栋的回答 - 知乎</a></p>
<h4 id="三角函数正交性">三角函数正交性</h4>
<p><span class="math display">\[ \sum_{-\pi}^{\pi}{\sin{mx}\sin{nx} dx} = \begin{cases} 0, m\neq n \\ \pi, m = n \end{cases} \]</span></p>
<p>证明： 积化和差，周期内积分。</p>
<h4 id="正余弦函数组成正交基时域-to-频域">正余弦函数组成正交基，时域 <span class="math inline">\(\to\)</span> 频域</h4>
<p><span class="math display">\[ 1, cos(x), sin(x), cos(2x), sin(2x), ... cos(nx), sin(nx) \]</span></p>
<h4 id="公式实值函数时">公式(实值函数时)</h4>
<p>频率表示为 <span class="math inline">\(2 \pi f\)</span></p>
<p><span class="math display">\[ s_{N}(x) = \frac{a_0}{2} + \sum_{n = 1}^{\infty}{[a_n\cos{(2 \pi fnx)} + b_n\sin{(2 \pi fnx)}]} \]</span></p>
<p><span class="math display">\[a_{n} = A_{n}\sin{(\phi_{n})}, b_{n} = A_{n}\cos{(\phi_{n})}\]</span></p>
<p><span class="math inline">\(\frac{a_0}{2}\)</span> 对应直流份量，任何连续周期信号都可以由一组适当的正弦曲线组合而成。（非周期信号的周期认为是无穷大）</p>
<p>一般地，可以用正交性求 <span class="math inline">\(a_{n}\)</span> 和 <span class="math inline">\(b_{n}\)</span>。</p>
<p><span class="math display">\[ a_{n} = 2f \int_{x_0}^{x_0 + \frac{1}{f}}{s(x)\cdot \cos{(2\pi fnx)}dx} \]</span> <span class="math display">\[ b_{n} = 2f \int_{x_0}^{x_0 + \frac{1}{f}}{s(x)\cdot \sin{(2\pi fnx)}dx} \]</span></p>
<h1 id="shift-invariant-kernels">Shift-invariant Kernels</h1>
<h2 id="random-fourier-featuresrff">Random Fourier Features(RFF)</h2>
<p><a href="https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf">Random Features for Large-Scale Kernel Machines</a></p>
<p><span class="math display">\[ k(x,y) = &lt;\phi(x), \phi(y)&gt; = z(x)&#39;z(y) \]</span></p>
<p>Gaussian: <span class="math display">\[ e^{-\frac{||\Delta||^2}{2}} \]</span> Shift-invariant: x + z, y + z, delta remains, Radial basis function (<strong>RBF</strong>) kernel</p>
<p>The only thing that matters is the length of <span class="math inline">\(\Delta\)</span></p>
<p>Random Feature Map function of Gaussian: <span class="math display">\[ \Phi(x) = \frac{1}{\sqrt{m}} (\sin(w_1^Tx),...\sin(w_m^Tx), \cos(w_1^Tx),...\cos(w_m^Tx)) \]</span></p>
<h3 id="bochners-theorem"><strong>Bochner's Theorem</strong></h3>
<p>A continuous kernel <span class="math inline">\(k(x, y) = f(x − y)\)</span> on <span class="math inline">\(\mathbb{R}^d\)</span> is positive definite if and only if <span class="math inline">\(k(δ)\)</span> is the Fourier transform of a non-negative measure. Here ignore the imaginery part <span class="math inline">\(\sin\)</span>.</p>
<p><span class="math display">\[ K: \mathbb{R}^d \times \mathbb{R}^d \to R \]</span> <span class="math display">\[ K(x,y) = f(x-y) = f(z) = \int_{\mathbb{R}^d}{p(w)cos(w^Tz)dw}=E_{w \sim \Omega}[cos(w^Tz)]\]</span> where <span class="math inline">\(p\)</span> is the probability distribution corresponding to <span class="math inline">\(k\)</span></p>
<p>Example, if <span class="math inline">\(K\)</span> is Gaussian then <span class="math inline">\(\Omega = N(0, I_d)\)</span></p>
<p>Since we have the form of expectation, we run <strong>simulations</strong> or say <strong>[Monte Carlo]</strong> to estimate shift-invariant kernels.</p>
<ul>
<li>choose your number of random samples</li>
<li>Sample <span class="math inline">\(w_1, ..., w_m \sim{iid} \Omega\)</span></li>
<li>Estimate: <span class="math inline">\(K(x,y)\)</span> as <span class="math inline">\(\hat{K}(x,y)\)</span></li>
<li><span class="math inline">\(\hat{K}(x,y)\)</span>: the mean of the sum, unbiased estimation of the original kernel.</li>
</ul>
<p><span class="math display">\[ \hat{K}(x,y) = \frac{1}{m} \sum_{i=1}^{m}{cos(w_i^Tz)} = \frac{1}{m} \sum_{i=1}^m{X_i} \]</span></p>
<ul>
<li><span class="math inline">\(X_i\)</span> is iid and bounded, strong concentration results.</li>
<li><span class="math inline">\(\hat{K}(x,y) = \phi(x)^T\phi(y)\)</span> ? How we can disentangle <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> from <span class="math inline">\(z\)</span>? cosine identities.</li>
</ul>
<p><span class="math display">\[ \hat{K}(x,y) = \frac{1}{m} \sum_{i=1}^{m}{cos(w_i^Tx - w_i^Ty)} \]</span> <span class="math display">\[ = \frac{1}{m} \sum_{i=1}^{m}{[cos(w_i^Tx)cos(w_i^Ty) + sin(w_i^Tx)sin(w_i^Ty)]} \]</span></p>
<ul>
<li><p>Disentangled!</p></li>
<li><p>Define</p></li>
</ul>
<p><span class="math display">\[ \phi(x) = \frac{1}{\sqrt{m}} \begin{bmatrix}\cos(w_1^Tx)\\...\\\cos(w_m^Tx)\\\sin(w_1^Tx)\\...\\\sin(w_m^Tx) \end{bmatrix} \]</span> <span class="math display">\[ \hat{K}(x,y) = \phi(x)^T\phi(y) \]</span></p>
<ul>
<li>Now <span class="math inline">\(w\)</span> can be distributions other than Gaussian: <span class="math inline">\(w_i \sim iid \Omega\)</span></li>
</ul>
<h3 id="attention-model---softmax-kernel-and-random-feature-map">Attention Model - Softmax kernel and random feature map</h3>
<p>Sequence. We wanna find out similarity between embeddings(tokens) and how they attend to each other. We use kernels to calculate this attention.</p>
<p>Construct a huge matrix (len*len).</p>
<p>One most popular similarity model is <strong>softmax attention</strong></p>
<p>Softmax kernel: <span class="math display">\[ K_{SM}(x,y) = e^{x^Ty}\]</span></p>
<p>Relationship between gaussian kernel and softmax kernel? Just by expanding <span class="math display">\[ K_{Gauss}(x,y) = K_{SM}(x,y) e^{-\frac{||x||^2}{2}} e^{-\frac{||y||^2}{2}} \]</span></p>
<p><a href="https://arxiv.org/pdf/2006.03555.pdf">Masked Language Modeling for Proteins via LinearlyScalable Long-Context Transformers</a> This one covers the relationship between two kernels.</p>
<p><span class="math display">\[ SM(x,y) = e^{\frac{||x||^2}{2}}K_{Gauss}(x,y)e^{\frac{||y||^2}{2}} \]</span></p>
<p>From above, <span class="math inline">\(K(x,y) = E[\hat{K}(x,y)]\)</span>, <span class="math display">\[ K_{Gauss}(x,y) = E[\phi(x)^T\phi(y)] \]</span></p>
<p><span class="math display">\[ \hat{SM}(x,y) = e^{\frac{||x||^2}{2}}\phi(x)^T\phi(y)e^{\frac{||y||^2}{2}} \]</span> <span class="math display">\[ = (e^{\frac{||x||^2}{2}}\phi(x))^T(e^{\frac{||y||^2}{2}}\phi(y)) \]</span> <span class="math display">\[ \phi_{SM}(x) = e^{\frac{||x||^2}{2}}\phi_{Gauss}(x) = e^{\frac{||x||^2}{2}} \frac{1}{\sqrt{m}} \begin{bmatrix}\cos(w_1^Tx)\\...\\\cos(w_m^Tx)\\\sin(w_1^Tx)\\...\\\sin(w_m^Tx) \end{bmatrix} \]</span> <span class="math display">\[ SM(x,y) = E[\hat{SM}(x,y)] = E[\phi_{SM}(x)^T\phi_{SM}(y)] \]</span></p>
<h1 id="orthogonal-random-featuresorf">Orthogonal random features(ORF)</h1>
<p><a href="https://papers.nips.cc/paper/6246-orthogonal-random-features.pdf">https://papers.nips.cc/paper/6246-orthogonal-random-features.pdf)</a></p>
<h2 id="standard-setting-for-constructing-random-features-via-gaussian-projections">Standard setting for constructing random features via Gaussian projections</h2>
<p><strong>IID</strong>: <span class="math inline">\(w_1, ..., w_m \sim{iid} \Omega\)</span>. Sampling independently from canonical multivariate gaussian distribution.</p>
<h2 id="constructing-orthogonal-random-features">Constructing orthogonal random features</h2>
<h3 id="definition">Definition</h3>
<p><strong>ORT</strong>: for variance reduction, <span class="math inline">\(w_1, ..., w_m\)</span> sampled in a way that,</p>
<ul>
<li><p>marginal distributions are still the same as for IID, i.e. <span class="math inline">\(w_1 \sim{N(0,I_d)}\)</span></p></li>
<li><p>different samples <span class="math inline">\(w_i\)</span> are conditioned to <strong>be exactly and definitely orthogonal</strong>, i.e. <span class="math inline">\(w_i^Tw_j = 0\)</span> for <span class="math inline">\(i\neq j\)</span></p></li>
</ul>
<p>They are not independent(if i.i.d, although <span class="math inline">\(E=0\)</span>, <span class="math inline">\(Var\neq 0\)</span>). Rows have to be <strong>deterministic</strong> in the Linear Space.</p>
<h3 id="how-we-obtain-this-orthogonality">How we obtain this orthogonality?</h3>
<ol type="1">
<li>Create Gaussian matrix <span class="math inline">\(G\in \mathbb{R}^{m\times d}\)</span> with entries (a.k.a <span class="math inline">\(w_i\)</span>) taken independently at random from scalar Gaussian distribution N(0,1).</li>
<li><strong>Gram-Schmidt</strong> orthogonalize <span class="math inline">\(G\)</span>. After this rows of G are exactly orthogonal and of unit L2-norm.</li>
<li>Renormalize each row of the resulting matrix by multiplying via random variable token from <span class="math inline">\(\chi(d)\)</span> distribution. The renormalization for each row can be same or different.</li>
<li><strong>Note</strong>: the procedures are feasible only when <span class="math inline">\(m \leq d\)</span></li>
</ol>
<h4 id="gram-schmidt-orthogonalization-施密特正交化"><strong>Gram-schmidt orthogonalization 施密特正交化</strong></h4>
<p>Gram–Schmidt process is a method for orthonormalizing a set of vectors in an inner product space, most commonly the Euclidean space Rn equipped with the standard inner product. The Gram–Schmidt process takes a finite, linearly independent set S = {v1, ..., vk} for <strong>k ≤ n</strong> and generates an orthogonal set S′ = {u1, ..., uk} that spans the same k-dimensional subspace of Rn as S.</p>
<p>Whenever move to the next row, subtract the projection from previous rows.</p>
<h2 id="complexity-analysis">Complexity analysis</h2>
<p>D: random features, d: dimensionality</p>
<p>ORF is as expensive as RFF.</p>
<ul>
<li>ORF:
<ul>
<li>generating <span class="math inline">\(d\times d\)</span> orthogonal matrix, <span class="math inline">\(O(d^3)\)</span> time and <span class="math inline">\(O(d^2)\)</span> space</li>
<li>computing the transformation, <span class="math inline">\(O(d^2)\)</span> time and space</li>
</ul></li>
</ul>
<h2 id="structured-orf-sorf-...to-be-continued">Structured ORF (SORF) ...to be continued</h2>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Data-Mining-Lec4</title>
    <url>/2020/10/01/Data-Mining-Lec4/</url>
    <content><![CDATA[<h1 id="attention-mechanism">Attention Mechanism</h1>
<p>Attention block: takes a sequence of vectors, <span class="math inline">\(x_1, ... x_L (\mathbb{R}^d)\)</span>, and output another series <span class="math inline">\(x_1, ... x_L (\mathbb{R}^d)\)</span></p>
<p>Within the block, <span class="math inline">\(W_Q, W_K, W_V\)</span>, <span class="math inline">\(X = \begin{bmatrix}x_1\\x_2\\...\\x_L\end{bmatrix} \in \mathbb{R}^{L\times d}\)</span> <span class="math display">\[ \mathbb{R}^{L\times d}\begin{cases} Q = W_QX\\ K = W_KX\\V=W_VX\end{cases} \]</span> <span class="math display">\[ Q = \begin{bmatrix}q_1\\q_2\\...\\q_L\end{bmatrix} \]</span> all these weight matrices will be learned.</p>
<p>rows:<span class="math inline">\(q_1, ..., q_L\)</span>, columns: <span class="math inline">\(k_1, ..., k_L\)</span> compose a orthogonal matrix A. <span class="math display">\[ A_{ij} = K(q_i, k_j) \]</span></p>
<p>...</p>
<h3 id="not-normalized-attention-av">Not normalized attention: <span class="math inline">\(AV\)</span></h3>
<p>The rows would be embeddings</p>
<p><span class="math display">\[\begin{bmatrix} x_1&#39;\\...\\x_L&#39;\end{bmatrix} = AV \in \mathbb{R}^{L\times L}\]</span> <span class="math display">\[ A:L\times L; V: L\times d\]</span> <span class="math display">\[ x_i&#39; = \sum_{j=1}^{L}{K(q_ik_j)v_j} \]</span></p>
<h3 id="normalized-attention">Normalized attention:</h3>
<p><span class="math display">\[ x_i&#39; = \sum_{j=1}^{L}{\frac{K(q_ik_j)v_j}{\sum_{s=1}^{L}{K(q_i, k_s)}}} \]</span> The weight sum up to 1 and &gt;=0; <span class="math inline">\(K: \mathbb{R}^d\times \mathbb{R}^d: \mathbb{R}^+\)</span></p>
<p><span class="math display">\[D^{-1}AV\]</span> D: diagonal L*L,</p>
<p>partition function: <span class="math display">\[ d_i = \sum_{s=1}^{L}{K(q_i, k_s)} \]</span></p>
<h2 id="unidirectional-attention">Unidirectional Attention</h2>
<h3 id="not-normalized-setting">Not-normalized setting:</h3>
<p><span class="math display">\[ A_{masked}V \]</span></p>
<p>masked attention matrix, zero out uptriangular part.</p>
<p><span class="math display">\[ A_{masked} =...\]</span></p>
<h3 id="normalized">Normalized:</h3>
<p><span class="math display">\[D_{masked}^{-1}A_{masked}V\]</span></p>
<p><span class="math display">\[D: diagonal: L*L\]</span></p>
<p><span class="math display">\[ d_i = \sum_{s=1}^{i}{K(q_i, k_s)} \]</span></p>
<h2 id="problems-with-standard-attention-algorithm">Problems with standard attention algorithm</h2>
<ul>
<li>Time and space complecity for computing attention is quadratic in L (cannot be used for very long sequences)</li>
</ul>
<h3 id="sparsification">Sparsification:</h3>
<ul>
<li>attend just to a few tokens(either learned or fixed)
<ul>
<li>in a unidirectional case (lower triangular part is non-zero) last <span class="math inline">\(l\)</span> tokens (like a column till the diagonal);</li>
<li>in a bidirectional case, closest <span class="math inline">\(l\)</span> tokens. (Like a diagonal strip)</li>
</ul></li>
</ul>
<p>for <code>Graph data</code>, people often attend only to neighbors (graph attention methods).</p>
<ul>
<li>attend to a few tokens, but learn those that you would like to attend to.</li>
</ul>
<p>How to choose what <span class="math inline">\(k\)</span>s to attend to? Close ones. Choose 10 closests.</p>
<p>if <span class="math inline">\(Q=K(W_Q = W_K)\)</span>, can <strong>cluster</strong> queries into groups</p>
<ul>
<li>clustering</li>
<li>hashing, nearest neighbour approach, 10 closest neighbours, code the query to reduce complexity</li>
</ul>
<h2 id="efficient-dense-attention">Efficient Dense Attention</h2>
<p>Approximate the attention? While the matrix is still dense? Decomposition.</p>
<h3 id="bidirectional-not-normalized-attention">Bidirectional not-normalized attention</h3>
<p><span class="math inline">\(AV\)</span></p>
<p>Let's try to rewrite <span class="math inline">\(A\)</span> as <span class="math inline">\(A\approx F_1\cdot F_2 \cdot... \cdot F_L\)</span> for some simpler matrices <span class="math inline">\(F_i\)</span></p>
<p><span class="math display">\[ (F_1\cdot F_2\cdot...\cdot F_L)\cdot V = F_1\cdot (F_2\cdot(...\cdot(F_L\cdot V)...) \]</span> <span class="math display">\[ AV \in \mathbb{R}^{L \times d} \]</span></p>
<p>Q: Can we rewrite <span class="math inline">\(A = F_1 \cdot F_2, L\times L = L \times m * m\times L\)</span>, where <span class="math inline">\(m &lt; L\)</span> <span class="math inline">\(A\)</span> can be full rank (<span class="math inline">\(\det A \neq 0\)</span>)</p>
<p>large matrices on the diagonal, small every where else</p>
<p>Look at the rank, <span class="math inline">\(F_1, F_2\)</span> has rank <span class="math inline">\(\leq m\)</span>, so no solution.</p>
<p>What aboyut random <span class="math inline">\(F_i\)</span>?</p>
<ul>
<li>Can we find random matrices <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span> such that:
<ul>
<li><span class="math inline">\(A = E[F_1F_2]\)</span></li>
<li>the error of the approximation small</li>
</ul></li>
</ul>
<p><span class="math display">\[ A_{ij} = e^{\frac{q_ik_j^T}{\sqrt{d}}} = e^{\frac{q_i}{d^{\frac{1}{4}}}}... \]</span></p>
<p>We conclude that</p>
<p><span class="math display">\[ A_{ij} = E[\phi(\bar{q_i})\phi(\bar{k_j}^T)] \]</span></p>
<p><span class="math display">\[ Q&#39; = \begin{bmatrix} \phi(\bar{q_1})\\...\\\phi(\bar{q_L}) \end{bmatrix} \]</span> <span class="math display">\[ K&#39; = \begin{bmatrix} \phi(\bar{k_1})\\...\\\phi(\bar{k_L}) \end{bmatrix} \]</span></p>
<p><span class="math display">\[ Q&#39;(K&#39;)^T_{ij} =  \phi(\bar{q_i})\phi(\bar{k_j})^T\]</span> <span class="math display">\[ E[Q&#39;(K&#39;)^T_{ij}] =  E[\phi(\bar{q_i})\phi(\bar{k_j})^T]\]</span> <span class="math display">\[ A = E[Q&#39;(K&#39;)^T]\]</span></p>
<p><span class="math display">\[ \phi_{SM}(x) = e^{\frac{||x||^2}{2}}\phi_{Gauss}(x) = e^{\frac{||x||^2}{2}} \frac{1}{\sqrt{m}} \begin{bmatrix}\cos(w_1^Ty)\\...\\\cos(w_m^Ty)\\\sin(w_1^Ty)\\...\\\sin(w_m^Ty) \end{bmatrix} \]</span></p>
<p><span class="math display">\[ AV = E[Q&#39;(K&#39;)^T]V = E[Q&#39;(K&#39;)^TV] \]</span></p>
<p><span class="math display">\[ AV \approx Q&#39;K&#39;^TV = Q&#39;(K&#39;^TV)\]</span></p>
<p><span class="math display">\[ Q\in \mathbb{R}^{L\times m}, K&#39; \in \mathbb{R}^{m\times L}, V \in \mathbb{R}^{L\times d} \]</span> <span class="math display">\[ m = 2v \]</span> space complexity: <span class="math inline">\(O(L\times d + m\times d + L\times m)\)</span></p>
<p>time complexity: <span class="math inline">\(O(mLd)\)</span> versus <span class="math inline">\(O(L^2d)\)</span> for standard attention complexity.</p>
<p>if <span class="math inline">\(m &lt;&lt; L\)</span></p>
<h2 id="problems-with-trignometric-features">Problems with trignometric features</h2>
<p>for a row, <span class="math display">\[ \sum_{s=1}^{L}{K(q_i, ks)}\]</span> if lots of entries in a row are close to 0 then lots of estimators' values could be potentially <strong>negative</strong></p>
<p>In general, as long as kernel used for attention can be written as</p>
<p><span class="math display">\[ K(x,y) = E[\phi(x)\phi(y)^T] \]</span></p>
<p>for some <span class="math inline">\(\phi: \mathbb{R}^{d} \to \mathbb{R}^{m} (m &lt; \infty)\)</span> deterministic or random, we can get attention computation mechanism with $O(mLd) time complexity and O(mL+Ld+md) space complexity</p>
<p>Remark:</p>
<p>if random features are used, random features for attention computation should be periodically redrawn in downstream algorithms using attention-based models.</p>
<p>Remark:</p>
<p>in practice, it suffices to take <span class="math inline">\(m: O(d\log d)\)</span> to ahve accurate estimation of the attention matrix.</p>
<p>as long as <span class="math inline">\(d &lt;&lt; L\)</span>, presented mechanism provides space and time complexity gains.</p>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Data-Mining-Lec7</title>
    <url>/2020/10/22/Data-Mining-Lec7/</url>
    <content><![CDATA[<h2 id="some-hints-on-midterm">Some hints on Midterm</h2>
<p>Problem1: PSD: Random feature decomposition to prove</p>
<p>Problem2: Find a random feature map of the product of two kernels, angular might be negative</p>
]]></content>
  </entry>
  <entry>
    <title>Data-Mining-Lec3</title>
    <url>/2020/09/24/Data-Mining-Lec3/</url>
    <content><![CDATA[<h2 id="general-scheme">General scheme</h2>
<p>Approximating the kernel <span class="math inline">\(K(x,y) : \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}\)</span></p>
<p><span class="math inline">\(\phi: \mathbb{R}^d \to \mathbb{R}^m\)</span> randomized</p>
<p><span class="math inline">\(K(x,y) = E[\phi(x)^T\phi(y)]\)</span> <span class="math display">\[\phi(x) = \frac{h(x)}{\sqrt{m}}\begin{bmatrix}f_1(w_1^Tx), ..., f_1(w_m^Tx)\\f_2(w_1^Tx), ..., f_2(w_m^Tx)\\...\\f_l(w_1^Tx), ..., f_l(w_m^Tx)\end{bmatrix}\]</span> for some <span class="math inline">\(h: \mathbb{R}^d \to \mathbb{R}\)</span></p>
<p>Example:</p>
<ol type="1">
<li>Dot-product kernel, <span class="math inline">\(l = 1\)</span>, <span class="math inline">\(f_1(x)=x\)</span>, <span class="math inline">\(h(x)=1\)</span>, <span class="math inline">\(\Omega = N(0, I_d)\)</span></li>
<li>Gaussian kernel, <span class="math inline">\(l=2\)</span>, <span class="math inline">\(f_1(x)=\cos(x), f_2(x)=\sin(x)\)</span>, <span class="math inline">\(\Omega = N(0, I_d)\)</span></li>
<li>Softmax kernel, <span class="math inline">\(l=2\)</span>, <span class="math inline">\(f_1(x)=\cos(x), f_2(x)=\sin(x)\)</span>, <span class="math inline">\(\Omega = N(0, I_d)\)</span>, renormalizer <span class="math inline">\(h(x) = e^{\frac{||x||^2}{2}}\)</span></li>
<li>Angular kernel, <span class="math inline">\(l=1, f_1(x)=sgn(x), h(x)=1, \Omega = N(0,I_d)\)</span></li>
</ol>
<h2 id="orthogonal-random-features">Orthogonal Random Features</h2>
<p><span class="math inline">\(w_1, ..., w_m \sim \Omega\)</span>, construct pdf function on a fixed isotopic sphere</p>
<p>ORFs:</p>
<p><span class="math inline">\(\to w_i \sim \Omega\)</span></p>
<p><span class="math inline">\(\to w_i^Tw_j = 0\)</span> with prob 1 for <span class="math inline">\(i \neq j\)</span></p>
<ol type="1">
<li>Why do we need ORFs?</li>
<li>How to construct them?</li>
</ol>
<p>We use ORFs reduce the variance of estimator. Then each new <span class="math inline">\(w_i\)</span> provides new information. We can have fewer features.</p>
<h2 id="recipe-for-constucting-orfsonly-for-omega-isotopic">Recipe for constucting ORFs(Only for <span class="math inline">\(\Omega\)</span>-isotopic)</h2>
<p>Only when <span class="math inline">\(m\leq d\)</span></p>
<ol type="1">
<li>Constuct <span class="math inline">\(w_1, ..., w_m \sim iid \Omega\)</span></li>
<li>Construct matrix G:</li>
</ol>
<p><span class="math display">\[ G = \begin{pmatrix}w_1^T\\...\\w_m^T\end{pmatrix} \in \mathbb{R}^{m\times d}\]</span></p>
<ol start="3" type="1">
<li><p>Conduct Gram-Schmidt orthogon of <span class="math inline">\(G_{ort}\)</span> to get with L2-normalized rows</p></li>
<li><p>Renormalize each row of <span class="math inline">\(G_{ort}\)</span> simply by multiplying it with a scalar random variable <span class="math inline">\(X\sim \Omega\)</span>, distribution of lengths of vectors taken from <span class="math inline">\(\Omega\)</span></p></li>
</ol>
<h2 id="proxies">Proxies</h2>
<p><span class="math inline">\(G_{ort} \to ?\)</span></p>
<ol type="1">
<li>Random-Hadamand Matrices <span class="math display">\[ G_{ort} = (\frac{1}{\sqrt{d}}HD_1)...(\frac{1}{\sqrt{d}}HD_k) \]</span> for some <span class="math inline">\(k\in N_{+}\)</span></li>
</ol>
<p><span class="math display">\[ D_i = diag(\tau_i^1, ... \tau_i^d) \]</span> <span class="math display">\[\tau_i \sim iid \{-1, +1\} \]</span></p>
<p>H is a Hadamand matrix (to be more precise, H is called a Kronecker-Hadamand matrix) <span class="math display">\[ H_0 = [1] \in \mathbb{R}^{1\times 1} \]</span> <span class="math display">\[ H_{t+1} = \begin{bmatrix} H_t, H_t\\ H_t, -H_t \end{bmatrix} \]</span></p>
<ul>
<li><span class="math inline">\(H_t \in \mathbb{R}^{2^t \times 2^t}\)</span> has orthogonal rows/columns.</li>
<li><span class="math inline">\(H_t\)</span> is symmetric</li>
</ul>
<p><span class="math display">\[ G_{ort} \in \mathbb{R}^{d \times d} = (\frac{1}{\sqrt{d}}HD_1)...(\frac{1}{\sqrt{d}}HD_k) \]</span> H have to be <span class="math inline">\(\mathbb{R}^{d \times d}\)</span>, what if not power of 2? <strong>Zero padding</strong> to our input data.</p>
<ul>
<li>Larger k implies better approximation of the original mechanism, <span class="math inline">\(\frac{1}{\sqrt{d}}\)</span> is normalized orthogonal, doing this for k times</li>
</ul>
<ol start="2" type="1">
<li>Givens rotations</li>
</ol>
<p><span class="math display">\[ G_{ort} = Giv_1 Giv_2...Giv_k, k = O(d\log{d}) \]</span></p>
<p>Givens rotations: ... diagonal 1, where i, j exhibits <span class="math display">\[ \begin{bmatrix} \cos{\theta}, \sin{\theta}\\ -\sin{\theta}, \cos{\theta}\end{bmatrix} \]</span></p>
<p>The ith and jth rotated <span class="math inline">\(\theta\)</span></p>
<h2 id="neural-networks-attention-mechanism">Neural Networks &amp; Attention Mechanism</h2>
<p>Our object of interest: sequential data <span class="math inline">\((x_1, ..., x_L)\)</span> -&gt; L-tuple, each <span class="math inline">\(\in \mathbb{R}^d\)</span></p>
<h3 id="examples">Examples</h3>
<ol type="a">
<li><p>text data, where <span class="math inline">\(x_i\)</span>s correspond to embeddings of words.</p></li>
<li><p>music data, where <span class="math inline">\(x_i\)</span>s correspond to embeddings of notes.</p></li>
<li><p>bio-informatics, protein-chains, embeddings of amino-acids.</p></li>
</ol>
<h3 id="self-attention">Self-Attention</h3>
<p><span class="math inline">\((x_1, ..., x_L)\)</span>, understanding how the tokens attend to each other</p>
<p>similarity functions: <span class="math inline">\(K(x_i, x_j) \to \mathbb{R}\)</span></p>
<h2 id="attention-matrix-a">Attention matrix A</h2>
<p><span class="math inline">\(A \in \mathbb{R}^{L \times L}\)</span></p>
<p><span class="math inline">\(A_{i,j} = K(x_i, x_j)\)</span></p>
<p>a.k.a kernel matrix corresponding to Kernel sequence <span class="math inline">\((x_1, ..., x_L)\)</span></p>
<p>usually fixed kernel, <span class="math inline">\(K(x_i, x_j) = SM(x_i, x_j) = e^{x_i^Tx_j}\)</span></p>
<p>we don't want the value to be exponentially large.</p>
<p>By renormalizing, <span class="math inline">\(e^{x_i^Tx_j} \to e^{\frac{x_i^Tx_j}{\sqrt{d}}}\)</span></p>
<p><span class="math inline">\(K(x_i, x_j) = e^{(w_1x_i)^T(w_2x_j)}\)</span></p>
<p>Thinking about xis as row vectors,</p>
<p>e<sup>{(w_1x_i)</sup>T(w_2x_j)}$</p>
<p><span class="math inline">\(K(x_i, x_j) = SM(x_i, x_j) = e^{x_ix_j^T}\)</span></p>
<p><span class="math display">\[W_Q, W_k: K_{W_Q, W_k}(x_i, x_j) = SM(x_iW_Q, x_jW_k) \]</span> <span class="math display">\[ = e^{x_iW_QW_K^Tx_j^T} \]</span></p>
<p><span class="math inline">\(q_i = x_iW_Q\)</span>, query-vector corresponding to <span class="math inline">\(x_i\)</span> <span class="math inline">\(k_j = x_jW_k\)</span>, key-vector corresponding to <span class="math inline">\(x_i\)</span></p>
<p>use <span class="math inline">\(x_i\)</span> to calculate <span class="math inline">\(q_i\)</span>, <span class="math inline">\(k_i\)</span>, see how <span class="math inline">\(q_i\)</span> attends to <span class="math inline">\(k_j\)</span>,<span class="math inline">\(q_j\)</span> attends to <span class="math inline">\(k_i\)</span></p>
<p><span class="math inline">\(A_{i,j} = e^{\frac{q_ik_j^T}{\sqrt{d}}}\)</span></p>
<p>L(L-tuple) -&gt; [attention block] -&gt; another L-tuple -&gt; [attention block2] ...</p>
<p>...</p>
<p>Output of the attention: L AV = [A(L*L) * [V(L*d) <span class="math inline">\(V_i = X_i \cdot W_V\)</span></p>
<h3 id="attention-block-version-i">Attention Block version I</h3>
<p>Input: <span class="math inline">\(X\)</span></p>
<p>Hyperparams: <span class="math inline">\(W_Q, W_K, W_V\)</span></p>
<p>Compute <span class="math inline">\(Q = XW_Q \in \mathbb{R}^{L\times d}\)</span>, and so are <span class="math inline">\(K, V\)</span></p>
<p>Output: <span class="math inline">\(AV\)</span> where <span class="math inline">\(A = e^{\frac{QK^T}{\sqrt{d}}}\)</span></p>
<p>Attention block is parametrized by 3 matrices.</p>
<p><span class="math inline">\(x_i\)</span> to <span class="math inline">\(q_i, k_i, v_i\)</span></p>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Data-Mining-Lec5</title>
    <url>/2020/10/08/Data-Mining-Lec5/</url>
    <content><![CDATA[<h2 id="transformers">Transformers</h2>
<p><strong>"Attention is all you need"</strong></p>
<h3 id="attention-block">Attention Block</h3>
<p>a sequence of feature vectors <span class="math inline">\(X = [x_1, ... x_L]^T \in \mathbb{R}^{L\times d}\)</span> are fed into an attention block.</p>
<p><span class="math inline">\(W_Q, W_K, W_V\)</span></p>
<p><span class="math inline">\(Q=XW_Q, K=XW_K, V=XW_V \in \mathbb{R}^{L\times d}\)</span></p>
<p><span class="math inline">\(A = e^{\frac{QK^T}{\sqrt{d}}}\in \mathbb{R}^{L\times L}\)</span></p>
<p>Elementwise exponential.</p>
<p><span class="math inline">\(AV \in \mathbb{R}^{L\times d}\)</span></p>
<h3 id="residual-connection">Residual Connection</h3>
<p>Skip connection, ResNet layer, (<strong>Residual connection</strong>), <span class="math inline">\(X + f(X)\)</span> where <span class="math inline">\(f\)</span> here is the attention block.</p>
<p>The adavantage of using Residual connection,...</p>
<p>Dynamical System: Initial condition as 0, revolutions,</p>
<p><span class="math inline">\(\frac{dx}{dt} = F(t)\)</span>: neural ODEs</p>
<h3 id="layernorm">LayerNorm</h3>
<p>After the attention block and skip connection, there is another <strong>LayerNorm</strong> layer. This normalization normalizes each input <span class="math inline">\(X = [x_1, ..., x_L]^T \to [\hat{x_i}...]^T\)</span></p>
<p><span class="math display">\[ \hat{x_i} = \frac{x_i - \frac{1}{d}_{j=i}x_i[j]}{stddev(x_1[1],...x_1[d])} \]</span></p>
<p>Other forms of norm: batch norm, ...</p>
<h3 id="mlp">MLP</h3>
<p>After the layernorm, we get <span class="math inline">\(\hat{X}\)</span>. Then it goes to multiple <strong>MLP</strong>s (<strong>MultiLayerPerceptron</strong>)</p>
<p>Perceptron: some computation unit. It takes some inputs <span class="math inline">\(y_1, y_2, y_3, y_4...\)</span>. Each channel has different weights<span class="math inline">\(w_1, w_2, ...\)</span>. The percepetron calculates the weighted sum <span class="math inline">\(\sum{w_iy_i}\)</span> and applies some nonlinear mapping <span class="math inline">\(\sigma(\sum{w_iy_i}): R \to R\)</span>.</p>
<p>Some examples of <span class="math inline">\(\sigma\)</span>: Sigmoid: <span class="math inline">\(\sigma(x) = \frac{1}{1+e^{-x}} \in [0,1)\)</span></p>
<p><span class="math inline">\(\tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}\)</span>: resized version of sigmoid</p>
<p>Relu(rectified linear unit): <span class="math display">\[y = \begin{cases} x, x&gt;0 \\ 0, x\leq 0 \end{cases}\]</span></p>
<h3 id="multilayer-how-to-organize-perceptrons">Multilayer: How to organize perceptrons</h3>
<ul>
<li><p>neurons in layer i broadcast signal to neurons in layer i+1.</p></li>
<li><p>weights of connections are learned(through optimizing)</p></li>
<li><p>Paralellization of different MLPs and finally layernorm</p></li>
</ul>
<h2 id="positive-random-features-for-softmax-and-gaussian-kernels">Positive random features for softmax and gaussian kernels</h2>
<p>Valari Likhoshevstov</p>
<p>Attention matrix: A</p>
<p>A: rows q, columns k</p>
<p><span class="math inline">\(A_{ij} = e^{q_ik_j^T}\)</span></p>
<p>renormalization: <span class="math display">\[\hat{A_{ij}} = \frac{e^{q_ik_j^T}}{\sum_{s=1}^{L}{e^{q_ik_s^T}}}\]</span> <span class="math display">\[ e^{q_ik_j^T} = SM(q_i, k_j) = E[\phi(q_i)\phi(k_j)]\]</span> where <span class="math display">\[\phi(x)\stackrel{def}{=} e^{\frac{||x||^2}{2}} \frac{1}{\sqrt{m}} \begin{bmatrix}\cos(w_1^Tx)\\...\\\cos(w_m^Tx)\\\sin(w_1^Tx)\\...\\\sin(w_m^Tx) \end{bmatrix}\]</span></p>
<p><span class="math display">\[w_i, ...w_m \sim N(0, I_d)\]</span></p>
<p>small value matters: error. Renormalize helps smaller values become accurate.</p>
<p><span class="math display">\[MSE(\hat{SM}_m^{trig}(x,y)) = \frac{1}{2m} exp(||x+y||^2)SM^{-2}(x,y)\times (1-\exp(-||x-y||^2))^2\]</span></p>
<p><span class="math display">\[SM(x,y) = E_{\omega \sim N(0, I_d)}[\exp(w^Tx - \frac{||x||^2}{2})\exp(w^Ty - \frac{||y||^2}{2})]\]</span></p>
<p><span class="math display">\[\phi_m^{+}(x) = e^{-\frac{||x||^2}{2}}\frac{1}{\sqrt{m}}\begin{bmatrix}\exp(w_1^Tx)\\...\\\exp(w_m^Tx) \end{bmatrix}\]</span></p>
<p><span class="math display">\[\hat{SM}(x,y)^{+}_{m} = \phi_m^{+}(x)(\phi_m^{+}(y))^T\]</span></p>
<h3 id="key-difference-btw-hatsmxytrig_m-and-hatsmxy_m">Key difference btw: <span class="math inline">\(\hat{SM}(x,y)^{trig}_{m}\)</span> and <span class="math inline">\(\hat{SM}(x,y)^{+}_{m}\)</span></h3>
<ul>
<li><span class="math inline">\(\hat{SM}(x,y)^{trig}_{m}\)</span> becomes arbitrarily accurate as <span class="math inline">\(x\to y\)</span></li>
<li><span class="math inline">\(\hat{SM}(x,y)^{+}_{m}\)</span> becomes ... as <span class="math inline">\(SM(x,y) \to 0\)</span></li>
</ul>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Data-Mining-Lec6</title>
    <url>/2020/10/15/Data-Mining-Lec6/</url>
    <content><![CDATA[<p><a href="https://syncedreview.com/2020/10/02/google-cambridge-deepmind-alan-turing-institutes-performer-transformer-slashes-compute-costs/#comments">Performers: variant of transformer</a></p>
<p>Random feature for different kernels Softmax: triangnometric, positive Orthogonal features construction: different ways(Givens, Hadamard, regular(GM), or even more), different renormalizations</p>
<p>concentration, <strong>computing variance</strong> of certain feature map, Cherbychev, concentration results</p>
<p>attention: ..., transformer, MLP, resnet, ...</p>
<h2 id="markovs-inequality">Markov's inequality</h2>
<p><span class="math display">\[ \mathbb{P}(Z\geq t) \leq \frac{\mathbb{E}[Z]}{t} , t\geq 0\]</span> Proof: <span class="math display">\[
\mathbb{P}(Z\geq t) = E[1\{Z\geq t\}] \leq E[\frac{Z}{t}]=\frac{\mathbb{E}[Z]}{t}
\]</span></p>
<h2 id="chebyshevs-inequality">Chebyshev's inequality</h2>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+%5Cmathbb%7BP%7D%28Z%5Cge+%5Cmathbb%7BE%7D%5BZ%5D%2Bt+~or~Z%5Cle+%5Cmathbb%7BE%7D%5BZ%5D-t%29%5Cle+%5Cfrac%7BVar%28Z%29%7D%7Bt%5E2%7D" /></p>
<p><img src="https://pic1.zhimg.com/v2-7e8070e499bcb47e78c3f668b8f00504_b.jpg" /> ## Hoeffding's inequality</p>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Data-Mining-Midterm</title>
    <url>/2020/10/19/Data-Mining-Midterm/</url>
    <content><![CDATA[<h1 id="data-mining-midterm">Data Mining Midterm</h1>
<p><strong>Hua Yao(UNI: hy2632)</strong></p>
<h2 id="problem-1-anisotropic-gaussian-kernels-50-points">Problem 1: Anisotropic Gaussian Kernels (50 points)</h2>
<p>Given: <span class="math display">\[K(x,y) = (2\pi)^{-\frac{d}{2}}(\det(\Sigma))^{-\frac{1}{2}}\exp(-\frac{1}{2}(x-y)^{\top}\Sigma^{-1}(x-y)), \]</span> <span class="math inline">\(\Sigma \in \mathbb{R}^{d\times d}\)</span> is positive definite symmetric.</p>
<h3 id="show-that-k-does-not-need-to-be-an-rbf-kernel-10-points">Show that K does not need to be an RBF kernel (10 points)</h3>
<p>Radius basis function kernel(RBF) is defined as <span class="math display">\[K(x,y) = \exp\big( -\frac{||x-y||^2}{2\sigma^2}  \big)\]</span> While <span class="math display">\[K(x,y) = \big[(2\pi)^{-\frac{d}{2}}(\det(\Sigma))^{-\frac{1}{2}}\big] \cdot \big[\exp(-\frac{1}{2}(x-y)^{\top}\Sigma^{-1}(x-y))\big]\]</span></p>
<p>Therefore, K is not necessarily an RBF for 2 reasons:</p>
<ul>
<li>The constant coefficient <span class="math inline">\((2\pi)^{-\frac{d}{2}}(\det(\Sigma))^{-\frac{1}{2}}\)</span> is not necessarily <span class="math inline">\(1\)</span></li>
<li><span class="math inline">\(\exp(-\frac{1}{2}(x-y)^{\top}\Sigma^{-1}(x-y))\)</span> does not necessarily have the form of <span class="math inline">\(\exp\big( -\frac{||x-y||^2_2}{2\sigma^2}\big)\)</span></li>
</ul>
<h3 id="give-necessary-and-sufficient-conditions-for-k-to-be-an-rbf-function-10-points">Give necessary and sufficient conditions for K to be an RBF function (10 points)</h3>
<h4 id="condition-1-2pi-fracd2detsigma-frac12-1">Condition 1: <span class="math inline">\((2\pi)^{-\frac{d}{2}}(\det(\Sigma))^{-\frac{1}{2}} = 1\)</span></h4>
<p><span class="math display">\[\det(\Sigma) = (\frac{1}{2\pi})^d\]</span></p>
<p>Also, since <span class="math inline">\(\Sigma\)</span> is positive definite symmetric, all of its eigenvalues are positive, <span class="math display">\[\therefore \det(\Sigma^{-1}) = (2\pi)^d \]</span></p>
<h4 id="condition-2-exp-frac12x-ytopsigma-1x-y-expbig--fracx-y22sigma2big">Condition 2: <span class="math inline">\(\exp(-\frac{1}{2}(x-y)^{\top}\Sigma^{-1}(x-y)) = \exp\big( -\frac{||x-y||^2}{2\sigma^2}\big)\)</span></h4>
<!-- Related: [Anisotropic Gaussian kernel adaptive filtering by Lie-group dictionary learning](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0237654#sec001) -->
<p><span class="math display">\[(x-y)^{\top}\Sigma^{-1}(x-y) = \frac{||x-y||^2}{\sigma^2}\]</span></p>
<p>Let <span class="math inline">\(z = x-y \in \mathbb{R}^{d}\)</span>,</p>
<p><span class="math display">\[z^{\top}\Sigma^{-1}z = \frac{||z||^2}{\sigma^2} = \frac{z^{\top}z}{\sigma^2} = z^{\top}\frac{I}{\sigma^2}z\]</span></p>
<p><span class="math display">\[z^{\top}(\Sigma^{-1}-\frac{I}{\sigma^2})z = \mathbf{0}\]</span></p>
<p><span class="math inline">\(\because \Sigma^{-1}, \frac{I}{\sigma^2}\)</span> are both real symmetric</p>
<p><span class="math display">\[\therefore \Sigma^{-1} - \frac{I}{\sigma^2} = 0\]</span></p>
<p><span class="math display">\[\therefore \Sigma^{-1} = \frac{I}{\sigma^2}, \Sigma = \sigma^2 I\]</span></p>
<h4 id="combine-results-from-the-previous-2-parts">Combine results from the previous 2 parts</h4>
<p>From Part 2, <span class="math display">\[\det({\Sigma}) = \prod_{i=1}^d{\lambda_i} = (\sigma^2)^d\]</span></p>
<p>From Part 1, <span class="math display">\[\det(\Sigma) = (\frac{1}{2\pi})^d\]</span></p>
<p><span class="math display">\[\therefore \sigma^2 = \frac{1}{2\pi}\]</span></p>
<h4 id="conclusion">Conclusion</h4>
<p>To sum up, the necessary and sufficient conditions for <span class="math inline">\(K\)</span> to be an RBF function is <span class="math display">\[\Sigma = \frac{1}{2\pi}I_d\]</span></p>
<p><span class="math display">\[\Sigma^{-1} = 2\pi I_d\]</span></p>
<h3 id="show-that-k-is-psd-function-and-provide-random-feature-map-phimathbbr_dmathbbr_m-for-k-25-points">Show that K is PSD function and provide random feature map <span class="math inline">\(\phi:\mathbb{R}_{d}→\mathbb{R}_{m}\)</span> for K (25 points)</h3>
<h4 id="random-feature-map-for-multivariate-gaussian">Random Feature Map for Multivariate Gaussian</h4>
<p><span class="math display">\[K(x,y) = (2\pi)^{-\frac{d}{2}}(\det(\Sigma))^{-\frac{1}{2}}\exp(-\frac{1}{2}(x-y)^{\top}\Sigma^{-1}(x-y)) \]</span></p>
<p><span class="math display">\[X = \{x_1, ...x_N\} \in \mathbb{R}^{d\times N}\]</span></p>
<p>For 1-d gaussian distribution, the pdf is</p>
<p><span class="math display">\[f(x) = \frac{1}{\sqrt{2 π}σ}e^{\frac{-(x - μ)^2}{2 σ^2}}\]</span></p>
<p>And we have trignometric random feature maps because</p>
<p><span class="math display">\[E_w[cos(\frac{w^T(x-y))}{\sigma}] = e^{\frac{-(x - y)^2}{2 σ^2}}\]</span></p>
<p>Similarly, for multivariate Gaussian distribution,</p>
<p><span class="math display">\[E_w[cos(w^T\Sigma^{-\frac12}(x-y))] = \exp(-\frac{1}{2}(x-y)^{\top}\Sigma^{-1}(x-y))\]</span></p>
<p>Here we propose the random feature map <span class="math display">\[ \phi(x) = \frac{\sqrt{(2\pi)^{-\frac{d}{2}}(\det(\Sigma))^{-\frac{1}{2}}}}{\sqrt{m}} \begin{bmatrix} \cos(w_1^T\Sigma^{-\frac12}x)\\...\\ \cos(w_m^T\Sigma^{-\frac12}x)\\ \sin(w_1^T\Sigma^{-\frac12}x)\\...\\ \sin(w_m^T\Sigma^{-\frac12}x) \end{bmatrix} \]</span></p>
<p><strong>The <span class="math inline">\(\Sigma^{-\frac12}\)</span> is derived from eigenvalue diagonalization:</strong> <!-- **Lemma**: Real symmetric matrix $A$ is positive definite iff there exists invertible matrix $P$ that $A=P^TP$ --> * Since <span class="math inline">\(\Sigma\)</span> is postive definite, the diagonalized <span class="math inline">\(\Lambda\)</span> have positive eigenvalues on its diagonal <span class="math display">\[\Sigma = U\Lambda U^T = (U\Lambda^{\frac12})(U\Lambda^{\frac12})^T = AA^T, A = U\Lambda^{\frac12}\]</span> * Then for <span class="math inline">\(\Sigma^{-1}\)</span>, <span class="math display">\[\Sigma^{-1} = (AA^T)^{-1}=(A^{-1})^TA^{-1}\]</span> <strong>Here <span class="math inline">\(A^{-1}\)</span> is the <span class="math inline">\(\Sigma^{-\frac12}\)</span>.</strong></p>
<h4 id="proof-of-psd">Proof of PSD</h4>
<p>From above we know the random feature map for this kernel. <span class="math display">\[K(x,y) = (2\pi)^{-\frac{d}{2}}(\det(\Sigma))^{-\frac{1}{2}}\exp(-\frac{1}{2}(x-y)^{\top}\Sigma^{-1}(x-y)) \]</span></p>
<p><span class="math display">\[X = \{x_1, ..., x_n\} \subseteq \mathbb{R}^d \]</span></p>
<p><span class="math display">\[\kappa(X)_{ij} =K(x_i,x_j)= E[\phi(x_i)\phi(x_j)^T]\]</span></p>
<p><span class="math display">\[\begin{aligned}
v\kappa(X)v^T &amp;= \sum_{ij}E[\phi(x_i)\phi(x_j)^T]v_iv_j\\
&amp;= \sum_{ij}E[\phi(x_i)\phi(x_j)^Tv_iv_j]\\
&amp;= E[\sum_{ij}\phi(x_i)\phi(x_j)^Tv_iv_j]\\
\end{aligned}\]</span></p>
<p><span class="math display">\[\Phi(X) \in \mathbb{R}^{N\times m} = 
\begin{bmatrix}
\phi(x_1)\\
...\\
\phi(x_N)\\
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[\Phi(X)\Phi(X)^T \in \mathbb{R}^{N\times N}, \Phi(X)\Phi(X)^T_{ij} = \phi(x_i)\phi(x_j)^T\]</span></p>
<p><span class="math display">\[\begin{aligned}
\therefore v\kappa(X)v^T 
&amp;= E[v\Phi(X)\Phi(X)^Tv^T]\\
&amp;= E[v\Phi(X)(v\Phi(X))^T] \geq 0
\end{aligned}\]</span></p>
<h3 id="what-is-time-complexity-of-computing-phi-5-points">What is time complexity of computing <span class="math inline">\(\phi\)</span> (5 points)?</h3>
<p>From above, <span class="math display">\[ \phi(x) = \frac{\sqrt{(2\pi)^{-\frac{d}{2}}(\det(\Sigma))^{-\frac{1}{2}}}}{\sqrt{m}} \begin{bmatrix} \cos(w_1^T\Sigma^{-\frac12}x)\\...\\ \cos(w_m^T\Sigma^{-\frac12}x)\\ \sin(w_1^T\Sigma^{-\frac12}x)\\...\\ \sin(w_m^T\Sigma^{-\frac12}x) \end{bmatrix} \]</span></p>
<ul>
<li><p>Firstly we diagonalize <span class="math inline">\(\Sigma\)</span> to get <span class="math inline">\(\Sigma^{-\frac12}\)</span> : <span class="math inline">\(\Sigma = U\Lambda U^T = (U\Lambda^{\frac12})(U\Lambda^{\frac12})^T = AA^T\)</span>. This decomposition takes <span class="math inline">\(O(d^3)\)</span>.</p></li>
<li><p>Then we compute each entry of <span class="math inline">\(\phi(x)\)</span>: <span class="math inline">\(\cos(w_i^T\Sigma^{-\frac12}x)\)</span>, this takes <span class="math inline">\(O(d^2)\)</span>. So for all entries it takes <span class="math inline">\(O(md^2)\)</span></p></li>
<li><h2 id="the-overall-time-complexity-is-the-larger-of-od3-and-omd2.-usually-md-so-the-final-result-is-omd2.">The overall time complexity is the larger of <span class="math inline">\(O(d^3)\)</span> and <span class="math inline">\(O(md^2)\)</span>. Usually <span class="math inline">\(m&gt;d\)</span> so the final result is <span class="math inline">\(O(md^2)\)</span>.</h2></li>
</ul>
<h2 id="problem-2-composite-attention-50-points">Problem 2: Composite Attention (50 points)</h2>
<!-- Reference: [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/pdf/2006.16236.pdf) -->
<h3 id="algorithm-1.-online-linear-time-attention">Algorithm 1. Online Linear-Time Attention</h3>
<!-- Reference: [Online and Linear-Time Attention by Enforcing Monotonic Alignments](https://arxiv.org/pdf/1704.00784.pdf) -->
<p>For example, we only attend to the closest 1 token, a.k.a only compute <span class="math inline">\(A_{i,i\pm 1}\)</span>; Or attend to a few tokens, but learn what to attend to.</p>
<h3 id="algorithm-2.-efficient-dense-attention">Algorithm 2. Efficient Dense Attention</h3>
<h4 id="construct-random-feature-map-for-this-composite-kernel">Construct random feature map for this composite kernel</h4>
<p>Noticed that this kernel is the product of softmax and angular kernel.</p>
<p><span class="math display">\[\begin{aligned}
A_{ij} &amp;= K_{SM}(d^{-\frac{1}{4}}q_i, d^{-\frac{1}{4}}k_j)K_{Ang}(q_i, k_j)\\
&amp;=E[\phi_{SM}(\bar{q_i})\phi_{SM}(\bar{k_j})^T]  E[\phi_{Ang}(q_i)\phi_{Ang}(k_j)^T]\\
&amp;\approx E[\phi_{SM}(\bar{q_i})\phi_{SM}(\bar{k_j})^T * \phi_{Ang}(q_i)\phi_{Ang}(k_j)^T]
\end{aligned}\]</span></p>
<p>Here we denote <span class="math inline">\(d^{-\frac{1}{4}}x\)</span> as <span class="math inline">\(\bar{x}\)</span>.</p>
<p>Use positive random features for <span class="math inline">\(\phi_{SM}\)</span>: <span class="math display">\[\phi_{SM} = \frac{1}{\sqrt m}e^{-\frac{||x||^2}{2}}[e^{w_1^Tx},...,e^{w_m^Tx}]\]</span></p>
<p><span class="math display">\[\phi_{Ang} = \frac{1}{\sqrt m}[sgn(w_1^Tx),...,sgn(w_m^Tx)] \]</span></p>
<p><span class="math display">\[\begin{aligned} 
E_w[\phi_{SM}(\bar{x})\phi_{SM}(\bar{y})^T*\phi_{Ang}(x)\phi_{Ang}(y)^T]
&amp;= E_w[\frac{1}{m^2}e^{-\frac{||\bar{x}||^2}{2}}e^{-\frac{||\bar{y}||^2}{2}}\sum_{i=1}^{m}{e^{w_i^T\bar{x}}e^{w_i^T\bar{y}}} \sum_{i=1}^{m}{sgn(w_i^Tx)sgn(w_i^Ty)}]\\
&amp;= E_w[\frac{1}{m^2}e^{-\frac{||\bar{x}||^2}{2}}e^{-\frac{||\bar{y}||^2}{2}}\sum_{i=1}^{m}{e^{w_i^T\bar{x}}e^{w_i^T\bar{y}}}\cdot m \cdot E_w[sgn(w^Tx)sgn(w^Ty)]]\\
&amp; \scriptstyle{\text{Use one point estimation}}\\
&amp;\approx E_w[\frac{1}{m}e^{-\frac{||\bar{x}||^2}{2}}e^{-\frac{||\bar{y}||^2}{2}}\sum_{i=1}^{m}{e^{w_i^T\bar{x}}e^{w_i^T\bar{y}}}{sgn(w_{m+1}^Tx)sgn(w_{m+1}^Ty)}]
\end{aligned}\]</span></p>
<p>The new proposed random feature here is <span class="math display">\[\phi_{comp}(x) = \frac{1}{\sqrt{m}}e^{-\frac{||\bar{x}||^2}{2}}\begin{bmatrix}
e^{w_1^T\bar{x}}sgn(w_{m+1}^Tx)\\
...\\
e^{w_m^T\bar{x}}sgn(w_{m+1}^Tx)
\end{bmatrix}, \bar{x} = d^{-\frac{1}{4}}x \]</span></p>
<p>Each time we generate an extra <span class="math inline">\(w_{m+1}\in \mathbb{R}^{d}\)</span> to compute the <span class="math inline">\(sgn\)</span> term and apply to all entries in the feature map.</p>
<h4 id="apply-the-dense-attention-mechanism">Apply the Dense Attention mechanism</h4>
<p>Since we already found a proper random feature map for this composite kernel, we can conduct "efficient dense attention" algorithm.</p>
<p><span class="math display">\[\begin{aligned}
A_{ij} &amp;= K_{SM}(d^{-\frac{1}{4}}q_i, d^{-\frac{1}{4}}k_j)K_{Angular}(q_i, k_j)\\
&amp;= K_{comp}(q_i, kj)\\
&amp;\approx E[\phi_{comp}(q_i)^T\phi_{comp}(k_j)]
\end{aligned}\]</span></p>
<p>Then in matrix form, <span class="math display">\[A\approx E[Q&#39;(K&#39;)^T]\]</span></p>
<p><span class="math display">\[Q&#39;=[\phi_{comp}(q_1)..., \phi_{comp}(q_l)]^T\]</span></p>
<p><span class="math display">\[K&#39;=[\phi_{comp}(k_1)..., \phi_{comp}(k_l)]^T\]</span></p>
<p><span class="math display">\[AV \approx E[Q&#39;(K&#39;)^T]V = E[Q&#39;(K&#39;)^TV]\approx Q&#39;(K&#39;)^TV = Q&#39;((K&#39;)^TV)\]</span></p>
<p><span class="math display">\[Q&#39;,K&#39;\in \mathbb{R}^{L\times m}, V\in \mathbb{R}^{L\times d}\]</span></p>
<p>We take: <span class="math inline">\(O(mLd)\)</span> time to get <span class="math inline">\((K&#39;)^TV\in \mathbb{R}^{m\times d}\)</span> and <span class="math inline">\(O(mLd)\)</span> time to get <span class="math inline">\(Q&#39;(K&#39;)^TV\)</span>.</p>
<hr />
<h2 id="problem-3-mlps-for-attention-30-points">Problem 3: MLPs for attention (30 points)</h2>
<p><span class="math display">\[E[Q&#39;(K&#39;)^T] = A = \exp(\frac{QK^T}{\sqrt{d}})\]</span></p>
<p><span class="math display">\[Q\in \mathbb{R}^{L\times d}, Q&#39;\in \mathbb{R}^{L\times m}\]</span></p>
<p><span class="math display">\[A_{ij} = \exp(\frac{q_ik_j^T}{\sqrt{d}})\]</span></p>
<p><span class="math display">\[\begin{aligned}
\therefore E[q_i&#39;k_j&#39;^T]   
&amp; =\exp(\frac{q_ik_j^T}{\sqrt{d}})\\
&amp; =K_{SM}(d^{-\frac{1}{4}}q_i, d^{-\frac{1}{4}}k_j)\\
&amp; =E[\phi(d^{-\frac{1}{4}}q_i)\phi(d^{-\frac{1}{4}}k_j)^T]
\end{aligned}\]</span></p>
<p>One valid pair of <span class="math inline">\(Q&#39;\)</span> and <span class="math inline">\(K&#39;\)</span> satisfy:</p>
<p><span class="math display">\[q_i&#39; = \phi(d^{-\frac{1}{4}}q_i)\]</span></p>
<p><span class="math display">\[k_j&#39; = \phi(d^{-\frac{1}{4}}k_j)\]</span></p>
<p><span class="math display">\[q_i&#39;, k_j&#39; \in \mathbb{R}^{m}\]</span></p>
<p>Here we use positive random features <span class="math inline">\(\phi_m^+:\mathbb{R}^d \to \mathbb{R}^m\)</span> to guarantee that the dimensionality of output is <span class="math inline">\(m\)</span>.</p>
<p><span class="math display">\[\phi_m^{+}(x) = e^{-\frac{||x||^2}{2}}\frac{1}{\sqrt{m}}\begin{bmatrix}\exp(w_1^Tx)\\...\\ \exp(w_m^Tx) \end{bmatrix}\]</span></p>
<p><span class="math display">\[w_1,...w_m \sim N(0,I_d)\]</span></p>
<p><span class="math display">\[q_i&#39; = \phi_m^{+}(d^{-\frac{1}{4}}q_i)\]</span></p>
<p><span class="math display">\[k_j&#39; = \phi_m^{+}(d^{-\frac{1}{4}}k_j)\]</span></p>
<p>Map all queries <span class="math inline">\(q_1, ... q_L\)</span> and keys <span class="math inline">\(k_1, ..., k_L\)</span> to generate rows of <span class="math inline">\(Q&#39;\)</span> and <span class="math inline">\(K&#39;\)</span>.</p>
<hr />
<h2 id="problem-4-gaussian-kernel-via-random-feature-maps-40-points">Problem 4: Gaussian kernel via random feature maps (40 points)</h2>
<p>Take hyperparameter <span class="math inline">\(\sigma^2=1\)</span>,</p>
<p><span class="math display">\[K_{Gauss}(x,y) = \exp(-\frac{||x-y||^2}{2})\]</span></p>
<h3 id="trigonometric-feature-map">Trigonometric feature map</h3>
<p><span class="math display">\[K_{Gauss}(x,y) = E[\phi(x)\phi(y)^T]\]</span></p>
<p><span class="math display">\[\phi(x)\stackrel{def}{=}\frac{1}{\sqrt{m}} \begin{bmatrix}\cos(w_1^Tx)\\...\\ \cos(w_m^Tx)\\ \sin(w_1^Tx)\\...\\ \sin(w_m^Tx) \end{bmatrix}\]</span></p>
<p><span class="math display">\[w\stackrel{dist}{=} N(0,I_d)\]</span></p>
<p>This feature map leads to unbiased estimation.</p>
<p><span class="math display">\[\begin{aligned}
\phi(x)\phi(y)^T &amp;= \frac{1}{m}\sum_{i=1}^{m}{\cos(w_i^Tx)\cos(w_i^Ty)+\sin(w_i^Tx)\sin(w_i^Ty)}\\
&amp;= \frac{1}{m}\sum_{i=1}^{m}{\cos(w_i^T(x-y))}
\end{aligned} \]</span></p>
<p><span class="math display">\[\begin{aligned}
E[\phi(x)\phi(y)^T] &amp;= E[\frac{1}{m}\sum_{i=1}^{m}{\cos(w_i^T(x-y))}]\\
&amp;=E_w[\cos(w^T(x-y))]\\
&amp;= \Re \int{e^{w^T(x-y)i}dP(w)}\\
&amp;=\exp(-\frac{||x-y||^2}{2})
\end{aligned} \]</span></p>
<h3 id="positive-feature-map">Positive feature map</h3>
<p>For softmax kernel, the postive feature map is <span class="math display">\[\phi_m&#39;^{+}(x) = e^{-\frac{||x||^2}{2}}\frac{1}{\sqrt{m}}\begin{bmatrix}\exp(w_1^Tx)\\...\\ \exp(w_m^Tx) \end{bmatrix}\]</span></p>
<p>We know that</p>
<p><span class="math display">\[\begin{aligned}
K_{Gauss}(x,y) &amp;= \exp(-\frac{||x-y||^2}{2}) \\
&amp;= e^{-\frac{||x||^2}{2}}e^{-\frac{||y||^2}{2}}e^{x^Ty}\\
&amp;= e^{-\frac{||x||^2}{2}}e^{-\frac{||y||^2}{2}}K_{SM}(x,y)\\
\end{aligned} \]</span></p>
<p><span class="math display">\[\phi_{Gauss}(x) = e^{-\frac{||x||^2}{2}}\phi_{SM}(x)\]</span></p>
<p>Then for Gaussian kernel <span class="math display">\[\phi_m^{+}(x) = e^{-||x||^2}\frac{1}{\sqrt{m}}\begin{bmatrix}\exp(w_1^Tx)\\...\\ \exp(w_m^Tx) \end{bmatrix}\]</span></p>
<p>Now prove that positive random map for gaussian kernel is unbiased.</p>
<p><span class="math display">\[\phi_m^{+}(x)\phi_m^{+}(y)^T = e^{-||x||^2}e^{-||y||^2}\frac{1}{m}\sum_{i=1}^{m}{\exp(w_i^T(x+y))}\]</span></p>
<p><span class="math display">\[\begin{aligned}
E[\phi_m^{+}(x)\phi_m^{+}(y)^T] &amp;= E\big[e^{-||x||^2}e^{-||y||^2}\frac{1}{m}\sum_{i=1}^{m}{\exp(w_i^T(x+y))}\big]\\
&amp;=e^{-||x||^2}e^{-||y||^2}E\big[\frac{1}{m}\sum_{i=1}^{m}{\exp(w_i^T(x+y))}\big]\\
&amp;=e^{-||x||^2}e^{-||y||^2}E_w\big[\exp(w^T(x+y))\big]
\end{aligned} \]</span></p>
<p><span class="math display">\[\begin{aligned}\because E_w\big[\exp(w^T(x+y))\big] 
&amp;= \int{e^{w^T(x+y)}dP(w)}\\
&amp;= \int_{-\infty}^{+\infty}{e^{w^Tz}\cdot \frac{1}{\sqrt{2\pi}}\cdot e^{-\frac{||w||^2}{2}}dw}\\
&amp;= \frac{1}{\sqrt{2\pi}} e^{\frac{||z||^2}{2}} \int_{-\infty}^{+\infty}{e^{-\frac{||z-w||^2}{2}}dw}\\
&amp;= \frac{1}{\sqrt{2\pi}} e^{\frac{||z||^2}{2}} \int_{-\infty}^{+\infty}{e^{-\frac{||a||^2}{2}}da} \\
&amp;= \frac{1}{\sqrt{2\pi}} e^{\frac{||z||^2}{2}}\cdot \sqrt{2\pi} \\
&amp;= e^{\frac{||z||^2}{2}}\\ 
&amp;= e^{\frac{||x+y||^2}{2}}\\ 
&amp;= e^{\frac{||x||^2}{2}}e^{\frac{||y||^2}{2}}e^{x^Ty}
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
\therefore  E[\phi_m^{+}(x)\phi_m^{+}(y)^T] 
&amp;= e^{-||x||^2}e^{-||y||^2}e^{\frac{||x||^2}{2}}e^{\frac{||y||^2}{2}}e^{x^Ty}\\ 
&amp;= e^{-\frac{||x||^2}{2}}e^{-\frac{||y||^2}{2}}e^{x^Ty}\\
&amp;= \exp(-\frac{||x-y||^2}{2}) \\
&amp;= K_{Gauss}(x,y) 
\end{aligned}\]</span></p>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
  </entry>
  <entry>
    <title>Data-Mining-施密特正交化</title>
    <url>/2020/10/02/Data-Mining-%E6%96%BD%E5%AF%86%E7%89%B9%E6%AD%A3%E4%BA%A4%E5%8C%96/</url>
    <content><![CDATA[<h1 id="最小二乘法与投影">最小二乘法与投影</h1>
<p>"最小二乘法的本质是希尔伯特空间的投影。"</p>
<p>Suppose we want to project some higher-dimensional vector to a lower-dimensional space(say plane).</p>
<p>The projection is <span class="math inline">\(\vec{p}\)</span>, original one is <span class="math inline">\(\vec{b}\)</span>, the vector that is vertical(or say orthogonal) to the space should be <span class="math display">\[\vec{e} = \vec{b} - \vec{p}\]</span></p>
<p>for the vector <span class="math inline">\(\vec{p}\)</span> that is within the plane(say <strong>n-dimensional</strong> space) <span class="math display">\[\vec{p} = Ax \]</span> where A is the matrix where each column <span class="math inline">\(a_i\)</span> is a basis.</p>
<p>since <span class="math inline">\(e\perp a_i\)</span> <span class="math display">\[a_i^T(b-Ax) = 0\]</span> <span class="math display">\[A^T(b-Ax) = 0\]</span> <span class="math display">\[A^TAx = A^Tb\]</span></p>
<p>If <span class="math inline">\(A^TA\)</span> is invertible <span class="math display">\[x = (A^TA)^{-1}A^Tb\]</span> <span class="math display">\[p = Ax = A(A^TA)^{-1}A^Tb\]</span></p>
<p>The best scenario is A is composed of orthogonal basis. A is standard orthogonal matrix.</p>
<hr />
<p><strong>Question</strong>: when is <span class="math inline">\(A^TA\)</span> invertible?</p>
<p><strong>Answer</strong>: when the column vectors of <span class="math inline">\(A(m\times n)\)</span> are linearly independent</p>
<p><strong>Proof(Contradiction):</strong></p>
<p>If column vectors are linearly independent and <span class="math inline">\(A^TA\)</span> is invertible,</p>
<p><span class="math inline">\(A^TAx=0\)</span> has non-zero solution (because <span class="math inline">\(\det{A^TA} = 0\)</span>)</p>
<p><span class="math display">\[x^TA^TAx = 0\]</span></p>
<p><span class="math display">\[(Ax)^TAx = 0\]</span></p>
<p><span class="math display">\[\therefore Ax = 0\]</span></p>
<p><span class="math inline">\(\because\)</span> A's columns are linearlly independent <span class="math display">\[\therefore x=0\]</span> Contradicted, so when column vectors are linearly independent, <span class="math inline">\(A^TA\)</span> is invertible</p>
<hr />
<h1 id="格拉姆-施密特正交化-gram-schmidt-orthogonalization">格拉姆-施密特正交化 Gram-Schmidt Orthogonalization</h1>
<p><a href="https://zhuanlan.zhihu.com/p/76703543">知乎回答：格拉姆-施密特正交化--QR分解法的来源</a></p>
<p>标准正交阵 <span class="math inline">\(Q\)</span> 满足:</p>
<p><span class="math display">\[ Q^TQ = I\]</span> <span class="math display">\[ Q^T = Q^{-1}\]</span></p>
<p>投影矩阵 <span class="math inline">\(P\)</span> 满足: <span class="math display">\[ P = A(A^TA)^{-1}A^T \]</span></p>
<p>当<span class="math inline">\(A\)</span>为正交阵<span class="math inline">\(Q\)</span>时， <span class="math display">\[P = Q(Q^TQ)^{-1}Q^T = QQ^T\]</span></p>
<p>如果有一个矩阵 <span class="math display">\[q = [a, b, c]\]</span> 经过正交化 <span class="math display">\[Q = [A, B, C]\]</span> 其中<span class="math inline">\(A, B, C\)</span>正交。令<span class="math inline">\(A=a\)</span>(第一个列向量不变，调整其他向量使正交)</p>
<p>研究<span class="math inline">\(b\)</span>在<span class="math inline">\(a(A)\)</span>上的投影<span class="math inline">\(p\)</span>，利用向量投影公式 <span class="math display">\[p = \frac{A^Tb}{A^TA}A\]</span></p>
<p><span class="math inline">\(B\)</span>就是<span class="math inline">\(b\)</span>到<span class="math inline">\(A\)</span>垂线的向量 <span class="math display">\[B = b - p = b - \frac{A^Tb}{A^TA}A\]</span></p>
<p>已经有了前两个维度，<span class="math inline">\(C\)</span>就是<span class="math inline">\(c\)</span>减去在<span class="math inline">\(A,B\)</span>平面上的投影，也可以认为是减去在前两个方向上的投影(因为平面上的投影也是两个方向上投影的叠加) <span class="math display">\[C = c - \frac{A^Tc}{A^TA}A - \frac{B^Tc}{B^TB}B\]</span></p>
<p>对<span class="math inline">\(u_i = A,B,C\)</span>模数归一化得到<span class="math inline">\(e_i\)</span>，就得到正交矩阵<span class="math inline">\(Q\)</span></p>
<h1 id="qr分解">QR分解</h1>
<p><a href="https://zhuanlan.zhihu.com/p/84415000">知乎回答：[数值计算] QR分解</a></p>
<p><span class="math inline">\(Ax = b, A\in \mathbb{R}^{m\times n}\)</span> 的可能情况有： 1. <span class="math inline">\(m=n\)</span> 2. <span class="math inline">\(m&gt;n\)</span>, over-determined 3. <span class="math inline">\(m&lt;n\)</span>, under-determined</p>
<h2 id="定义">定义</h2>
<p><span class="math inline">\(A\in \mathbb{R}^{m\times n}, m\geq n\)</span>可以被分解成 <span class="math inline">\(A=QR\)</span>, 其中: * <span class="math inline">\(Q\in \mathbb{R}^{m\times m}\)</span> 是正交矩阵 * <span class="math inline">\(R = \begin{bmatrix}\hat{R}\\0\end{bmatrix}\in \mathbb{R}^{m\times n}\)</span> * <span class="math inline">\(\hat{R}\in \mathbb{R}^{n\times n}\)</span>是上三角阵</p>
<h2 id="正交矩阵性质">正交矩阵性质</h2>
<ul>
<li><span class="math inline">\(Q^TQ = QQ^T = I\)</span></li>
<li>左乘正交矩阵不影响欧式范数</li>
</ul>
<h2 id="计算qr分解的方法">计算QR分解的方法</h2>
<ul>
<li>Gram-Schmidt orthogonalization</li>
<li>Householder Triangularization</li>
<li>Givens Rotations</li>
</ul>
<h2 id="gram-schmidt-orthogonalization">Gram-Schmidt Orthogonalization</h2>
<h3 id="reduced-qr分解">Reduced QR分解</h3>
<p>GSO从矩阵<span class="math inline">\(A\in \mathbb{R}^{m\times n}\)</span>的第一个列向量<span class="math inline">\(A_{:,0}\)</span>开始构建互相正交的基。 对于原矩阵<span class="math inline">\(A\)</span> <span class="math display">\[A_{:,0} = r_{0,0}q_0\]</span> <span class="math display">\[A_{:,1} = r_{0,1}q_0 + r_{1,1}q_1\]</span> <span class="math display">\[...\]</span> <span class="math display">\[A_{:,n-1} = r_{0,n-1}q_0 + r_{1,n-1}q_1 ... + r_{n-1,n-1}q_{n-1}\]</span></p>
<p><span class="math display">\[ A = \hat{Q} \hat{R} \]</span> <span class="math display">\[ [A_{:,0}, ... A_{:,n-1}] = [q_0, ..., q_{n-1}] \begin{bmatrix} r_{0,0} \ r_{0,1} \ ... r_{0,n-1}\\ ~~~~~~~ r_{1,1} \ ... r_{1,n-1} \\ ~~~~... \\ ~~~~~~~~~~~~~r_{n-1, n-1}\end{bmatrix} \]</span></p>
<p><span class="math inline">\(\hat{Q} \in \mathbb{R}^{m\times n}, \hat{R} \in \mathbb{R}^{n\times n}\)</span> 称为 Reduced-QR，<span class="math inline">\(\hat{Q}\)</span>不为方阵。 <span class="math inline">\(\hat{Q}^T\hat{Q} = I\)</span>，因为<span class="math inline">\(m&gt;n\)</span>，不满足<span class="math inline">\(\hat{Q}\hat{Q}^T = I\)</span> （满秩方阵才能两侧逆）</p>
<h3 id="full-qr分解">Full-QR分解</h3>
<p><span class="math inline">\(Q \in \mathbb{R}^{m\times m}, R \in \mathbb{R}^{m\times n}\)</span></p>
<p>若<span class="math inline">\(m&gt;n\)</span>，需要把<span class="math inline">\(\hat{Q}\)</span>的n个基拓展到m个基。多出的<span class="math inline">\(m-n\)</span>个基对应到<span class="math inline">\(R = \begin{bmatrix} \hat{R} \\ 0 \end{bmatrix}\)</span> 的空白部分。</p>
<h3 id="python-numpy代码">Python numpy代码</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">q, r = np.linalg.qr(A) <span class="comment"># Reduced-QR</span></span><br><span class="line">q, r = np.linalg.qr(A, mode = <span class="string">&quot;complete&quot;</span>) <span class="comment"># Full-QR</span></span><br></pre></td></tr></table></figure>
<h3 id="classic-gso-cgso">Classic GSO (CGSO)</h3>
<p><span class="math display">\[ [A_{:,0}, ... A_{:,n-1}] = [q_0, ..., q_{n-1}] \begin{bmatrix} r_{0,0} \ r_{0,1} \ ... r_{0,n-1}\\ ~~~~~~~ r_{1,1} \ ... r_{1,n-1} \\ ~~~~... \\ ~~~~~~~~~~~~~r_{n-1, n-1}\end{bmatrix} \]</span></p>
<p>每一次迭代<span class="math inline">\(q_0, ..., q_{j-1}\)</span>已知，<span class="math inline">\(r_{j,j}\)</span>未知，<span class="math inline">\(r_{0,j}, ..., r_{j-1,j}\)</span>满足 <span class="math display">\[r_{i,j} = q_i^TA_{:,~j}\]</span></p>
<p><span class="math display">\[r_{j,j}q_j = v_j =  A_{:,j} - \sum_{i=0}^{j-1}{r_{i,j}q_i} = A_{:,j} - \sum_{i=0}^{j-1}{q_i^TA_{:,~j}q_i}\]</span></p>
<p>对<span class="math inline">\(v_i\)</span>按模数归一化， <span class="math display">\[q_j = \frac{v_j}{||v_j||_2}\]</span> <span class="math display">\[r_{j,j} = ||v_j||_2\]</span></p>
<h3 id="modified-gso-mgso">Modified GSO (MGSO)</h3>
<p>...</p>
<h2 id="givens-rotations">Givens Rotations</h2>
<h3 id="矩阵形式">矩阵形式</h3>
<p><img src="https://www.zhihu.com/equation?tex=G%28i%2Cj%2C%5Ctheta%29+%3D++%5Cbegin%7Bbmatrix%7D+1+%26+%5Cdots+%26+0+%26+%5Cdots+%26+0+%26+%5Cdots+%26+0%5C%5C+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots%5C%5C+0+%26+%5Cdots+%26+cos%5Ctheta+%26+%5Cdots+%26+sin%5Ctheta+%26+%5Cdots+%26+0%5C%5C+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots%5C%5C+0+%26+%5Cdots+%26+-sin%5Ctheta+%26+%5Cdots+%26+cos%5Ctheta+%26+%5Cdots+%26+0%5C%5C+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots%5C%5C+0+%26+%5Cdots+%26+0+%26+%5Cdots+%26+0+%26+%5Cdots+%26+1%5C%5C+%5Cend%7Bbmatrix%7D+%5Ctag%7B8%7D" alt="G(i,j,θ)" /> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+i+%26%3C+j+%5C%5C+G_%7Bi%2Ci%7D+%26%3D+c%5C%5C+G_%7Bj%2Cj%7D+%26%3D+c%5C%5C+G_%7Bi%2Cj%7D+%26%3D+s%5C%5C+G_%7Bj%2Ci%7D+%26%3D+-s%5C%5C+G_%7Bk%2Ck%7D+%26%3D+1%2C%5C+for%5C+k%5Cne+i%5C+or%5C+j%5C%5C+G_%7Bt%2Cs%7D+%26%3D+0%2C%5C+otherwise+%5Cend%7Baligned%7D+%5Ctag%7B9%7D" alt="Equations" /></p>
<h3 id="矩阵作用">矩阵作用</h3>
<p><span class="math inline">\(A\in \mathbb{R}^{m\times n}, m \geq n, A_{i,j}, A_{i,k}, j &lt; k\)</span>, <span class="math display">\[\begin{bmatrix}c &amp; s\\-s &amp; c \end{bmatrix} \begin{bmatrix}A_{i,j}\\A_{i,k}\end{bmatrix} = \begin{bmatrix} \alpha \\ 0 \end{bmatrix}\]</span> <span class="math display">\[\alpha = \sqrt{A_{i,j}^2 + A_{i,k}^2}\]</span> <span class="math display">\[c=\frac{A_{i,j}}{\alpha}\]</span> <span class="math display">\[s=\frac{A_{i,k}}{\alpha}\]</span></p>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Data-Mining-Recitation1</title>
    <url>/2020/10/05/Data-Mining-Recitation1/</url>
    <content><![CDATA[<h1 id="recitation1">Recitation1:</h1>
<p>主要讨论Random Feature map 对高斯核的模拟，以及G正交化的影响</p>
<p>P.S. <a href="https://chrismroberts.com/2020/01/06/using-markdown-in-hexo-to-add-images/">hexo部署时添加图片 Adding Images to Hexo Posts with Markdown</a> <code>npm i -s hexo-asset-link</code></p>
<p><a href="https://nbconvert.readthedocs.io/en/latest/install.html#installing-tex">jupyter导出pdf</a> 安装TeX</p>
<h4 id="import-packages">Import packages</h4>
<figure class="highlight elm"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="title">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="title">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="title">from</span> scipy.stats <span class="keyword">import</span> norm</span><br></pre></td></tr></table></figure>
<h4 id="calculate-gram-schmidt-orthogonalization">Calculate Gram-Schmidt Orthogonalization</h4>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">def gram<span class="constructor">_schmidt_columns(X)</span>:</span><br><span class="line">    Q, R = np.linalg.qr(X)</span><br><span class="line">    return Q</span><br><span class="line"></span><br><span class="line">def orthgonalize(V):</span><br><span class="line">    N = <span class="module-access"><span class="module"><span class="identifier">V</span>.</span></span>shape<span class="literal">[<span class="number">0</span>]</span></span><br><span class="line">    d = <span class="module-access"><span class="module"><span class="identifier">V</span>.</span></span>shape<span class="literal">[<span class="number">1</span>]</span></span><br><span class="line">    turns = <span class="built_in">int</span>(N/d)</span><br><span class="line">    remainder = N%d</span><br><span class="line">    </span><br><span class="line">    #V =  np.random.normal(size=<span class="literal">[(<span class="identifier">turns</span>+<span class="number">1</span>)<span class="operator">*</span><span class="identifier">d</span>, <span class="identifier">d</span>]</span>)</span><br><span class="line">    #V =  np.random.normal(size =<span class="literal">[N, <span class="identifier">d</span>]</span>)</span><br><span class="line">    #print(<span class="module-access"><span class="module"><span class="identifier">V</span>.</span></span>shape)</span><br><span class="line">    V_ = np.zeros<span class="constructor">_like(V)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(turns):</span><br><span class="line">        v = gram<span class="constructor">_schmidt_columns(V[<span class="params">i</span><span class="operator">*</span><span class="params">d</span>:(<span class="params">i</span>+1)</span>*d, :].T).T</span><br><span class="line">        V_<span class="literal">[<span class="identifier">i</span><span class="operator">*</span><span class="identifier">d</span>:(<span class="identifier">i</span>+<span class="number">1</span>)<span class="operator">*</span><span class="identifier">d</span>, :]</span> = v</span><br><span class="line">    <span class="keyword">if</span> remainder != <span class="number">0</span>:</span><br><span class="line">        V_<span class="literal">[<span class="identifier">turns</span><span class="operator">*</span><span class="identifier">d</span>:,:]</span> = gram<span class="constructor">_schmidt_columns(V[<span class="params">turns</span><span class="operator">*</span><span class="params">d</span>:,:].T)</span>.T</span><br><span class="line">        </span><br><span class="line">    return V_</span><br></pre></td></tr></table></figure>
<h6 id="step-1-i.i.d.-sampling">Step 1: i.i.d. sampling</h6>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">random_matrix = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size = (<span class="number">6</span>,<span class="number">3</span>))</span><br><span class="line">random_matrix</span><br></pre></td></tr></table></figure>
<pre><code>array([[ 1.59895494,  0.29885485,  0.43694542],
       [-0.43523095, -0.10324217, -1.28703438],
       [-0.7452128 , -0.12095329,  0.31481384],
       [ 0.0779075 ,  0.43587534, -0.04371553],
       [ 0.35963102,  0.90466958, -1.25109187],
       [ 1.58862906, -0.08350228, -0.51967566]])</code></pre>
<h6 id="step-2-calculate-the-norm-of-each-sampling-for-later-renormalization">Step 2: Calculate the norm of each sampling for later renormalization</h6>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">norms = np.linalg.norm(random_matrix, axis = <span class="number">1</span>).reshape(<span class="number">6</span>,<span class="number">1</span>)</span><br><span class="line">norms</span><br></pre></td></tr></table></figure>
<pre><code>array([[1.6843077 ],
       [1.36254996],
       [0.81797284],
       [0.44493588],
       [1.58524206],
       [1.67355242]])</code></pre>
<h6 id="step-3-orthogonalize-the-random-features">Step 3: Orthogonalize the random features</h6>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">a</span> = orthgonalize(random_matrix)</span><br></pre></td></tr></table></figure>
<figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line"><span class="keyword">a</span>, <span class="keyword">a</span>.shape</span><br></pre></td></tr></table></figure>
<pre><code>(array([[-0.94932472, -0.17743483, -0.25942137],
        [-0.25846666, -0.02888627,  0.9655882 ],
        [-0.17882269,  0.98370853, -0.01843854],
        [-0.17509826, -0.9796363 ,  0.09825129],
        [ 0.14721431, -0.12472189, -0.98120966],
        [ 0.97348269, -0.15734411,  0.16605506]]), (6, 3))</code></pre>
<h6 id="step-4-renormalize-the-orthogonalized-random-features">Step 4: Renormalize the orthogonalized random features</h6>
<figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line"><span class="keyword">a</span>*norms</span><br></pre></td></tr></table></figure>
<pre><code>array([[-1.59895494, -0.29885485, -0.43694542],
       [-0.35217373, -0.03935899,  1.31566217],
       [-0.14627211,  0.80464686, -0.01508222],
       [-0.0779075 , -0.43587534,  0.04371553],
       [ 0.23337031, -0.19771438, -1.55545483],
       [ 1.62917431, -0.26332362,  0.27790185]])</code></pre>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">np</span><span class="selector-class">.dot</span>(<span class="selector-tag">a</span><span class="selector-attr">[0, :]</span>, <span class="selector-tag">a</span><span class="selector-attr">[1, :]</span>)</span><br></pre></td></tr></table></figure>
<pre><code>5.551115123125783e-17</code></pre>
<figure class="highlight vim"><table><tr><td class="code"><pre><span class="line"># G就是(w1, ..., wm)</span><br><span class="line"># 高斯核的phi(<span class="keyword">x</span>)</span><br><span class="line">def baseline_eval_gaussian(<span class="keyword">x</span>, G, <span class="keyword">m</span>):</span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="comment">&quot;Calculate the result of baseline random feature mapping</span></span><br><span class="line"></span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        <span class="keyword">x</span>: array, dimension = d</span><br><span class="line">            The data point <span class="keyword">to</span> <span class="built_in">input</span> <span class="keyword">to</span> the baseline mapping</span><br><span class="line">        </span><br><span class="line">        G: matrix, dimension = <span class="keyword">m</span>*d</span><br><span class="line">            The matrix in the baseline random feature mapping</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">m</span>: integer</span><br><span class="line">            The <span class="keyword">number</span> of dimension that we want <span class="keyword">to</span> reduce <span class="keyword">to</span></span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="comment">&quot;</span></span><br><span class="line">    <span class="keyword">left</span> = np.<span class="built_in">cos</span>(np.dot(G, <span class="keyword">x</span>).astype(np.float32))</span><br><span class="line">    <span class="keyword">right</span> = np.<span class="built_in">sin</span>(np.dot(G, <span class="keyword">x</span>).astype(np.float32))</span><br><span class="line">    <span class="keyword">return</span> ((<span class="number">1</span>/<span class="keyword">m</span>)**<span class="number">0.5</span>) * np.<span class="keyword">append</span>(<span class="keyword">left</span>, <span class="keyword">right</span>)</span><br><span class="line"></span><br><span class="line">def find_sigma(random_sample):</span><br><span class="line">    all_distances = []</span><br><span class="line">    <span class="keyword">for</span> i in <span class="built_in">range</span>(<span class="built_in">len</span>(random_sample)): # dimensionality: d</span><br><span class="line">        #print(<span class="keyword">f</span><span class="string">&#x27;Calculating the distance of &#123;i&#125;th samples&#x27;</span>)</span><br><span class="line">        distances = []</span><br><span class="line">        <span class="keyword">for</span> <span class="keyword">j</span> in <span class="built_in">range</span>(<span class="built_in">len</span>(random_sample)):</span><br><span class="line">            <span class="keyword">if</span> j!=i:</span><br><span class="line">                distances.<span class="keyword">append</span>(np.linalg.<span class="keyword">norm</span>(random_sample[i] - random_sample[<span class="keyword">j</span>]))</span><br><span class="line">        distances.<span class="keyword">sort</span>()</span><br><span class="line">        all_distances.<span class="keyword">append</span>(distances[<span class="number">50</span>]) # 只取第<span class="number">50</span>个</span><br><span class="line">        # all_distances.<span class="keyword">append</span>(np.mean(distances))</span><br><span class="line">    <span class="keyword">return</span> np.mean(all_distances)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">find_sigma(np.random.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">100</span>,<span class="number">50</span>)))</span><br></pre></td></tr></table></figure>
<pre><code>10.11949067744566</code></pre>
<h4 id="main">Main</h4>
<h5 id="load-dataset">Load dataset</h5>
<figure class="highlight clean"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line">drive.mount(<span class="string">&#x27;/content/drive&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&quot;/content/drive&quot;, force_remount=True).</code></pre>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">%</span><span class="bash"><span class="built_in">cd</span> <span class="string">&quot;/content/drive/My Drive/20FA/Recitation&quot;</span></span></span><br></pre></td></tr></table></figure>
<pre><code>/content/drive/My Drive/20FA/Recitation</code></pre>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">d</span> = <span class="number">10</span>  <span class="comment"># Dimension of data</span></span><br><span class="line"><span class="attr">d_</span> = <span class="number">10</span>  <span class="comment"># Number of multipliers</span></span><br></pre></td></tr></table></figure>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">letters = pd.read<span class="constructor">_csv(&#x27;<span class="params">letter</span>-<span class="params">recognition</span>.<span class="params">csv</span>&#x27;)</span></span><br><span class="line">letters.head<span class="literal">()</span></span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
letter
</th>
<th>
xbox
</th>
<th>
ybox
</th>
<th>
width
</th>
<th>
height
</th>
<th>
onpix
</th>
<th>
xbar
</th>
<th>
ybar
</th>
<th>
x2bar
</th>
<th>
y2bar
</th>
<th>
xybar
</th>
<th>
x2ybar
</th>
<th>
xy2bar
</th>
<th>
xedge
</th>
<th>
xedgey
</th>
<th>
yedge
</th>
<th>
yedgex
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
T
</td>
<td>
2
</td>
<td>
8
</td>
<td>
3
</td>
<td>
5
</td>
<td>
1
</td>
<td>
8
</td>
<td>
13
</td>
<td>
0
</td>
<td>
6
</td>
<td>
6
</td>
<td>
10
</td>
<td>
8
</td>
<td>
0
</td>
<td>
8
</td>
<td>
0
</td>
<td>
8
</td>
</tr>
<tr>
<th>
1
</th>
<td>
I
</td>
<td>
5
</td>
<td>
12
</td>
<td>
3
</td>
<td>
7
</td>
<td>
2
</td>
<td>
10
</td>
<td>
5
</td>
<td>
5
</td>
<td>
4
</td>
<td>
13
</td>
<td>
3
</td>
<td>
9
</td>
<td>
2
</td>
<td>
8
</td>
<td>
4
</td>
<td>
10
</td>
</tr>
<tr>
<th>
2
</th>
<td>
D
</td>
<td>
4
</td>
<td>
11
</td>
<td>
6
</td>
<td>
8
</td>
<td>
6
</td>
<td>
10
</td>
<td>
6
</td>
<td>
2
</td>
<td>
6
</td>
<td>
10
</td>
<td>
3
</td>
<td>
7
</td>
<td>
3
</td>
<td>
7
</td>
<td>
3
</td>
<td>
9
</td>
</tr>
<tr>
<th>
3
</th>
<td>
N
</td>
<td>
7
</td>
<td>
11
</td>
<td>
6
</td>
<td>
6
</td>
<td>
3
</td>
<td>
5
</td>
<td>
9
</td>
<td>
4
</td>
<td>
6
</td>
<td>
4
</td>
<td>
4
</td>
<td>
10
</td>
<td>
6
</td>
<td>
10
</td>
<td>
2
</td>
<td>
8
</td>
</tr>
<tr>
<th>
4
</th>
<td>
G
</td>
<td>
2
</td>
<td>
1
</td>
<td>
3
</td>
<td>
1
</td>
<td>
1
</td>
<td>
8
</td>
<td>
6
</td>
<td>
6
</td>
<td>
6
</td>
<td>
6
</td>
<td>
5
</td>
<td>
9
</td>
<td>
1
</td>
<td>
7
</td>
<td>
5
</td>
<td>
10
</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">letters</span> = np.asarray(letters)</span><br><span class="line"><span class="attr">data</span> = letters[:, <span class="number">1</span>:d+<span class="number">1</span>]</span><br><span class="line"><span class="attr">data</span> = list(data)</span><br></pre></td></tr></table></figure>
<figure class="highlight haskell"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">data</span>[0]</span></span><br></pre></td></tr></table></figure>
<pre><code>array([2, 8, 3, 5, 1, 8, 13, 0, 6, 6], dtype=object)</code></pre>
<h5 id="parameters-setup">Parameters setup</h5>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="meta">#d = 10  # dimension of the data point </span></span><br><span class="line">N = [d*i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,d_+<span class="number">1</span>)]  # number <span class="keyword">of</span> samplings</span><br><span class="line">episode = <span class="number">100</span></span><br><span class="line">epoch =  <span class="number">450</span>  # number <span class="keyword">of</span> experiments <span class="keyword">to</span> <span class="keyword">perform</span> <span class="keyword">for</span> <span class="keyword">each</span> episode</span><br></pre></td></tr></table></figure>
<figure class="highlight ebnf"><table><tr><td class="code"><pre><span class="line"><span class="attribute">N</span></span><br></pre></td></tr></table></figure>
<pre><code>[10, 20, 30, 40, 50, 60, 70, 80, 90, 100]</code></pre>
<h5 id="start-running">Start running</h5>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 正交化可以降低variance</span></span><br><span class="line"><span class="comment"># 对不同样本大小测试</span></span><br><span class="line"></span><br><span class="line"><span class="attr">iid_estimates</span> = np.zeros((d_, episode, epoch))</span><br><span class="line"><span class="attr">orthog_estimates</span> = np.zeros((d_, episode, epoch))</span><br><span class="line"></span><br><span class="line"><span class="attr">MSE_iid</span> = []</span><br><span class="line"><span class="attr">MSE_orthog</span> = []</span><br><span class="line"></span><br><span class="line"><span class="attr">MSE_iid_</span> = []</span><br><span class="line"><span class="attr">MSE_orthog_</span> = []</span><br><span class="line"></span><br><span class="line"><span class="attr">list_of_samples</span> = []</span><br><span class="line"><span class="attr">true_values</span> = []</span><br><span class="line"></span><br><span class="line"><span class="attr">random_sample</span> = random.sample(data, <span class="number">1000</span>)</span><br><span class="line"><span class="attr">sigma</span> = find_sigma(random_sample)</span><br><span class="line"></span><br><span class="line"><span class="attr">random_sample</span> = [i/sigma for i in random_sample]</span><br></pre></td></tr></table></figure>
<figure class="highlight ebnf"><table><tr><td class="code"><pre><span class="line"><span class="attribute">sigma</span></span><br></pre></td></tr></table></figure>
<pre><code>5.760360599001658</code></pre>
<figure class="highlight vala"><table><tr><td class="code"><pre><span class="line"><span class="meta"># 取出20000个数据点中的1000个作为random_sample</span></span><br><span class="line"><span class="meta"># find_sigma从每行中取第50个，整体做平均，可以省去一个mean的运算</span></span><br><span class="line"><span class="meta"># random_sample的每行数据点除以标准差，归一化</span></span><br><span class="line">random_sample[<span class="number">0</span>], len(random_sample)</span><br></pre></td></tr></table></figure>
<pre><code>(array([0.34720048608530246, 0.17360024304265123, 0.34720048608530246,
        0.34720048608530246, 0.17360024304265123, 0.8680012152132562,
        1.7360024304265125, 0.6944009721706049, 0.8680012152132562,
        1.7360024304265125], dtype=object), 1000)</code></pre>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="section"># [<span class="string">tqdm</span>](<span class="link">https://tqdm.github.io/</span>)</span></span><br><span class="line">for i in tqdm(range(10000000)):</span><br><span class="line"><span class="code">    pass</span></span><br></pre></td></tr></table></figure>
<pre><code>  0%|          | 0/10000000 [00:00&lt;?, ?it/s] [A
  3%|▎         | 318248/10000000 [00:00&lt;00:03, 3182475.45it/s] [A
  6%|▋         | 639853/10000000 [00:00&lt;00:02, 3192472.62it/s] [A
 10%|▉         | 960927/10000000 [00:00&lt;00:02, 3197929.60it/s] [A
 13%|█▎        | 1320869/10000000 [00:00&lt;00:02, 3308643.11it/s] [A
 17%|█▋        | 1694130/10000000 [00:00&lt;00:02, 3425362.30it/s] [A
 21%|██        | 2086242/10000000 [00:00&lt;00:02, 3560404.17it/s] [A
 25%|██▍       | 2491758/10000000 [00:00&lt;00:02, 3695671.55it/s] [A
 29%|██▉       | 2901121/10000000 [00:00&lt;00:01, 3806689.20it/s] [A
 33%|███▎      | 3300819/10000000 [00:00&lt;00:01, 3861844.78it/s] [A
 37%|███▋      | 3714063/10000000 [00:01&lt;00:01, 3939226.23it/s] [A
 41%|████▏     | 4127830/10000000 [00:01&lt;00:01, 3996729.87it/s] [A
 45%|████▌     | 4548321/10000000 [00:01&lt;00:01, 4056985.18it/s] [A
 50%|████▉     | 4974120/10000000 [00:01&lt;00:01, 4115263.66it/s] [A
 54%|█████▍    | 5401755/10000000 [00:01&lt;00:01, 4162298.98it/s] [A
 58%|█████▊    | 5841688/10000000 [00:01&lt;00:00, 4230680.58it/s] [A
 63%|██████▎   | 6272699/10000000 [00:01&lt;00:00, 4254198.50it/s] [A
 67%|██████▋   | 6700434/10000000 [00:01&lt;00:00, 4261115.77it/s] [A
 71%|███████▏  | 7141608/10000000 [00:01&lt;00:00, 4305207.12it/s] [A
 76%|███████▌  | 7594108/10000000 [00:01&lt;00:00, 4368868.06it/s] [A
 80%|████████  | 8036809/10000000 [00:02&lt;00:00, 4386147.78it/s] [A
 85%|████████▍ | 8484505/10000000 [00:02&lt;00:00, 4413000.38it/s] [A
 89%|████████▉ | 8925941/10000000 [00:02&lt;00:00, 4386127.92it/s] [A
 94%|█████████▍| 9377578/10000000 [00:02&lt;00:00, 4424403.07it/s] [A
100%|██████████| 10000000/10000000 [00:02&lt;00:00, 4092170.78it/s]</code></pre>
<figure class="highlight maxima"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(episode), <span class="built_in">position</span>=<span class="number">0</span>, leave=True): # episode = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">    x, y = <span class="built_in">random</span>.sample(random_sample, k=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    true_value = math.<span class="built_in">exp</span>((-<span class="number">1</span>)*(<span class="built_in">np</span>.linalg.norm(x - y)**<span class="number">2</span>)/(<span class="number">2</span>)) #K_Gauss(x, y) = e^&#123;(x-y)^<span class="number">2</span>/<span class="number">2</span>&#125;</span><br><span class="line">    list_of_samples.<span class="built_in">append</span>((x, y)) # <span class="number">1000</span>个值，与true_value对应</span><br><span class="line">    true_values.<span class="built_in">append</span>(true_value)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(len(N)): # <span class="number">10</span>组样本，sample_size10到<span class="number">100</span></span><br><span class="line"></span><br><span class="line">    mse_iid_n = []</span><br><span class="line">    mse_orthog_n = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(episode), <span class="built_in">position</span>=<span class="number">0</span>, leave=True):</span><br><span class="line">        #<span class="built_in">print</span>(f&#x27;&#123;N[n]&#125; samplings, &#123;e&#125;th episode&#x27;)</span><br><span class="line">        <span class="literal">true</span> = <span class="built_in">np</span>.repeat(true_values[e], epoch) # epoch=<span class="number">450</span></span><br><span class="line">        </span><br><span class="line">        x = list_of_samples[e][<span class="number">0</span>]</span><br><span class="line">        y = list_of_samples[e][<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line"></span><br><span class="line">            <span class="built_in">np</span>.<span class="built_in">random</span>.seed()</span><br><span class="line">            # N[n]即m的值变化，aka V=[w1, ..., wm]变化</span><br><span class="line">            V = <span class="built_in">np</span>.<span class="built_in">random</span>.normal(<span class="number">0</span>, <span class="number">1</span>, (N[n],d))   #create N*d iid <span class="built_in">matrix</span></span><br><span class="line">            V_orthog = orthgonalize(V)   #create N*d <span class="built_in">matrix</span> with orthogonal blocks</span><br><span class="line">            norms = <span class="built_in">np</span>.linalg.norm(V, axis=<span class="number">1</span>).reshape([N[n],<span class="number">1</span>])</span><br><span class="line">            </span><br><span class="line">            # K(x, y) = Φ(x)^T Φ(y)</span><br><span class="line">            # Φ(x) = <span class="number">1</span>/<span class="built_in">sqrt</span>(m) * [<span class="built_in">cos</span>(w1x), ... <span class="built_in">cos</span>(wmx), <span class="built_in">sin</span>(w1x), ..., <span class="built_in">sin</span>(wmx)]</span><br><span class="line">            iid_estimates[n, e, i] = <span class="built_in">np</span>.dot(baseline_eval_gaussian(x, V, N[n]), # Original <span class="string">&quot;G&quot;</span></span><br><span class="line">                        baseline_eval_gaussian(y, V, N[n]))</span><br><span class="line"></span><br><span class="line">            orthog_estimates[n, e, i] = <span class="built_in">np</span>.dot(baseline_eval_gaussian(x, V_orthog*norms, N[n]), # Orthogonalized <span class="string">&quot;G&quot;</span></span><br><span class="line">                        baseline_eval_gaussian(y, V_orthog*norms, N[n]))</span><br><span class="line">            </span><br><span class="line">        mse_iid_n.<span class="built_in">append</span>(mean_squared_error(<span class="literal">true</span>, iid_estimates[n, e])) # <span class="built_in">compare</span> m <span class="built_in">values</span></span><br><span class="line">        mse_orthog_n.<span class="built_in">append</span>(mean_squared_error(<span class="literal">true</span>, orthog_estimates[n, e]))</span><br><span class="line">    </span><br><span class="line">    MSE_iid_.<span class="built_in">append</span>(mse_iid_n)</span><br><span class="line">    MSE_iid.<span class="built_in">append</span>(<span class="built_in">np</span>.<span class="built_in">mean</span>(mse_iid_n))</span><br><span class="line">    </span><br><span class="line">    MSE_orthog_.<span class="built_in">append</span>(mse_orthog_n)</span><br><span class="line">    MSE_orthog.<span class="built_in">append</span>(<span class="built_in">np</span>.<span class="built_in">mean</span>(mse_orthog_n))</span><br></pre></td></tr></table></figure>
<pre><code>100%|██████████| 100/100 [00:00&lt;00:00, 41614.29it/s]
100%|██████████| 100/100 [00:21&lt;00:00,  4.61it/s]
100%|██████████| 100/100 [00:26&lt;00:00,  3.74it/s]
100%|██████████| 100/100 [00:31&lt;00:00,  3.16it/s]
100%|██████████| 100/100 [00:35&lt;00:00,  2.78it/s]
100%|██████████| 100/100 [00:40&lt;00:00,  2.47it/s]
100%|██████████| 100/100 [00:44&lt;00:00,  2.23it/s]
100%|██████████| 100/100 [00:49&lt;00:00,  2.00it/s]
100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]
100%|██████████| 100/100 [00:58&lt;00:00,  1.72it/s]
100%|██████████| 100/100 [01:02&lt;00:00,  1.60it/s]</code></pre>
<h5 id="visualization">Visualization</h5>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">sns.set_style(<span class="string">&quot;darkgrid&quot;</span>)    </span><br><span class="line"></span><br><span class="line">x1 = range(1, d_+1)</span><br><span class="line">x1 = x1 + np.zeros((episode, d_))</span><br><span class="line">x1 = np.sort(x1.reshape(episode<span class="number">*d</span>_,)) </span><br><span class="line"></span><br><span class="line">mse_iid_ = np.asarray(MSE_iid_).reshape(-1)</span><br><span class="line">category = [<span class="string">&#x27;MC&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(episode<span class="number">*d</span>_)]</span><br><span class="line">df1 = pd.DataFrame(&#123;<span class="string">&#x27;D/d&#x27;</span>: x1, <span class="string">&#x27;MSE&#x27;</span>:mse_iid_, <span class="string">&#x27;Category&#x27;</span>:category&#125;)</span><br><span class="line"></span><br><span class="line">mse_ortho_ = np.asarray(MSE_orthog_).reshape(-1)</span><br><span class="line">category = [<span class="string">&#x27;OMC&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(episode<span class="number">*d</span>_)]</span><br><span class="line">df3 = pd.DataFrame(&#123;<span class="string">&#x27;D/d&#x27;</span>: x1, <span class="string">&#x27;MSE&#x27;</span>:mse_ortho_, <span class="string">&#x27;Category&#x27;</span>:category&#125;)</span><br><span class="line"></span><br><span class="line">df = pd.concat([df1, df3], <span class="attribute">ignore_index</span>=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">sns.lineplot(<span class="attribute">x</span>=<span class="string">&quot;D/d&quot;</span>, <span class="attribute">y</span>=<span class="string">&quot;MSE&quot;</span>, <span class="attribute">hue</span>=<span class="string">&quot;Category&quot;</span>,</span><br><span class="line">                   <span class="attribute">err_style</span>=<span class="string">&quot;bars&quot;</span>, <span class="attribute">ci</span>=95, <span class="attribute">data</span>=df)</span><br><span class="line">plt.title(<span class="string">&#x27;Gaussian_kernel_Comparison&#x27;</span>)</span><br><span class="line">plt.savefig(f<span class="string">&#x27;Gaussian_kernel_Comparison_&#123;datetime.datetime.now()&#125;.png&#x27;</span>, dpi = 300)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/10/05/Data-Mining-Recitation1/DM_recitation_files/DM_recitation_34_0.png" /></p>
<figure class="highlight clean"><table><tr><td class="code"><pre><span class="line"># !sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-<span class="keyword">generic</span>-recommended</span><br></pre></td></tr></table></figure>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">!jupyter nbconvert --<span class="keyword">to</span> PDF <span class="module-access"><span class="module"><span class="identifier">DM_recitation</span>.</span></span>ipynb</span><br></pre></td></tr></table></figure>
<pre><code>[NbConvertApp] Converting notebook DM_recitation.ipynb to PDF
[NbConvertApp] Support files will be in DM_recitation_files/
[NbConvertApp] Making directory ./DM_recitation_files
[NbConvertApp] Writing 76544 bytes to ./notebook.tex
[NbConvertApp] Building PDF
[NbConvertApp] Running xelatex 3 times: [u&#39;xelatex&#39;, u&#39;./notebook.tex&#39;, &#39;-quiet&#39;]
[NbConvertApp] Running bibtex 1 time: [u&#39;bibtex&#39;, u&#39;./notebook&#39;]
[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations
[NbConvertApp] PDF successfully created
[NbConvertApp] Writing 75123 bytes to DM_recitation.pdf</code></pre>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Data-Mining-Recitation2</title>
    <url>/2020/10/11/Data-Mining-Recitation2/</url>
    <content><![CDATA[<h2 id="monte-carlo-approximation-techniques">Monte Carlo Approximation techniques</h2>
<h2 id="evolutionary-strategies-策略梯度">Evolutionary Strategies, 策略梯度</h2>
<p>Policy optimization can be done through gradient ascend:</p>
<p><span class="math display">\[\nabla_\theta \mathbb{E}_{\epsilon\sim\mathcal{N}(0, I)} F(\theta + \sigma \epsilon) = \frac{1}{\sigma}\mathbb{E}_{\epsilon\sim\mathcal{N}(0, I)} [ \epsilon F(\theta + \sigma\epsilon) ]\]</span></p>
<p><strong>Proof:</strong></p>
<p>一些参考资料：<br />
<a href="https://lilianweng.github.io/lil-log/2019/09/05/evolution-strategies.html">Evolution Strategies</a> <!-- $$
\begin{aligned}
& \nabla_\theta \mathbb{E}_{\theta\sim\mathcal{N}(\hat{\theta}, \sigma^2 I)} F(\theta) \\
&= \nabla_\theta \mathbb{E}_{\epsilon\sim\mathcal{N}(0, I)} F(\hat{\theta} + \sigma\epsilon) \\
&= \nabla_\theta \int_{\epsilon} p(\epsilon) F(\hat{\theta} + \sigma\epsilon) d\epsilon & \scriptstyle{\text{; Gaussian }p(\epsilon)=(2\pi)^{-\frac{n}{2}} \exp(-\frac{1}{2}\epsilon^\top\epsilon)} \\
&= \int_{\epsilon} p(\epsilon) \nabla_\epsilon \log p(\epsilon) \nabla_\theta \epsilon\;F(\hat{\theta} + \sigma\epsilon) d\epsilon & \scriptstyle{\text{; log-likelihood trick}}\\
&= \mathbb{E}_{\epsilon\sim\mathcal{N}(0, I)} [ \nabla_\epsilon \big(-\frac{1}{2}\epsilon^\top\epsilon\big) \nabla_\theta \big(\frac{\theta - \hat{\theta}}{\sigma}\big) F(\hat{\theta} + \sigma\epsilon) ] & \\
&= \mathbb{E}_{\epsilon\sim\mathcal{N}(0, I)} [ (-\epsilon) (\frac{1}{\sigma}) F(\hat{\theta} + \sigma\epsilon) ] & \\
&= \frac{1}{\sigma}\mathbb{E}_{\epsilon\sim\mathcal{N}(0, I)} [ \epsilon F(\hat{\theta} + \sigma\epsilon) ] & \scriptstyle{\text{; negative sign can be absorbed.}}
\end{aligned}
$$ --> <a href="https://arxiv.org/pdf/1703.03864.pdf">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a><br />
<a href="https://www.zhihu.com/question/275040392/answer/383301722">知乎回答 OpenAI ES</a><br />
<a href="https://blog.csdn.net/Xurui_Luo/article/details/106160581">CSDN推导</a></p>
<p><span class="math display">\[
\begin{aligned}
&amp;\nabla_\theta \mathbb{E}_{\epsilon\sim\mathcal{N}(0, I)} F(\theta + \sigma\epsilon) \\
&amp;= \nabla_\theta \int_{\epsilon} p(\epsilon) F(\theta + \sigma\epsilon) d\epsilon &amp; \scriptstyle{\text{; Gaussian }p(\epsilon)=(2\pi)^{-\frac{n}{2}} \exp(-\frac{1}{2}\epsilon^\top\epsilon)} \\
&amp;= \frac{1}{\sigma}\int_{\epsilon} \nabla_\epsilon p(\epsilon) F(\theta + \sigma\epsilon) d\epsilon  \\
&amp;= \frac{1}{\sigma}\int_{\epsilon} p(\epsilon) \nabla_\epsilon \log p(\epsilon) F(\hat{\theta} + \sigma\epsilon) d\epsilon &amp; \scriptstyle{\text{; log-likelihood trick}}\\
&amp;= \frac{1}{\sigma}\mathbb{E}_{\epsilon\sim\mathcal{N}(0, I)} [ \nabla_\epsilon \big(-\frac{1}{2}\epsilon^\top\epsilon\big)  F(\hat{\theta} + \sigma\epsilon) ] &amp; \\
&amp;= \frac{1}{\sigma}\mathbb{E}_{\epsilon\sim\mathcal{N}(0, I)} [ (-\epsilon)  F(\hat{\theta} + \sigma\epsilon) ] &amp; \\
&amp;= \frac{1}{\sigma}\mathbb{E}_{\epsilon\sim\mathcal{N}(0, I)} [ \epsilon F(\hat{\theta} + \sigma\epsilon) ] &amp; \scriptstyle{\text{; negative sign can be absorbed.}}
\end{aligned}
\]</span></p>
<p><a href="http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/">Log likelihood trick</a></p>
<p><a href="http://gregorygundersen.com/blog/2018/04/29/reparameterization/">The Reparameterization Trick</a></p>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>IEOR E4706: Deterministic Cash-Flows</title>
    <url>/2021/02/04/IEOR-E4706-Deterministic-Cash-Flows/</url>
    <content><![CDATA[<h2 id="利率理论">利率理论</h2>
<ul>
<li><p>复利 compounded interest <span class="math display">\[\lim_{n\to \infty} A(1+r/n)^{mn} = Ae^{rm}\]</span></p></li>
<li><p>“七二定律”： <span class="math display">\[e^{rm} = 2 \\
rm = \ln2 \\
m = \frac{\ln2}{r} = \frac{0.72}{r}\]</span></p></li>
<li><p>有效利率 effective interest rate，实际复合年利率</p></li>
<li><p><strong>Gordon Growth Model</strong> <span class="math display">\[V_0 = \frac{D_1}{r-g}\]</span></p></li>
</ul>
<p>公司估值；成长股价格估计。缺点是假设现金流固定。</p>
<h2 id="固收证券">固收证券</h2>
<p>如息票债券。受利率影响较大。可转股债券兼具固收和非固收的特征。</p>
<h3 id="年金永续年金和摊销">年金，永续年金和摊销</h3>
<p><span class="math inline">\(n\)</span> 期年金，每期支付 <span class="math inline">\(A\)</span> 的现价 <span class="math display">\[P = \frac Ar \bigg(1-\frac1{(1+r)^n}\bigg)\]</span></p>
<h3 id="ytm-到期收益率">YTM 到期收益率</h3>
<p>到期收益率使债券未来现金流的现值等于现价，即IRR。</p>
<h3 id="macauley-duration-麦考利久期">Macauley Duration 麦考利久期</h3>
<p>按现金流量折现值加权的支付日期。</p>
<p><span class="math display">\[D = \frac{\sum_{k=1}^{n}{(k/m)c_k/[1+(\lambda / m)]^k}}{P}\]</span></p>
<p>修正久期<span class="math inline">\(D_M\)</span>是P关于YTM的一阶导 / P: <span class="math display">\[D_M = \frac1P \frac{dP}{d\lambda}\]</span></p>
<p>麦考利久期 <span class="math inline">\(D\)</span> 与修正久期 <span class="math inline">\(D_M\)</span> 的关系是</p>
<p><span class="math display">\[D_M = \frac D {1+ \lambda/m}\]</span></p>
<h3 id="convexity-凸性">Convexity 凸性</h3>
<p>凸性与YTM二阶导有关。</p>
<p><span class="math display">\[C = \frac1P \frac{d^2P}{d\lambda^2}\]</span></p>
<h3 id="immunization-久期免疫">Immunization 久期免疫</h3>
<p><span class="math display">\[\begin{aligned}
P(\lambda+\Delta \lambda) &amp;\approx P(\lambda) + \Delta \lambda \frac{dP}{d\lambda}(\lambda) + \frac{(\Delta \lambda)^2}{2}\frac{d^2P}{d\lambda^2}(\lambda) \\
&amp;= P(\lambda) - D_M P(\lambda) \Delta \lambda + \frac{P(\lambda)C}{2} (\Delta \lambda)^2
\end{aligned}\]</span></p>
<p>这个二阶泰勒展开告诉我们当YTM有细微变化时，可以通过增加债券到投资组合实现现价不变。</p>
<p>考虑一系列债券的投资组合，其现价、久期、凸性满足</p>
<p><span class="math display">\[\begin{aligned} 
P_p &amp;= \sum{P_k} \\
D_p &amp;= \sum{\frac{P_k}{P_p}D_k}\\
C_p &amp;= \sum{\frac{P_k}{P_p}C_k}
\end{aligned}\]</span></p>
<p>这是一个包含三个等式的线性方程组。投资组合中只需要3种债券即可满足方程组有解。</p>
<p><span class="math display">\[\begin{bmatrix}P_o \\ D_o \\ C_o \end{bmatrix} = \begin{bmatrix}1 &amp;1 &amp;1 \\ \frac{D_1}{P_o} &amp; \frac{D_2}{P_o} &amp; \frac{D_3}{P_o} \\ \frac{C_1}{P_o} &amp; \frac{C_2}{P_o} &amp; \frac{C_3}{P_o} \end{bmatrix} \cdot \begin{bmatrix} P_1 \\ P_2 \\ P_3 \end{bmatrix}\]</span></p>
<p>然而以上结论基于的假设是 YTM "flat"， 并且 "only parallel shifts in interest rates" 即不同的债券的利率变化相同。</p>
<h3 id="案例">案例</h3>
<h4 id="callable-bond">Callable Bond</h4>
<p>20年可赎回债券，coupon 10%，发行时ytm 10%，5年后决定按1.05赎回，说明5-20的ytm <span class="math inline">\(\lambda &lt; 9.366\%\)</span> 。案例说明 ytm 会变化。</p>
<h4 id="久期免疫的可套利性">久期免疫的可套利性</h4>
<p>假设原先有一个obligation，生成一个immunization的组合，</p>
<p><span class="math display">\[P(0) = 0\]</span> <span class="math display">\[P&#39;(0) = 0 \\
P&#39;(0) (1+r)^2 = 0\]</span></p>
<p>通过叠加以上两式并加以构造，可以证明</p>
<p><span class="math display">\[P&#39;&#39;(0)(1+r)^2 \geq 0\]</span> 则 <span class="math inline">\(P(0)\)</span> 为 local minimum，无论 <span class="math inline">\(\lambda\)</span> 如何变动该久期免疫的组合都盈利。那么在假设利率 parallet shift的情境下就可以套利。所以该理论需要改进。</p>
<h2 id="term-structure-of-interest-rates-利率期限结构">Term Structure of Interest Rates 利率期限结构</h2>
<p>即期利率 spot rate</p>
<p>远期利率 forward rate</p>
<h3 id="zero-coupon-bond-构造无息债券">zero coupon bond 构造无息债券</h3>
<p>假设两组债券，通过组合使 coupon rate 为0，得出终值和现价，从而计算出该年限下的即期利率。</p>
<h3 id="利率期限结构理论与缺点">利率期限结构理论与缺点</h3>
<ol type="1">
<li>预期假说 Expectation Hypothesis：<span class="math inline">\(f_{i,j} = s_{j-i}\)</span> that prevails at time i。如此收益率曲线应该平坦。</li>
<li>市场分隔假说 market segmentation： 购买长期债券和短期债券的人群本质上不同。解释性不强。</li>
<li>流动性假说 liquidity preference：长期债券需要对风险做出补偿。</li>
</ol>
<h3 id="利率期限结构理论下的久期与免疫">利率期限结构理论下的久期与免疫</h3>
<p>从YTM到利率期限结构，久期和凸性仍有定义，并可以免疫<strong>利率期限结构整体的平行移动</strong>。</p>
<h3 id="案例和应用">案例和应用</h3>
<h4 id="浮动利率债券">浮动利率债券</h4>
<p>coupon rate 总等于当期的即期利率，则任意时间价值等同于面值。可以通过从最后一期开始递归来理解。</p>
<h4 id="抵押-mortgage-mathematics">抵押 Mortgage Mathematics</h4>
<p>已知 <span class="math inline">\(M(0) = M\)</span>，每期付款 <span class="math inline">\(B\)</span>，<span class="math inline">\(M(t)\)</span> 表示 <span class="math inline">\(t\)</span> 时刻剩余总额。</p>
<p><span class="math display">\[M(k) = (1+r) M(k-1) - B \]</span> <span class="math display">\[M(k) - \frac Br = (1+r) [M(k-1) - \frac Br] \]</span> <span class="math display">\[M(k) = (1+r)^kM + \frac Br[1-(1+r)^k] \]</span> <span class="math display">\[M(n) = (1+r)^nM + \frac Br[1-(1+r)^n] = 0\]</span> <span class="math display">\[\therefore B = \frac{(1+r)^nMr}{(1+r)^n-1}\]</span></p>
<p><span class="math display">\[M(k) = M\frac{(1+r)^n - (1+r)^k}{(1+r)^n - 1}\]</span></p>
<p>本金的付款现金流和利息的付款现金流现值的表达式 <span class="math inline">\(V\)</span> 和 <span class="math inline">\(W\)</span>: ...</p>
<p>本金 payment stream 的久期长于利息。</p>
<figure>
<img src="https://i.stack.imgur.com/dLnok.jpg" alt="Interest vs Principal" /><figcaption aria-hidden="true">Interest vs Principal</figcaption>
</figure>
<h4 id="动态规划-dp-在-lattice-中的应用">动态规划 DP 在 lattice 中的应用</h4>
<p>一个二叉树的定价</p>
<h2 id="yieldstaticoption-adjusted-spreads-收益率静态期权调整后的价差">Yield/Static/Option-Adjusted Spreads 收益率/静态/期权调整后的价差</h2>
<h3 id="收益率价差">收益率价差</h3>
<p>公司债和国债的 YTM 做差。不能反映利率的期限结构。</p>
<h3 id="静态价差">静态价差</h3>
<p>也叫 Zero-volatility spread / Z-spread 或有效价差。 假定公司债的利率期限结构是国债的利率期限结构整体平移 s 。根据定价和现金流反算出 s。</p>
<p><img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT3FuyqB7_uvfW5JjDvP7TQ_BXzeeinqZzSRg&amp;usqp=CAU" /></p>
<h3 id="利率调整价差-oas">利率调整价差 OAS</h3>
<p>callable / putable 的债含有期权。可以使用 the lattice model 定价，在 lattice 的每个节点加上 OAS。</p>
<h3 id="有效久期和有效凸性">有效久期和有效凸性</h3>
<p>债券含有期权性质(e.g. MBS)，当利率变化时现金流亦有可能变化。</p>
<h2 id="讲义">讲义</h2>
<div class="pdfobject-container" data-target="./0DeterministicCashFlows.pdf" data-height="1000px"></div>
]]></content>
      <categories>
        <category>Finance</category>
      </categories>
      <tags>
        <tag>Financial Engineering</tag>
      </tags>
  </entry>
  <entry>
    <title>IEOR E4706: Forwards, Swaps, Futures and Options</title>
    <url>/2021/02/09/IEOR-E4706-Forwards-Swaps-Futures-and-Options/</url>
    <content><![CDATA[<h2 id="简要">简要</h2>
<p>学习 <strong>鞅定价</strong> (margingale pricing) 之前，</p>
<p>Put-call parity of european options 欧式期权平价。</p>
<p>Replication portfolio for pricing the European call 复制组合 + 无套利是推导欧式期权定价的方法。</p>
<p>binomial tree for pricing the American put 二叉树方法定价期权，美式看跌期权可以提前行权。</p>
<p>Black-Scholes formula BS 公式。</p>
<hr />
<h2 id="forwards-远期">Forwards 远期</h2>
<p>参数：<span class="math inline">\(t, T, F, S_T, f_T\)</span></p>
<p>远期价值 <strong>value</strong>:</p>
<p><span class="math display">\[f_T = \pm(S_T - F)\]</span></p>
<p>远期价格 <strong>price</strong>：<span class="math inline">\(F\)</span></p>
<h3 id="计算远期的价格-f">计算远期的价格 <span class="math inline">\(F\)</span></h3>
<p>假设 underlying securities stored at 0-cost. storage cost 可以为正，如商品；也可以为负成本，如有派息的股票。</p>
<h4 id="零成本的情况">零成本的情况</h4>
<p>定义 discounting factor <span class="math inline">\(d(0,T) &lt; 1\)</span>, S 是 security 的 spot price</p>
<p><span class="math display">\[F = S / d(0,T)\]</span></p>
<h4 id="非零成本的情况">非零成本的情况</h4>
<p>远期价格应当在无成本的情况下加上折现到行权日的成本，因为不用承担期间成本。</p>
<p>如果成本发生在每期期初， <span class="math display">\[F = \frac{S}{d(0,M)} + \sum_{j=0}^{M-1} {\frac{c_j}{d(j, M)}}\]</span></p>
<p>如果成本发生在每期期末， <span class="math display">\[F = \frac{S}{d(0,M)} + \sum_{j=0}^{M-1} {\frac{c_j}{d(j+1, M)}}\]</span></p>
<h3 id="计算远期的价值-f_t-当-t0">计算远期的价值 <span class="math inline">\(f_t\)</span>， 当 <span class="math inline">\(t&gt;0\)</span></h3>
<p>根据无套利，<span class="math inline">\(f_0 = 0\)</span>,</p>
<p><span class="math display">\[f_t = (F_t - F_0) \cdot d(0,t)\]</span></p>
<h3 id="tight-markets">Tight Markets</h3>
<p>含成本的远期价格的计算表明，仓储成本为正的商品，其远期价格 <span class="math inline">\(F\)</span> 应随 <span class="math inline">\(M\)</span> 增加而增加， 而事实并非如此。有时商品不能short sell，则松弛此约束，short buy 一份商品以及卖出该商品 forwards 的方法可能可以套利。</p>
<p><span class="math display">\[F \leq \frac{S}{d(0,M)} + \sum_{j=0}^{M-1} {\frac{c_j}{d(j, M)}}\]</span></p>
<p>convenience yield: 对于持有商品者是一个 negative cost</p>
<p><span class="math display">\[F = \frac{S}{d(0,M)} + \sum_{j=0}^{M-1} {\frac{c_j - y}{d(j, M)}}\]</span></p>
<hr />
<h2 id="swaps-互换">Swaps 互换</h2>
<p>最常见的是利率和货币互换。</p>
<p>去除随机性, floating -&gt; fixed.</p>
<h3 id="利率互换">利率互换</h3>
<p>需规定本金 <span class="math inline">\(P\)</span>, 期数 <span class="math inline">\(M\)</span>, 到期日 <span class="math inline">\(T\)</span>。</p>
<p>理解多空：</p>
<ul>
<li><p><strong>Long</strong> the interest rate swap means: pay the counterpart a <strong>fixed</strong> rate.</p></li>
<li><p><strong>Short</strong>: floating rate.</p></li>
</ul>
<h3 id="货币互换">货币互换</h3>
<p>略</p>
<h3 id="互换的定价">互换的定价</h3>
<p>方法：确定现金流 + 无套利。</p>
<h4 id="例商品互换的定价">例：商品互换的定价</h4>
<p><span class="math display">\[ C = N \times (0, S_1-X, S_2-X, ..., S_M -X)\]</span></p>
<p>未来时间点的一系列的商品又等效于N份的远期</p>
<p><span class="math display">\[ V = N \sum_{i=1}^{M}{d(0,i)(F_i-X)}\]</span></p>
<p>根据无套利，<span class="math inline">\(X\)</span> 应当满足 <span class="math inline">\(V=0\)</span></p>
<h4 id="例利率互换的定价">例：利率互换的定价</h4>
<p><span class="math display">\[ C = P \times (0, r_0 - r_f, r_1 - r_f, ..., r_{M-1} - r_f)\]</span></p>
<p>浮动利率的部分恰好是面值为 <span class="math inline">\(P\)</span> 的浮动利率债券的派息，所以其贴现值为 <span class="math inline">\(P \left[1-d(0,M) \right]\)</span></p>
<p><span class="math display">\[V = P \left[1-d(0,M) - r_f\sum_{i=1}^{M}{d(0,i)}\right]\]</span></p>
<hr />
<h2 id="期货-futures">期货 Futures</h2>
<p>交易所，标准化合约，相比远期，减少了对手方风险 (counter-party risk)</p>
<p>underlying asset 可以为任意变量，如板球比分。</p>
<p>期货数量会放大变动 - 杠杆 leverage。</p>
<p>保证金 - margin call - 强制平仓 close out</p>
<h3 id="优劣">优劣</h3>
<p>... 如前所述</p>
<p>期货价格或多或少与标的资产价格线性相关，也有一些非线性风险。</p>
<h3 id="期货价格-远期价格-即期价格-的关系">期货价格 / 远期价格 / 即期价格 的关系</h3>
<ol type="1">
<li><p>鞅定价方法可以解释当利率随机时，如果利率变化和价格变化正相关，期货价格比远期价格高；否则期货价格比远期价格低。</p></li>
<li><p><span class="math inline">\(F\)</span> 和 <span class="math inline">\(E[S_T]\)</span> 的关系: 当标的资产 systematic risk 为正，<span class="math inline">\(F &lt; E[S_T]\)</span>. If you sell the futures and long one asset at <span class="math inline">\(t=0\)</span> , you would bear the risk and on expectation earn <span class="math inline">\(E(S_T - F) &gt; 0\)</span>.</p></li>
</ol>
<h3 id="用期货对冲完全对冲和最小方差对冲-minimum-variance-hedges">用期货对冲：完全对冲和最小方差对冲 Minimum-Variance Hedges</h3>
<p>期货是对冲标的资产<strong>线性</strong>风险的工具，因为到期日价格 <span class="math inline">\(S_T\)</span> 线性地影响期货的价值。</p>
<h4 id="完全对冲">完全对冲</h4>
<p>销售大宗商品 - 防止价格下跌，提前锁定价格 - 卖出 (sell short) 期货。</p>
<p>完全对冲常常不存在: * 整数手数 * 标的资产存在差别 * 到期日不完全符合 * 期货价格 <span class="math inline">\(P_T\)</span> 和标的资产价格不线性。</p>
<h4 id="最小方差对冲">最小方差对冲</h4>
<p>创建一个投资组合，考虑 terminal day 需要对冲的现金流为 <span class="math inline">\(Z_T\)</span> ，买入 <span class="math inline">\(h\)</span> 手的期货进行对冲，<span class="math inline">\(t\)</span> 时刻的期货价格记为 <span class="math inline">\(F_t\)</span></p>
<p><span class="math display">\[ Y_T  = Z_T + h(F_T - F_0)\]</span></p>
<p>最小化方差</p>
<p><span class="math display">\[ Var(Y_T) = Var(Z_T) + h^2Var(F_T) + 2hCov(Z_T, F_T)\]</span></p>
<p><span class="math display">\[h^* = -\frac{Cov(Z_T, F_T)}{Var(F_T)}\]</span></p>
<p><span class="math display">\[Var(Y_T^*) = Var(Z_T) - \frac{Cov(Z_T, F_T)^2}{Var(F_T)}\]</span></p>
<p>可以看出完全对冲需要满足的条件是：期货价格与标的资产价格完全线性相关，即没有非线性项。实际上这一条件不成立，所以不存在完全对冲。</p>
<p><span class="math display">\[Var(Z_T)Var(F_T) = Cov(Z_T, F_T)^2\]</span></p>
<p><span class="math display">\[corr(Z_T, F_T) = \pm 1\]</span></p>
<p>另一个结论是只要 <span class="math inline">\(E[F_T] \neq F_0\)</span>, <span class="math inline">\(E[Z_T] \neq E[Y_T^*]\)</span>，对冲就有作用。</p>
<h4 id="例子">例子</h4>
<p>假设 <span class="math inline">\(y = S_T W + (F_T - F_0)h\)</span>。</p>
<p>符号表示 <span class="math inline">\(\sigma_S^2 = Var(S_T), \sigma_F^2 = Var(F_T), \sigma_{ST} = Cov(S_T, F_T)\)</span>。 <span class="math inline">\(k\)</span> 是标的资产总价与期货每手价格的比值， <span class="math inline">\(h = -kw\)</span></p>
<p><span class="math display">\[\sigma_y = W\sigma_S \sqrt{1+\left(\frac{S_0\sigma_F}{F_0\sigma_S}\right)^2-2\frac{S_0\sigma_{S,F}}{F_0\sigma_S^2}}\]</span></p>
<p>当 <span class="math inline">\(S_T, F_T\)</span> 完全正相关，</p>
<p><span class="math display">\[\sigma_y = W\sigma_S(1-\frac{S_0\sigma_{F}}{F_0\sigma_S})\]</span></p>
<p>通常 <span class="math inline">\(\sigma_y\)</span> 仍然不是0。但在比例合适的情况下有可能实现完全对冲。</p>
<h4 id="例子对冲盈利">例子：对冲盈利</h4>
<p>假设现金流是两个产品利润之和，产品价格中的宏观经济项有几何布朗运动形式，经推导</p>
<p><span class="math display">\[h^* = -Cov(R,S_2)/Var(S_2)\]</span></p>
<h4 id="动态对冲策略-dynamic-hedging-strategy">动态对冲策略 dynamic hedging strategy</h4>
<p>随着时间更新的动态的对冲策略。<span class="math inline">\(h_2\)</span> 是基于 <span class="math inline">\(t_1\)</span> 已有信息的函数。</p>
<h3 id="备注">备注</h3>
<p>常见的期货市场包括 利率期货 和 股指期货 (equity index futures)。指数期货而非指数本身被用于对冲指数期权。</p>
<p>Roll the hedge forward: 期货到期日比合同日期更前。这样做的风险在于，假设目前市场上的期货合约到期日为 <span class="math inline">\(T_1 &lt; T_2\)</span>，在 0 时刻卖出一份这样的期货并在 <span class="math inline">\(T_1\)</span> 时刻平仓，然后在 <span class="math inline">\(T_1\)</span> 时刻卖出一份 <span class="math inline">\(T_2\)</span> 到期的期货，在 <span class="math inline">\(T_2\)</span> 平仓，那么 <span class="math inline">\(T_2\)</span> 净现金流为:</p>
<p><span class="math display">\[Y_2 = F_1 - S_2\]</span></p>
<p><span class="math inline">\(F_1\)</span>的风险仍然是未知的。</p>
<p>关于期货价格 <span class="math inline">\(F_t\)</span> 的定价在鞅定价部分讨论。</p>
<hr />
<h2 id="intro-期权和二叉树模型">Intro: 期权和二叉树模型</h2>
<p>定义：欧（美）式看涨（跌）期权...</p>
<p>european-call 在 <span class="math inline">\(t &lt; T\)</span> 的内在价值是 <span class="math inline">\(\max{\{S_t-K, 0\}}\)</span>. european-put 反号。</p>
<h3 id="期权定价的约束边界-无模型">期权定价的约束边界 (无模型)</h3>
<p>标的资产价格 <span class="math inline">\(S_t\)</span> 是一个随机过程；期权的回报是关于标的资产价格的非线性函数；</p>
<p>因而期权的定价需要建模。但无需模型也能得到一些必要条件。</p>
<p>符号表示： <span class="math inline">\(C_E(t;K,T)\)</span> 表示欧式看涨在 <span class="math inline">\(t\)</span> 时刻的价格，行权价<span class="math inline">\(K\)</span>，到期日<span class="math inline">\(T\)</span>。</p>
<p>美式期权的价格应不低于欧式期权，因其有更高的自由度。</p>
<h4 id="期权平价欧式-put-call-parity">期权平价(欧式) put-call parity</h4>
<p><span class="math display">\[p_E(t;K,T) + S_t = c_E(t;K,T) + Kd(t,T)\]</span></p>
<blockquote>
<p>Wiki: 一份由买入欧式看涨期权和卖出欧式看跌期权组合成的投资组合，其价格等于一份与它们有相同标的资产、行使价与到期日的远期合约的价格。这是因为在到期日，如果资产价格高于行使价，则会执行欧式看涨期权，反之则执行欧式看跌期权。在任一种情况下，都等于用行使价买入一单位标的资产。因此这个投资组合等价于在到期日用行使价买入一单位标的资产的远期合约。在无套利原则下，两者在初始的价格应当等同，此即买卖权平价关系。</p>
</blockquote>
<p>分析： - <span class="math inline">\(C-P\)</span> 在到期日必然执行其一： - 若 <span class="math inline">\(S_T &gt; K\)</span>，则主动执行 call， <span class="math inline">\(S_T - K\)</span>，盈利 - 若 <span class="math inline">\(S_T &lt; K\)</span>，则 put 被执行，<span class="math inline">\(S_T - K\)</span>，损失 - 无论哪一种都等同于成交价为 <span class="math inline">\(K\)</span> 的远期合约 - 价值等同于 <span class="math inline">\(S_t - Kd(t,T)\)</span></p>
<p>对于该证券派息的情况，</p>
<p><span class="math display">\[p_E(t;K,T) + S_t - D = c_E(t;K,T) + Kd(t,T)\]</span></p>
<p><span class="math inline">\(D\)</span> 是所有股利的现值。</p>
<p>假设无派息、<span class="math inline">\(S_T &gt; K\)</span> 和 <span class="math inline">\(S_T &lt; K\)</span> 都有正概率 （否则无风险套利）。于是 put 和 call 都有正价格。</p>
<p><span class="math display">\[c_E(t;K,T) = S_t + p_E(t;K,T) - Kd(t,T) &gt; S_t - Kd(t,T)\]</span></p>
<p>美式期权价格高于欧式</p>
<p><span class="math display">\[c_A(t;K,T) \geq c_E(t;K,T) &gt; \max \{S_t - Kd(t,T),0\} \geq \max\{ S_t - K,0 \}\]</span></p>
<p>上式表明</p>
<ul>
<li>当<span class="math inline">\(S_T &gt;/&lt; K\)</span> 都有正的概率 ，<strong>无派息股票的 American call 的价格总是严格大于其内在价值</strong>。</li>
<li><strong>American call 提前行权总是不划算的</strong>。
<ul>
<li>call 是付 <span class="math inline">\(K\)</span> 得 <span class="math inline">\(S\)</span>。考虑 <span class="math inline">\(K\)</span> 的时间价值，晚支付更好。</li>
</ul></li>
</ul>
<h3 id="一阶段二叉树模型-定价">一阶段二叉树模型 （定价）</h3>
<p>一些参数：</p>
<ul>
<li><span class="math inline">\(R\)</span>：每阶段无风险增长比率。</li>
<li><span class="math inline">\(u, p\)</span>：增长 / 减少比率。</li>
<li><span class="math inline">\(S_0\)</span>：0时刻标的资产价格</li>
</ul>
<p>无套利要求</p>
<p><span class="math display">\[d&lt;R&lt;u\]</span></p>
<h4 id="复制组合定价">复制组合定价</h4>
<p>假设买入x份 <span class="math inline">\(c_E\)</span> 和持有y份现金，<span class="math inline">\(T\)</span>时刻对应有两种实际收益 <span class="math inline">\(C_u, C_d\)</span>。</p>
<p><span class="math display">\[uS_0x + Ry = C_u\]</span> <span class="math display">\[dS_0x + Ry = C_d\]</span></p>
<p><span class="math display">\[\begin{bmatrix}u &amp;R \\d &amp;R\end{bmatrix} \begin{bmatrix}S_0x \\y\end{bmatrix} = \begin{bmatrix}C_u \\C_d\end{bmatrix}\]</span></p>
<p><span class="math inline">\(0\)</span> 时刻复制组合的定价，也就是该看涨期权的定价</p>
<p><span class="math display">\[\begin{aligned} S_0x + y &amp;=  \begin{bmatrix}1 &amp;1\end{bmatrix} \begin{bmatrix}S_0x \\y\end{bmatrix}\\
&amp;= \begin{bmatrix}1 &amp;1\end{bmatrix} \begin{bmatrix}u &amp;R \\d &amp;R\end{bmatrix}^{-1}\begin{bmatrix}C_u \\C_d\end{bmatrix} \\
&amp;= \begin{bmatrix}1 &amp;1\end{bmatrix} \begin{bmatrix}\frac{1}{u-d} &amp;-\frac{1}{(u-d)} \\-\frac{d}{R(u-d)} &amp;\frac{u}{R(u-d)} \end{bmatrix}\begin{bmatrix}C_u \\C_d\end{bmatrix} \\
&amp;= C_u\frac{R-d}{R(u-d)} + C_d\frac{u-R}{R(u-d)} \\
&amp;= \frac1R \left[ C_u \frac{R-d}{u-d} + C_d \frac{u-R}{u-d}  \right] \\
&amp;= \frac1R \left[ qC_u + (1-q)C_d \right] \\
&amp;= \frac1R E_0^Q[C1]
\end{aligned}\]</span></p>
<p>定义</p>
<p><span class="math display">\[q = C_u \frac{R-d}{u-d}\]</span></p>
<p>以上称为<strong>风险中性定价</strong> risk-neutral pricing，<span class="math inline">\(q, (1-q)\)</span> 称为<strong>风险中性概率</strong>。</p>
<p>可以看出给期权定价用到的<strong>风险中性概率</strong>是由 <span class="math inline">\(u,d,R\)</span> 决定的，而不是由二叉树本身的概率分布 <span class="math inline">\(p\)</span> 决定。</p>
<h3 id="多阶段二叉树模型">多阶段二叉树模型</h3>
<p>多阶段不过是一阶段的多次叠加。最终结果的概率分布是二项分布（binomial distribution）。</p>
<h4 id="c_e-的定价"><span class="math inline">\(c_E\)</span> 的定价</h4>
<p>借助公式</p>
<p><span class="math display">\[C_0 = \frac1{R^3}E_0^Q[\max{S_T-K, 0}]\]</span></p>
<p>可以略去二叉树中间节点的繁琐计算。</p>
<h4 id="p_a-的定价"><span class="math inline">\(p_A\)</span> 的定价</h4>
<p><span class="math inline">\(c_A\)</span> 不提前行权，等同于 <span class="math inline">\(c_E\)</span></p>
<p>美式的二叉树从最末端出发，每个节点处定价取 风险中性定价 和 内在价值的较大值</p>
<p><span class="math display">\[ V_t(S) = \max{\left[K-S， \frac1RE_t^Q[V_{t+1}(S_{t+1})] \right] }\]</span></p>
<hr />
<h2 id="几何布朗运动校正后的二叉树模型">几何布朗运动校正后的二叉树模型</h2>
<p>具体解释了为什么风险中性概率 <span class="math inline">\(q\)</span> 与上升概率 <span class="math inline">\(p\)</span> 无关，而仅与 <span class="math inline">\(u,d,R\)</span> 有关。</p>
<hr />
<h2 id="讲义">讲义</h2>
<div class="pdfobject-container" data-target="./1for_swap_fut_options.pdf" data-height="1000px"></div>
]]></content>
      <categories>
        <category>Finance</category>
      </categories>
      <tags>
        <tag>Financial Engineering</tag>
      </tags>
  </entry>
  <entry>
    <title>Kaggle: Titanic</title>
    <url>/2020/11/06/Kaggle-Titanic/</url>
    <content><![CDATA[<h1 id="the-titanic">The Titanic</h1>
<p><strong><a href="https://www.kaggle.com/c/titanic">Kaggle: Titanic</a></strong></p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">Simple</span> implementation with basic data cleaning, one-hot encoding and lightGBM classifier.</span><br><span class="line"></span><br><span class="line"><span class="attribute">Score</span>: <span class="number">0</span>.<span class="number">76794</span></span><br><span class="line"><span class="attribute">Rank</span>: <span class="number">11846</span>/<span class="number">17593</span></span><br></pre></td></tr></table></figure>
<h1 id="import-packages-load-dataset">Import Packages / Load Dataset</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%cd /content/drive/My Drive/Kaggle/titanic</span><br></pre></td></tr></table></figure>
<pre><code>/content/drive/My Drive/Kaggle/titanic</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> lightgbm</span><br><span class="line"><span class="keyword">import</span> xgboost</span><br><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_train = pd.read_csv(os.getcwd()+<span class="string">&#x27;/train.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="data-exploration">Data Exploration</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_train.describe()</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Fare
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
count
</th>
<td>
891.000000
</td>
<td>
891.000000
</td>
<td>
891.000000
</td>
<td>
714.000000
</td>
<td>
891.000000
</td>
<td>
891.000000
</td>
<td>
891.000000
</td>
</tr>
<tr>
<th>
mean
</th>
<td>
446.000000
</td>
<td>
0.383838
</td>
<td>
2.308642
</td>
<td>
29.699118
</td>
<td>
0.523008
</td>
<td>
0.381594
</td>
<td>
32.204208
</td>
</tr>
<tr>
<th>
std
</th>
<td>
257.353842
</td>
<td>
0.486592
</td>
<td>
0.836071
</td>
<td>
14.526497
</td>
<td>
1.102743
</td>
<td>
0.806057
</td>
<td>
49.693429
</td>
</tr>
<tr>
<th>
min
</th>
<td>
1.000000
</td>
<td>
0.000000
</td>
<td>
1.000000
</td>
<td>
0.420000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
</tr>
<tr>
<th>
25%
</th>
<td>
223.500000
</td>
<td>
0.000000
</td>
<td>
2.000000
</td>
<td>
20.125000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
7.910400
</td>
</tr>
<tr>
<th>
50%
</th>
<td>
446.000000
</td>
<td>
0.000000
</td>
<td>
3.000000
</td>
<td>
28.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
14.454200
</td>
</tr>
<tr>
<th>
75%
</th>
<td>
668.500000
</td>
<td>
1.000000
</td>
<td>
3.000000
</td>
<td>
38.000000
</td>
<td>
1.000000
</td>
<td>
0.000000
</td>
<td>
31.000000
</td>
</tr>
<tr>
<th>
max
</th>
<td>
891.000000
</td>
<td>
1.000000
</td>
<td>
3.000000
</td>
<td>
80.000000
</td>
<td>
8.000000
</td>
<td>
6.000000
</td>
<td>
512.329200
</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_train.head()</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Ticket
</th>
<th>
Fare
</th>
<th>
Cabin
</th>
<th>
Embarked
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Braund, Mr. Owen Harris
</td>
<td>
male
</td>
<td>
22.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
A/5 21171
</td>
<td>
7.2500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th...
</td>
<td>
female
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
PC 17599
</td>
<td>
71.2833
</td>
<td>
C85
</td>
<td>
C
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
1
</td>
<td>
3
</td>
<td>
Heikkinen, Miss. Laina
</td>
<td>
female
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
STON/O2. 3101282
</td>
<td>
7.9250
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Futrelle, Mrs. Jacques Heath (Lily May Peel)
</td>
<td>
female
</td>
<td>
35.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
113803
</td>
<td>
53.1000
</td>
<td>
C123
</td>
<td>
S
</td>
</tr>
<tr>
<th>
4
</th>
<td>
5
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Allen, Mr. William Henry
</td>
<td>
male
</td>
<td>
35.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
373450
</td>
<td>
8.0500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_train.isna().sum()</span><br><span class="line"><span class="comment"># Age, Cabin have lots of missing values.</span></span><br></pre></td></tr></table></figure>
<pre><code>PassengerId      0
Survived         0
Pclass           0
Name             0
Sex              0
Age            177
SibSp            0
Parch            0
Ticket           0
Fare             0
Cabin          687
Embarked         2
dtype: int64</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_train.isna().mean()</span><br></pre></td></tr></table></figure>
<pre><code>PassengerId    0.000000
Survived       0.000000
Pclass         0.000000
Name           0.000000
Sex            0.000000
Age            0.198653
SibSp          0.000000
Parch          0.000000
Ticket         0.000000
Fare           0.000000
Cabin          0.771044
Embarked       0.002245
dtype: float64</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_train.Embarked.unique()</span><br></pre></td></tr></table></figure>
<pre><code>array([&#39;S&#39;, &#39;C&#39;, &#39;Q&#39;, nan], dtype=object)</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Cabin Types</span></span><br><span class="line">df_train.Cabin.dropna().map(<span class="keyword">lambda</span> x: x[<span class="number">0</span>]).unique()</span><br></pre></td></tr></table></figure>
<pre><code>array([&#39;C&#39;, &#39;E&#39;, &#39;G&#39;, &#39;D&#39;, &#39;A&#39;, &#39;B&#39;, &#39;F&#39;, &#39;T&#39;], dtype=object)</code></pre>
<h1 id="preprocessing">Preprocessing</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_clean = df_train.copy()</span><br><span class="line"><span class="comment"># Make Sex a binary attribute</span></span><br><span class="line">df_clean.Sex = df_clean.Sex.apply(<span class="keyword">lambda</span> x: (x==<span class="string">&#x27;male&#x27;</span>) * <span class="number">1</span>)</span><br><span class="line"><span class="comment"># Cabin: Keep the initial</span></span><br><span class="line">df_clean.Cabin = df_clean.Cabin.map(<span class="keyword">lambda</span> x: re.findall(<span class="string">&quot;^[a-zA-Z]&quot;</span>, x)[<span class="number">0</span>] <span class="keyword">if</span> <span class="keyword">not</span> (x <span class="keyword">is</span> np.nan) <span class="keyword">else</span> x)</span><br><span class="line"><span class="comment"># Title from Name</span></span><br><span class="line">df_clean[<span class="string">&quot;Title&quot;</span>] = df_clean.Name.map(<span class="keyword">lambda</span> x: re.findall(pattern = <span class="string">&quot;([A-Z][a-zA-Z]+)\.&quot;</span>, string = x)[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># Make rare titles &quot;Other&quot;</span></span><br><span class="line">df_clean.Title = df_clean.Title.map(<span class="keyword">lambda</span> x: <span class="string">&quot;Other&quot;</span> <span class="keyword">if</span> (df_clean.Title.value_counts()[x] &lt; <span class="number">10</span>) <span class="keyword">else</span> x)</span><br><span class="line"><span class="comment"># Impute Cabin simply by set &quot;Unk&quot; group</span></span><br><span class="line">df_clean.Cabin = df_clean.Cabin.fillna(value=<span class="string">&quot;Unk&quot;</span>)</span><br><span class="line"><span class="comment"># Impute Age by the median of same Pclass&amp;Title</span></span><br><span class="line">byPclassAndTitle = df_clean.groupby([<span class="string">&quot;Pclass&quot;</span>, <span class="string">&quot;Title&quot;</span>]).agg(np.median)[<span class="string">&quot;Age&quot;</span>].reset_index()</span><br><span class="line">df_clean = pd.concat(</span><br><span class="line">    [</span><br><span class="line">     df_clean[df_clean.Age.notna()], </span><br><span class="line">     pd.merge(df_clean[df_clean.Age.isna()], byPclassAndTitle, how=<span class="string">&#x27;left&#x27;</span>, on=[<span class="string">&quot;Pclass&quot;</span>, <span class="string">&quot;Title&quot;</span>]).drop(columns=<span class="string">&quot;Age_x&quot;</span>).rename(columns=&#123;<span class="string">&quot;Age_y&quot;</span>:<span class="string">&quot;Age&quot;</span>&#125;)[df_clean.columns]</span><br><span class="line">    ],</span><br><span class="line">    axis=<span class="number">0</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># Embarked</span></span><br><span class="line">df_clean.Embarked = df_clean.Embarked.fillna(<span class="string">&quot;unk&quot;</span>)</span><br><span class="line">df_clean.sort_values(<span class="string">&quot;PassengerId&quot;</span>, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_clean.Title.value_counts()</span><br></pre></td></tr></table></figure>
<pre><code>Mr        517
Miss      182
Mrs       125
Master     40
Other      27
Name: Title, dtype: int64</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_clean.Embarked.value_counts()</span><br></pre></td></tr></table></figure>
<pre><code>S      644
C      168
Q       77
unk      2
Name: Embarked, dtype: int64</code></pre>
<h1 id="model">Model</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_clean.columns</span><br></pre></td></tr></table></figure>
<pre><code>Index([&#39;PassengerId&#39;, &#39;Survived&#39;, &#39;Pclass&#39;, &#39;Name&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;,
       &#39;Parch&#39;, &#39;Ticket&#39;, &#39;Fare&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;, &#39;Title&#39;],
      dtype=&#39;object&#39;)</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kept_cols = [&#x27;Pclass&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;, &#x27;SibSp&#x27;,</span></span><br><span class="line"><span class="comment">#        &#x27;Parch&#x27;, &#x27;Fare&#x27;, &#x27;Cabin&#x27;, &#x27;Embarked&#x27;, &#x27;Title&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cat_cols = [&#x27;Cabin&#x27;, &#x27;Embarked&#x27;, &#x27;Title&#x27;]</span></span><br><span class="line"></span><br><span class="line">kept_cols = [<span class="string">&#x27;Pclass&#x27;</span>, <span class="string">&#x27;Sex&#x27;</span>, <span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;SibSp&#x27;</span>,</span><br><span class="line">       <span class="string">&#x27;Parch&#x27;</span>, <span class="string">&#x27;Fare&#x27;</span> , <span class="string">&#x27;Title&#x27;</span>]</span><br><span class="line">cat_cols = [<span class="string">&#x27;Title&#x27;</span>]</span><br><span class="line"></span><br><span class="line">num_cols = [i <span class="keyword">for</span> i <span class="keyword">in</span> kept_cols <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> cat_cols]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = pd.concat([df_clean[num_cols], pd.get_dummies(df_clean[cat_cols], drop_first=<span class="literal">True</span>)], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight ebnf"><table><tr><td class="code"><pre><span class="line"><span class="attribute">X</span></span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Pclass
</th>
<th>
Sex
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Fare
</th>
<th>
Title_Miss
</th>
<th>
Title_Mr
</th>
<th>
Title_Mrs
</th>
<th>
Title_Other
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
3
</td>
<td>
1
</td>
<td>
22.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
7.2500
</td>
<td>
0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
1
</th>
<td>
1
</td>
<td>
0
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
71.2833
</td>
<td>
0
</td>
<td>
0
</td>
<td>
1
</td>
<td>
0
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
0
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
7.9250
</td>
<td>
1
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
3
</th>
<td>
1
</td>
<td>
0
</td>
<td>
35.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
53.1000
</td>
<td>
0
</td>
<td>
0
</td>
<td>
1
</td>
<td>
0
</td>
</tr>
<tr>
<th>
4
</th>
<td>
3
</td>
<td>
1
</td>
<td>
35.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
8.0500
</td>
<td>
0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
...
</th>
<td>
...
</td>
<td>
...
</td>
<td>
...
</td>
<td>
...
</td>
<td>
...
</td>
<td>
...
</td>
<td>
...
</td>
<td>
...
</td>
<td>
...
</td>
<td>
...
</td>
</tr>
<tr>
<th>
886
</th>
<td>
2
</td>
<td>
1
</td>
<td>
27.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
13.0000
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
1
</td>
</tr>
<tr>
<th>
887
</th>
<td>
1
</td>
<td>
0
</td>
<td>
19.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
30.0000
</td>
<td>
1
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
176
</th>
<td>
3
</td>
<td>
0
</td>
<td>
18.0
</td>
<td>
1
</td>
<td>
2
</td>
<td>
23.4500
</td>
<td>
1
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
889
</th>
<td>
1
</td>
<td>
1
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
30.0000
</td>
<td>
0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
890
</th>
<td>
3
</td>
<td>
1
</td>
<td>
32.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
7.7500
</td>
<td>
0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
</tbody>
</table>
<p>
891 rows × 10 columns
</p>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = df_clean[<span class="string">&#x27;Survived&#x27;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line">clf = lightgbm.LGBMClassifier(</span><br><span class="line">    max_depth=<span class="number">5</span>,</span><br><span class="line">    min_child_weight=<span class="number">0.1</span>,</span><br><span class="line">    n_jobs=<span class="number">-1</span>, num_leaves=<span class="number">15</span>, </span><br><span class="line">)</span><br><span class="line"><span class="comment"># clf = xgboost.XGBClassifier()</span></span><br><span class="line">clf.fit(X=X_train, y=y_train)</span><br><span class="line">clf.score(X_val, y_val)</span><br></pre></td></tr></table></figure>
<pre><code>0.8715083798882681</code></pre>
<h1 id="do-the-same-thing-for-our-test-data">Do the same thing for our test data</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_test = pd.read_csv(os.getcwd()+<span class="string">&#x27;/test.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_test_clean = df_test.copy()</span><br><span class="line"><span class="comment"># Make Sex a binary attribute</span></span><br><span class="line">df_test_clean.Sex = df_test_clean.Sex.apply(<span class="keyword">lambda</span> x: (x==<span class="string">&#x27;male&#x27;</span>) * <span class="number">1</span>)</span><br><span class="line"><span class="comment"># Cabin: Keep the initial</span></span><br><span class="line">df_test_clean.Cabin = df_test_clean.Cabin.map(<span class="keyword">lambda</span> x: re.findall(<span class="string">&quot;^[a-zA-Z]&quot;</span>, x)[<span class="number">0</span>] <span class="keyword">if</span> <span class="keyword">not</span> (x <span class="keyword">is</span> np.nan) <span class="keyword">else</span> x)</span><br><span class="line"><span class="comment"># Title from Name</span></span><br><span class="line">df_test_clean[<span class="string">&quot;Title&quot;</span>] = df_test_clean.Name.map(<span class="keyword">lambda</span> x: re.findall(pattern = <span class="string">&quot;([A-Z][a-zA-Z]+)\.&quot;</span>, string = x)[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># Make rare titles &quot;Other&quot;</span></span><br><span class="line">df_test_clean.Title = df_test_clean.Title.map(<span class="keyword">lambda</span> x: <span class="string">&quot;Other&quot;</span> <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&quot;Mr&quot;</span>, <span class="string">&quot;Miss&quot;</span>, <span class="string">&quot;Mrs&quot;</span>, <span class="string">&quot;Master&quot;</span>] <span class="keyword">else</span> x)</span><br><span class="line"><span class="comment"># Impute Cabin simply by set &quot;Unk&quot; group</span></span><br><span class="line">df_test_clean.Cabin = df_test_clean.Cabin.fillna(value=<span class="string">&quot;Unk&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Impute Age by the median of same Pclass&amp;Title</span></span><br><span class="line"><span class="comment"># Use the result from train</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df_test_clean = pd.concat(</span><br><span class="line">    [</span><br><span class="line">     df_test_clean[df_test_clean.Age.notna()], </span><br><span class="line">     pd.merge(df_test_clean[df_test_clean.Age.isna()], byPclassAndTitle, how=<span class="string">&#x27;left&#x27;</span>, on=[<span class="string">&quot;Pclass&quot;</span>, <span class="string">&quot;Title&quot;</span>]).drop(columns=<span class="string">&quot;Age_x&quot;</span>).rename(columns=&#123;<span class="string">&quot;Age_y&quot;</span>:<span class="string">&quot;Age&quot;</span>&#125;)[df_test_clean.columns]</span><br><span class="line">    ],</span><br><span class="line">    axis=<span class="number">0</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># Embarked</span></span><br><span class="line">df_test_clean.Embarked = df_test_clean.Embarked.fillna(<span class="string">&quot;unk&quot;</span>)</span><br><span class="line">df_test_clean.sort_values(<span class="string">&quot;PassengerId&quot;</span>, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_test = pd.concat([df_test_clean[num_cols], pd.get_dummies(df_test_clean[cat_cols], drop_first=<span class="literal">True</span>)], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_test = X_test.assign(Cabin_T = <span class="number">0</span>).assign(Embarked_unk=<span class="number">0</span>)[X_train.columns]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf.predict(X_test)</span><br></pre></td></tr></table></figure>
<pre><code>array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
       1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
       1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
       1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
       1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
       0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
       0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
       0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
       0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
       1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1])</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_test_truth = pd.read_csv(os.getcwd()+<span class="string">&#x27;/gender_submission.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf.score(X_test, y_test_truth.Survived)</span><br></pre></td></tr></table></figure>
<pre><code>0.8827751196172249</code></pre>
<h1 id="export-prediction">Export Prediction</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_submission = y_test_truth.copy()</span><br><span class="line">y_submission[<span class="string">&quot;Survived&quot;</span>] = clf.predict(X_test)</span><br><span class="line">y_submission.set_index(<span class="string">&quot;PassengerId&quot;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">y_submission.to_csv(os.getcwd()+<span class="string">&#x27;/Submission.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title>LC1541平衡括号字符串的最少插入次数</title>
    <url>/2020/10/23/LC1541%E5%B9%B3%E8%A1%A1%E6%8B%AC%E5%8F%B7%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%9C%80%E5%B0%91%E6%8F%92%E5%85%A5%E6%AC%A1%E6%95%B0/</url>
    <content><![CDATA[<h2 id="题目描述"><a href="https://leetcode-cn.com/problems/minimum-insertions-to-balance-a-parentheses-string/">题目描述</a></h2>
<p>给你一个括号字符串 s ，它只包含字符 '(' 和 ')' 。一个括号字符串被称为平衡的当它满足：</p>
<p>任何左括号 '(' 必须对应两个连续的右括号 '))' 。 左括号 '(' 必须在对应的连续两个右括号 '))' 之前。 比方说 "())"， "())(())))" 和 "(())())))" 都是平衡的， ")()"， "()))" 和 "(()))" 都是不平衡的。</p>
<p>你可以在任意位置插入字符 '(' 和 ')' 使字符串平衡。</p>
<p>请你返回让 s 平衡的最少插入次数。</p>
<h2 id="示例">示例</h2>
<figure class="highlight 1c"><table><tr><td class="code"><pre><span class="line">输入：s = <span class="string">&quot;(()))&quot;</span></span><br><span class="line">输出：<span class="number">1</span></span><br><span class="line">解释：第二个左括号有与之匹配的两个右括号，但是第一个左括号只有一个右括号。</span><br><span class="line">我们需要在字符串结尾额外增加一个 &#x27;)&#x27; 使字符串变成平衡字符串 <span class="string">&quot;(())))&quot;</span> 。</span><br></pre></td></tr></table></figure>
<h2 id="分析">分析</h2>
<ul>
<li><p>当前步如果是左括号，则左边的问题结束(因为没有连续右括号了)，默认左边问题已经解决，只需要记住当前步多了一个左括号（可以放到栈里也可以计数）。</p></li>
<li><p>当前步如果是右括号，</p>
<ul>
<li>如果下一步也是右括号，两个并成一个 "))"，如果Left里有左括号就pop出来一个，和现在的 "))"消去；如果目前没有多余的左括号，就把 "))" 存到 Right2 栈里 (计数也行)</li>
<li>如果下一步是个左括号，那这一步的 ')' 就需要单独处理：
<ul>
<li>当前Left栈里有左括号，直接在这一步插入一个 ')' 消去一组 "())", 这种借来的 ')'先计入rightDemand中</li>
<li>2.2 当前Left栈空，就在这个')'左右加 '(' 和 ')'，leftDemand 和 rightDemand 各计一个</li>
</ul></li>
</ul></li>
<li><p>最后多余的每一个 '(' 需要给rightDemand加两个 ')', 每一个"))" 需要给leftDemand加一个'('</p></li>
</ul>
<h2 id="代码">代码</h2>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Stack;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LC1541</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">minInsertions</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">        Stack&lt;String&gt; Left = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">        Stack&lt;String&gt; Right2 = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">        <span class="keyword">int</span> rightDemand = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> leftDemand = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (i &lt; s.length()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (s.charAt(i) == <span class="string">&#x27;(&#x27;</span>) &#123;</span><br><span class="line">                Left.push(<span class="string">&quot;(&quot;</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (i + <span class="number">1</span> &lt; s.length() &amp;&amp; s.charAt(i + <span class="number">1</span>) == <span class="string">&#x27;)&#x27;</span>) &#123; <span class="comment">// try to combine &quot;))&quot;</span></span><br><span class="line">                    i += <span class="number">1</span>; <span class="comment">// skip 1</span></span><br><span class="line">                    <span class="keyword">if</span> (!Left.empty()) &#123;</span><br><span class="line">                        Left.pop();</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        Right2.push(<span class="string">&quot;))&quot;</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (!Left.empty()) &#123;</span><br><span class="line">                        <span class="comment">// Add &quot;)&quot; to the right immediately and eliminate one left.</span></span><br><span class="line">                        Left.pop();</span><br><span class="line">                        rightDemand++;</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="comment">// only one single &#x27;)&#x27;, immediately eliminate it</span></span><br><span class="line">                        leftDemand += <span class="number">1</span>;</span><br><span class="line">                        rightDemand += <span class="number">1</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            i++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        rightDemand += <span class="number">2</span> * Left.size();</span><br><span class="line">        leftDemand += Right2.size();</span><br><span class="line">        <span class="keyword">return</span> rightDemand + leftDemand;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// String str = &quot;))())(&quot;;</span></span><br><span class="line">        String str = <span class="string">&quot;(()))&quot;</span>;</span><br><span class="line">        System.out.println(minInsertions(str));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Java数据结构与算法</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>Modern Portfolio Theory / Mean-variance analysis / Efficient Frontier</title>
    <url>/2021/02/02/Modern-Portfolio-Theory-Mean-variance-analysis-Efficient-Frontier/</url>
    <content><![CDATA[<h1 id="code-repo">Code Repo</h1>
<p>https://github.com/hy2632/Efficient-Frontier</p>
<h2 id="references">References</h2>
<ul>
<li><p><a href="https://en.wikipedia.org/wiki/Modern_portfolio_theory">Wikipedia - Modern portfolio theory - Mathematical Model</a></p></li>
<li><p><a href="https://mp.weixin.qq.com/s/0RlN0UTh3slFgErd8XRGhQ">马科维茨投资模型</a></p></li>
<li><p><a href="http://www.columbia.edu/~mh2078/FoundationsFE.html#:~:text=It%20is%20a%20core%20course,and%20financial%20problems%20and%20products.">Foundations of Financial Engineering (Columbia University, Fall 2016)</a></p></li>
</ul>
<h2 id="brief">Brief</h2>
<p>Analysis on the modern portfolio theory.</p>
<ul>
<li>Efficient frontier with / without risk free asset</li>
<li>Optimal sharpe ratio / portfolio</li>
</ul>
<h2 id="example-of-usage">Example of usage</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">instance = EfficientFrontier(</span><br><span class="line">    symbols=[<span class="string">&quot;AAPL&quot;</span>, <span class="string">&quot;XOM&quot;</span>, <span class="string">&quot;PFE&quot;</span>, <span class="string">&quot;F&quot;</span>, <span class="string">&quot;WMT&quot;</span>, <span class="string">&quot;BA&quot;</span>, <span class="string">&quot;TSLA&quot;</span>, <span class="string">&quot;AMD&quot;</span>],</span><br><span class="line">    start_date=<span class="string">&quot;20200101&quot;</span>,</span><br><span class="line">    end_date=<span class="string">&quot;20210130&quot;</span>, </span><br><span class="line">    simulation_times=<span class="number">5000</span>, </span><br><span class="line">    solve_granularity=<span class="number">1000</span>, </span><br><span class="line">    analytical_solution=<span class="literal">True</span>,</span><br><span class="line">    use_optimizer=<span class="literal">True</span>,</span><br><span class="line">    tangency_line=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line">instance.plot((<span class="number">12</span>,<span class="number">8</span>))</span><br></pre></td></tr></table></figure>
<p><img src="/2021/02/02/Modern-Portfolio-Theory-Mean-variance-analysis-Efficient-Frontier/figure.png" /></p>
<h2 id="content">Content</h2>
<ul>
<li>Monte Carlo simulation of portfolios consisting of assets in the designated pool.</li>
<li>Solve for the curve of the efficient frontier <strong>without risk-free asset</strong> analytically (a constrained optimization problem) using lagrange multiplier. This solution did not take into consideration the constraint that all weights are greater than 0, a.k.a. it allows short and leverage.</li>
</ul>
<!-- Rendering provided by http://www.sciweavers.org/free-online-latex-equation-editor -->
<p><span class="math display">\[
\begin{bmatrix}2\Sigma &amp;-R &amp; -{\bf1}\\ R^T &amp;0 &amp; 0 \\ {\bf1}^T &amp;0 &amp;0 \end{bmatrix} * \begin{bmatrix}w\\\lambda_1\\\lambda_2\end{bmatrix} 
= \begin{bmatrix}0\\\mu \\ 1\end{bmatrix}\]</span></p>
<ul>
<li><p>(2021/1/30 Update) <code>optimizerSolver()</code> to solve for the efficient frontier using scipy optimizer. Added the constraint of no short. Due to the limit of the optimizer, this solution performs bad for low <code>mu</code> part.</p></li>
<li><p>(2021/2/1 Update) <code>tangencySolver()</code> which solves for the "with risk-free asset" case. The solution can be proved to be the tangency line of the efficient frontier curve in the "no risk-free asset" setting.</p></li>
<li><p>Plotting the figure of all above.</p></li>
</ul>
<h3 id="frontier-with-risk-free-asset-the-tangency-line">2021/2/1: Frontier with risk-free asset —— the tangency line</h3>
<p>Consider the constrained problem that</p>
<p><span class="math display">\[ \min{\frac12 w^T\Sigma w} \]</span> <span class="math display">\[ \text{s.t. } (1 - \sum_{i=1}^n{w_i})r_f + w^TR = \mu\\
\]</span></p>
<p>Lagrange multiplier:</p>
<p><span class="math display">\[F = \frac12 w^T\Sigma w - \lambda[w^T(R - r_f {\bf1}) - (\mu - r_f)] \]</span></p>
<p><span class="math display">\[\frac{\partial F}{\partial w} = \Sigma w - \lambda(R - r_f {\bf1}) = 0 \\
\to w = \lambda \Sigma^{-1} (R - r_f {\bf1})\]</span></p>
<p><span class="math display">\[ (1 - \sum_{i=1}^n{w_i})r_f + w^TR = \mu \\
\to (R - r_f {\bf1})^T w = \mu - r_f \]</span></p>
<p>Solution:</p>
<p><span class="math display">\[\therefore \lambda = \frac{\mu - r_f}{(R - r_f {\bf1})^T\Sigma^{-1}(R - r_f {\bf1})} \]</span> <span class="math display">\[w = \lambda\Sigma^{-1} (R - r_f {\bf1}) = \frac{(\mu - r_f)\Sigma^{-1} (R - r_f {\bf1})}{(R - r_f {\bf1})^T\Sigma^{-1}(R - r_f {\bf1})}\]</span></p>
<p>It can be proved that for any combination of risk-free asset and any risky asset, <span class="math inline">\(\mu / \sigma \propto \alpha\)</span>. Therefore, if we got an optimal efficient risky portfolio without risk-free asset, the efficient frontier with risk-free asset should be the tangency line which crosses that point.</p>
]]></content>
  </entry>
  <entry>
    <title>Machine Learning: A Probabilistic Perspective, Kevin P. Murphy</title>
    <url>/2020/11/27/Machine-Learning-A-Probabilistic-Perspective-Kevin-P-Murphy/</url>
    <content><![CDATA[<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">Machine</span> <span class="string">Learning</span></span><br><span class="line"><span class="attr">A</span> <span class="string">Probabilistic Perspective</span></span><br><span class="line"></span><br><span class="line"><span class="attr">Kevin</span> <span class="string">P. Murphy</span></span><br><span class="line"></span><br><span class="line"><span class="attr">The</span> <span class="string">MIT Press</span></span><br></pre></td></tr></table></figure>
<div class="pdfobject-container" data-target="./Machine_Learning_A_Probabilistic_Perspective.pdf" data-height="1000px"></div>
]]></content>
      <categories>
        <category>Textbooks</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>LaTex 速查表</title>
    <url>/2020/10/08/LaTex-%E9%80%9F%E6%9F%A5%E8%A1%A8/</url>
    <content><![CDATA[<p>一些常用LaTex的命令。</p>
<div class="pdfobject-container" data-target="./Latex.pdf" data-height="1000px"></div>
]]></content>
      <categories>
        <category>misc</category>
      </categories>
  </entry>
  <entry>
    <title>MNIST Recognition</title>
    <url>/2020/11/15/MNIST-Recognition/</url>
    <content><![CDATA[<h2 id="reference">Reference</h2>
<p>https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627</p>
<h1 id="import">Import</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">from</span> tqdm.notebook <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%cd /content/drive/My Drive/<span class="number">20</span>FA/DataMining/DigitRecog</span><br></pre></td></tr></table></figure>
<pre><code>/content/drive/.shortcut-targets-by-id/107/20FA/DataMining/DigitRecog</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,),(<span class="number">0.5</span>)),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainset = datasets.MNIST(<span class="string">&#x27;PATH_TO_STORE_TRAINSET&#x27;</span>, download=<span class="literal">True</span>, train=<span class="literal">True</span>, transform=transform)</span><br><span class="line">valset = datasets.MNIST(<span class="string">&#x27;PATH_TO_STORE_TESTSET&#x27;</span>, download=<span class="literal">True</span>, train=<span class="literal">False</span>, transform=transform)</span><br><span class="line"></span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">valloader = torch.utils.data.DataLoader(valset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line">print(images.shape)</span><br><span class="line">print(labels.shape)</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([64, 1, 28, 28])
torch.Size([64])</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.imshow(images[<span class="number">0</span>].squeeze(), cmap=<span class="string">&quot;gray_r&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/11/15/MNIST-Recognition/mnist_7_0.png" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">figure = plt.figure()</span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">61</span>):</span><br><span class="line">    plt.subplot(<span class="number">6</span>, <span class="number">10</span>, index)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.imshow(images[index].squeeze(), cmap=<span class="string">&#x27;gray_r&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/11/15/MNIST-Recognition/mnist_8_0.png" /></p>
<h1 id="neural-network-1">Neural Network 1</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2 CNN layers</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Conv2d(</span><br><span class="line">        in_channels=<span class="number">1</span>, </span><br><span class="line">        out_channels=<span class="number">1</span>,</span><br><span class="line">        kernel_size=(<span class="number">3</span>,<span class="number">3</span>),</span><br><span class="line">        stride=<span class="number">1</span>,</span><br><span class="line">        padding=<span class="number">1</span>,</span><br><span class="line">    ),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(</span><br><span class="line">        in_channels=<span class="number">1</span>, </span><br><span class="line">        out_channels=<span class="number">1</span>,</span><br><span class="line">        kernel_size=(<span class="number">3</span>,<span class="number">3</span>),</span><br><span class="line">        stride=<span class="number">2</span>,</span><br><span class="line">        padding=<span class="number">1</span>,</span><br><span class="line">    ),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Flatten(<span class="number">1</span>, <span class="number">-1</span>),</span><br><span class="line">    nn.Linear(in_features=<span class="number">196</span>, out_features=<span class="number">10</span>),</span><br><span class="line">    nn.LogSoftmax(dim=<span class="number">1</span>),</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">criterion = nn.NLLLoss()</span><br><span class="line">images, labels = next(iter(trainloader))</span><br><span class="line"></span><br><span class="line">logps=model(images)</span><br><span class="line">loss=criterion(logps, labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">&#x27;Before backward pass: \n&#x27;</span>, model[<span class="number">0</span>].weight.grad)</span><br><span class="line">loss.backward()</span><br><span class="line">print(<span class="string">&#x27;After backward pass: \n&#x27;</span>, model[<span class="number">0</span>].weight.grad)</span><br></pre></td></tr></table></figure>
<pre><code>Before backward pass: 
 None
After backward pass: 
 tensor([[[[ 0.0013, -0.0030, -0.0132],
          [-0.0029, -0.0017, -0.0082],
          [ 0.0073,  0.0069, -0.0052]]]])</code></pre>
<h2 id="training">Training</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Put the parameters of model into optimizer</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">time0 = time()</span><br><span class="line">epochs = <span class="number">15</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> tqdm(range(epochs)):</span><br><span class="line">    running_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> trainloader:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        output = model(images)</span><br><span class="line">        loss = criterion(output, labels)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">&quot;Epoch &#123;&#125; - Training loss: &#123;&#125;&quot;</span>.format(e, running_loss/len(trainloader)))</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;\nTraining Time (in minutes) =&quot;</span>,(time()-time0)/<span class="number">60</span>)</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value=&#39;&#39;)))


Epoch 0 - Training loss: 0.397744912908339
Epoch 1 - Training loss: 0.35094216181588833
Epoch 2 - Training loss: 0.328661654382817
Epoch 3 - Training loss: 0.3178785914328815
Epoch 4 - Training loss: 0.3090840268220856
Epoch 5 - Training loss: 0.3036002234291674
Epoch 6 - Training loss: 0.2991932151255323
Epoch 7 - Training loss: 0.29626753309499354
Epoch 8 - Training loss: 0.28997316002559814
Epoch 9 - Training loss: 0.28794241896760997
Epoch 10 - Training loss: 0.28507200847747227
Epoch 11 - Training loss: 0.28353214655508363
Epoch 12 - Training loss: 0.2812810523955743
Epoch 13 - Training loss: 0.2819290509594402
Epoch 14 - Training loss: 0.27882546477957065


Training Time (in minutes) = 4.208930110931396</code></pre>
<h2 id="validation">Validation</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">images, labels = next(iter(valloader))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    logps = model(images)</span><br><span class="line"></span><br><span class="line">ps = torch.exp(logps)</span><br><span class="line">probab = list(ps.numpy()[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">&quot;Predicted Digit =&quot;</span>, probab.index(max(probab)))</span><br><span class="line"></span><br><span class="line">plt.imshow(images[<span class="number">0</span>].squeeze(dim=<span class="number">0</span>), cmap=<span class="string">&#x27;gray_r&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Predicted Digit = 2

&lt;matplotlib.image.AxesImage at 0x7f0f21a086a0&gt;</code></pre>
<p><img src="/2020/11/15/MNIST-Recognition/mnist_16_2.png" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct_count, all_count = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> images,labels <span class="keyword">in</span> tqdm(valloader):</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        logps = model(images)</span><br><span class="line"></span><br><span class="line">    pred_label = logps.argmax(dim=<span class="number">1</span>)</span><br><span class="line">    true_label = labels</span><br><span class="line">    correct_count += torch.sum(pred_label == true_label).item()</span><br><span class="line">    all_count += len(labels)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;Number Of Images Tested =&quot;</span>, all_count)</span><br><span class="line">print(<span class="string">&quot;\nModel Accuracy =&quot;</span>, (correct_count/all_count))</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(FloatProgress(value=0.0, max=157.0), HTML(value=&#39;&#39;)))



Number Of Images Tested = 10000

Model Accuracy = 0.9232</code></pre>
<h1 id="neural-network-2">Neural Network 2</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Baseline: 3 Linears</span></span><br><span class="line">input_size = <span class="number">784</span></span><br><span class="line">hidden_sizes = [<span class="number">128</span>, <span class="number">64</span>]</span><br><span class="line">output_size = <span class="number">10</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Flatten(<span class="number">1</span>, <span class="number">-1</span>),</span><br><span class="line">    nn.Linear(input_size, hidden_sizes[<span class="number">0</span>]),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(hidden_sizes[<span class="number">0</span>], hidden_sizes[<span class="number">1</span>]),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(hidden_sizes[<span class="number">1</span>], output_size),</span><br><span class="line">    nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="training-1">Training</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Put the parameters of model into optimizer</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">time0 = time()</span><br><span class="line">epochs = <span class="number">15</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> tqdm(range(epochs)):</span><br><span class="line">    running_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> tqdm(trainloader):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        output = model(images)</span><br><span class="line">        loss = criterion(output, labels)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">&quot;Epoch &#123;&#125; - Training loss: &#123;&#125;&quot;</span>.format(e, running_loss/len(trainloader)))</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;\nTraining Time (in minutes) =&quot;</span>,(time()-time0)/<span class="number">60</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 14 - Training loss: 0.11990390112822148


Training Time (in minutes) = 3.121389027436574</code></pre>
<h2 id="validation-1">Validation</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">images, labels = next(iter(valloader))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    logps = model(images)</span><br><span class="line"></span><br><span class="line">ps = torch.exp(logps)</span><br><span class="line">probab = list(ps.numpy()[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">&quot;Predicted Digit =&quot;</span>, probab.index(max(probab)))</span><br><span class="line"></span><br><span class="line">plt.imshow(images[<span class="number">0</span>].squeeze(dim=<span class="number">0</span>), cmap=<span class="string">&#x27;gray_r&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Predicted Digit = 9

&lt;matplotlib.image.AxesImage at 0x7f0f212fa198&gt;</code></pre>
<p><img src="/2020/11/15/MNIST-Recognition/mnist_23_2.png" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct_count, all_count = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> images,labels <span class="keyword">in</span> tqdm(valloader):</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        logps = model(images)</span><br><span class="line"></span><br><span class="line">    pred_label = logps.argmax(dim=<span class="number">1</span>)</span><br><span class="line">    true_label = labels</span><br><span class="line">    correct_count += torch.sum(pred_label == true_label).item()</span><br><span class="line">    all_count += len(labels)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;Number Of Images Tested =&quot;</span>, all_count)</span><br><span class="line">print(<span class="string">&quot;\nModel Accuracy =&quot;</span>, (correct_count/all_count))</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(FloatProgress(value=0.0, max=157.0), HTML(value=&#39;&#39;)))



Number Of Images Tested = 10000

Model Accuracy = 0.9619</code></pre>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title>Hexo+GitHubio</title>
    <url>/2020/08/20/My-New-Post/</url>
    <content><![CDATA[<h2 id="weblink">Weblink:</h2>
<p>知乎专栏<a href="https://zhuanlan.zhihu.com/p/26625249" class="uri">https://zhuanlan.zhihu.com/p/26625249</a>; 大卫博客<a href="https://univeryinli.github.io/about" class="uri">https://univeryinli.github.io/about</a></p>
<h3 id="安装">安装</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo -g <span class="comment">#安装Hexo</span></span><br><span class="line">npm update hexo -g <span class="comment">#升级</span></span><br><span class="line">hexo init <span class="comment">#初始化博客</span></span><br></pre></td></tr></table></figure>
<h3 id="命令简写">命令简写</h3>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">hexo n <span class="string">&quot;我的博客&quot;</span> == hexo new <span class="string">&quot;我的博客&quot;</span> #新建文章</span><br><span class="line">hexo g == hexo generate #生成</span><br><span class="line">hexo s == hexo<span class="built_in"> server </span>#启动服务预览</span><br><span class="line">hexo d == hexo deploy #部署</span><br><span class="line"></span><br><span class="line">hexo<span class="built_in"> server </span>#Hexo会监视文件变动并自动更新，无须重启服务器</span><br><span class="line">hexo<span class="built_in"> server </span>-s #静态模式</span><br><span class="line">hexo<span class="built_in"> server </span>-p 5000 #更改端口</span><br><span class="line">hexo<span class="built_in"> server </span>-i 192.168.1.1 #自定义 IP</span><br><span class="line">hexo clean #清除缓存，若是网页正常情况下可以忽略这条命令</span><br><span class="line"></span><br><span class="line">刚刚的三个命令依次是新建一篇博客文章、生成网页、在本地预览的操作</span><br></pre></td></tr></table></figure>
<h3 id="推送网站"><strong>推送网站</strong></h3>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">vim</span> <span class="string">_config.yml;</span></span><br><span class="line">    <span class="attr">deploy</span>:<span class="string"></span></span><br><span class="line"> <span class="attr">type</span>: <span class="string">git</span></span><br><span class="line">   <span class="attr">repo</span>: <span class="string">这里填入你之前在GitHub上创建仓库的完整路径，记得加上 .git</span></span><br><span class="line">    <span class="attr">branch</span>: <span class="string">master</span></span><br><span class="line"></span><br><span class="line"><span class="attr">npm</span> <span class="string">install hexo-deployer-git --save</span></span><br><span class="line"><span class="attr">hexo</span> <span class="string">clean </span></span><br><span class="line"><span class="attr">hexo</span> <span class="string">g </span></span><br><span class="line"><span class="attr">hexo</span> <span class="string">d</span></span><br></pre></td></tr></table></figure>
<h3 id="next-主题">next 主题</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/theme-next/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure>
<h3 id="配置next">配置next</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> themes/next</span><br><span class="line">vim _config.yml</span><br></pre></td></tr></table></figure>
<h4 id="新建about页">新建about页</h4>
<p>编辑<code>source/about/index.md</code>, 进入<code>themes/next/_config.yml</code>, <figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">menu:</span></span><br><span class="line">  <span class="attr">home:</span> <span class="string">/</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-home</span></span><br><span class="line">  <span class="attr">about:</span> <span class="string">/about</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-user</span></span><br></pre></td></tr></table></figure> 再生成部署。</p>
]]></content>
  </entry>
  <entry>
    <title>Uber盈利和经营策略研究</title>
    <url>/2020/08/22/ResearchOnUber/</url>
    <content><![CDATA[<h3 id="提要">提要</h3>
<p>这是 DROM8110 Business Analytics Strategy 的最终报告，运用了 Contributed Value Analysis / Sustainability Analysis 等框架，分析了 Uber 的主营业务——叫车(Ride-Hailing) 的商业模型、盈利前景和可行策略。</p>
<h3 id="其他作者">其他作者:</h3>
<ul>
<li>RUOMING GU (rg3266@columbia.edu)</li>
<li>YUXIN ZHANG (yz3718@columbia.edu)</li>
</ul>
<h3 id="正文">正文:</h3>
<div class="pdfobject-container" data-target="./BAS_FINAL_YAO.pdf" data-height="1000px"></div>
]]></content>
      <categories>
        <category>报告</category>
      </categories>
      <tags>
        <tag>Business Analytics Strategy</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Tutorial -- Wordcount in Spark</title>
    <url>/2020/09/16/Spark-Tutorial-Wordcount-in-Spark/</url>
    <content><![CDATA[<h2 id="cs246---colab-1-wordcount-in-spark">CS246 - Colab 1 Wordcount in Spark</h2>
<div class="pdfobject-container" data-target="./CS246Colab1.pdf" data-height="1000px"></div>
]]></content>
  </entry>
  <entry>
    <title>Systematic Trading: Early Profit/Loss Taker?</title>
    <url>/2020/12/26/Systematic-Trading-Early-Profit-Loss-Taker/</url>
    <content><![CDATA[<h2 id="preface">Preface</h2>
<p>In Robert Carver's book "Systematic Trading", he compared the two mindsets of trading: "Early Profit Taker" and "Early Loss Taker". The previous one is our mankind's "flawed instinction" and the latter one is believed to outperform the previous one.</p>
<p>This notebook implements the argument and verifies it through different examples.</p>
<p>Some of the parameters in the method is at your own discretion. Stocks and futures might take values to different orders of magnitude w.r.t. the "tolerance_lo", let alone the fact that everyone has his own extent of tolerance.</p>
<h2 id="import-packages-and-load-datasets">Import packages and load datasets</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%cd /content/drive/MyDrive/Quant</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas_datareader <span class="keyword">as</span> pdr</span><br><span class="line"><span class="keyword">from</span> pandas_datareader <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> pdb</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">BABA = data.get_data_yahoo(<span class="string">&quot;BABA&quot;</span>, <span class="string">&quot;20151201&quot;</span>, <span class="string">&quot;20201201&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Download US 10 Year Treasury Futures dataset from https://www.macrotrends.net/futures/10-year-treasury</span></span><br><span class="line">USTTEN_Future = pd.read_csv(<span class="string">&quot;10-year-treasury-futures.csv&quot;</span>, skiprows=<span class="number">16</span>)</span><br><span class="line">USTTEN_Future.rename(columns=&#123;<span class="string">&quot; value&quot;</span>:<span class="string">&quot;Adj Close&quot;</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line">USTTEN_Future[<span class="string">&quot;date&quot;</span>] = USTTEN_Future[<span class="string">&quot;date&quot;</span>].astype(<span class="string">&quot;datetime64&quot;</span>)</span><br><span class="line">USTTEN_Future_slice = USTTEN_Future[(USTTEN_Future[<span class="string">&quot;date&quot;</span>] &gt; <span class="string">&quot;2011-04-30&quot;</span>) &amp; (USTTEN_Future[<span class="string">&quot;date&quot;</span>] &lt; <span class="string">&quot;2012-03-01&quot;</span>)]</span><br><span class="line"></span><br><span class="line">USTTEN_Future_slice.set_index(<span class="string">&quot;date&quot;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">USTTEN_Future_slice</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Adj Close
</th>
</tr>
<tr>
<th>
date
</th>
<th>
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
2011-05-02
</th>
<td>
121.2344
</td>
</tr>
<tr>
<th>
2011-05-03
</th>
<td>
121.4375
</td>
</tr>
<tr>
<th>
2011-05-04
</th>
<td>
121.6406
</td>
</tr>
<tr>
<th>
2011-05-05
</th>
<td>
122.0625
</td>
</tr>
<tr>
<th>
2011-05-06
</th>
<td>
122.2813
</td>
</tr>
<tr>
<th>
...
</th>
<td>
...
</td>
</tr>
<tr>
<th>
2012-02-23
</th>
<td>
131.2031
</td>
</tr>
<tr>
<th>
2012-02-24
</th>
<td>
131.1344
</td>
</tr>
<tr>
<th>
2012-02-27
</th>
<td>
131.5125
</td>
</tr>
<tr>
<th>
2012-02-28
</th>
<td>
131.5031
</td>
</tr>
<tr>
<th>
2012-02-29
</th>
<td>
131.0156
</td>
</tr>
</tbody>
</table>
<p>
210 rows × 1 columns
</p>
</div>
<h2 id="modeling-2-strategies">Modeling 2 strategies</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">strategy_modeling</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        principal = <span class="number">1e5</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        strategy = <span class="string">&#x27;Early_Profit_Taker&#x27;</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        tol_hi = <span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        tol_lo = <span class="number">0.05</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        position_hi = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        position_lo = <span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        data = BABA,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        This module compare two different transaction strategies: &quot;Early Profit Taker&quot; and &quot;Early Loss Taker&quot;, mentioned by Robert Carver</span></span><br><span class="line"><span class="string">        in his book Systematic Trading. The previous one is mankind&#x27;s instinction, while the latter one usually outperforms.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        ## Author</span></span><br><span class="line"><span class="string">        Hua Yao (hy2632@columbia.edu)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        ## Reference:</span></span><br><span class="line"><span class="string">        Robert Carver, Systematic Trading: A unique new method for designing trading and investing systems, Page 15.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">                principal: int, your principal</span></span><br><span class="line"><span class="string">                strategy: str, &#x27;Early_Profit_Taker&#x27; / &#x27;Early_Loss_Taker&#x27;</span></span><br><span class="line"><span class="string">                tol_hi: the higher threshold of percentage change in price since last transaction</span></span><br><span class="line"><span class="string">                tol_lo: the lower...</span></span><br><span class="line"><span class="string">                position_hi: high position 高仓位</span></span><br><span class="line"><span class="string">                position_lo: low position 低仓位</span></span><br><span class="line"><span class="string">                data = BABA: pandas_datareader.data.get_data_yahoo style dataframe. Used &quot;Adj Close&quot; as the stock price.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    start_date = data.index[<span class="number">0</span>]</span><br><span class="line">    end_date = data.index[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    last_transaction = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    position = np.ones(len(data)) * position_hi</span><br><span class="line">    num_stocks = np.ones(len(data)) * principal * position_hi / data.iloc[<span class="number">0</span>][<span class="string">&quot;Adj Close&quot;</span>]</span><br><span class="line">    book_value = np.ones(len(data)) * principal</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> strategy == <span class="string">&#x27;Early_Profit_Taker&#x27;</span>:</span><br><span class="line">        tol_profit = tol_lo</span><br><span class="line">        tol_loss = tol_hi</span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">assert</span> strategy == <span class="string">&#x27;Early_Loss_Taker&#x27;</span></span><br><span class="line">        tol_profit = tol_hi</span><br><span class="line">        tol_loss = tol_lo</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(<span class="number">1</span>, len(data)):</span><br><span class="line">        book_value[idx] = book_value[idx - <span class="number">1</span>] + num_stocks[idx] * (data.iloc[idx][<span class="string">&quot;Adj Close&quot;</span>] / data.iloc[idx - <span class="number">1</span>][<span class="string">&quot;Adj Close&quot;</span>] - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> position[idx] == position_lo: <span class="comment"># currently in position_lo and last transaction was selling, now buy</span></span><br><span class="line">            <span class="keyword">if</span> data.iloc[idx][<span class="string">&quot;Adj Close&quot;</span>] / data.iloc[last_transaction][<span class="string">&quot;Adj Close&quot;</span>] - <span class="number">1</span> &gt; tol_loss: <span class="comment"># the price increases over the tolerance of loss after selling</span></span><br><span class="line">                position[idx + <span class="number">1</span>:] = position_hi <span class="comment"># buy</span></span><br><span class="line">                num_stocks[idx + <span class="number">1</span>:] = (book_value[idx] * position_hi) / data.iloc[idx][<span class="string">&quot;Adj Close&quot;</span>] <span class="comment"># num_stocks decided at buying, high position</span></span><br><span class="line">                last_transaction = idx <span class="comment"># record</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># currently in position_hi and last transaction was buying</span></span><br><span class="line">            <span class="keyword">if</span> (data.iloc[idx][<span class="string">&quot;Adj Close&quot;</span>] / data.iloc[last_transaction][<span class="string">&quot;Adj Close&quot;</span>] - <span class="number">1</span> &gt; tol_profit) | \</span><br><span class="line">                (data.iloc[idx][<span class="string">&quot;Adj Close&quot;</span>] / data.iloc[last_transaction][<span class="string">&quot;Adj Close&quot;</span>] - <span class="number">1</span> &lt; - tol_loss): <span class="comment"># the price increases over the tolerance of profit or decreases over the tolerance of loss after buying</span></span><br><span class="line">                position[idx + <span class="number">1</span>:] = position_lo <span class="comment"># sell out to position_lo</span></span><br><span class="line">                num_stocks[idx + <span class="number">1</span>:] = (book_value[idx] * position_lo) / data.iloc[idx][<span class="string">&quot;Adj Close&quot;</span>] <span class="comment"># num_stocks decided at selling, low position</span></span><br><span class="line">                last_transaction = idx</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> position, num_stocks, book_value-principal</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="evaluate-the-performance">Evaluate the performance</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_strategy_comparison</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">    principal = <span class="number">1e5</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    tol_hi = <span class="number">0.3</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    tol_lo = <span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    position_hi = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    position_lo = <span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    data = BABA,</span></span></span><br><span class="line"><span class="function"><span class="params"></span>):</span></span><br><span class="line"></span><br><span class="line">    position, num_stocks, gain = strategy_modeling(</span><br><span class="line">                                            principal,</span><br><span class="line">                                            <span class="string">&#x27;Early_Profit_Taker&#x27;</span>,</span><br><span class="line">                                            tol_hi,</span><br><span class="line">                                            tol_lo,</span><br><span class="line">                                            position_hi,</span><br><span class="line">                                            position_lo,</span><br><span class="line">                                            data,</span><br><span class="line">                                        )</span><br><span class="line">    position_, num_stocks_, gain_ = strategy_modeling(</span><br><span class="line">                                            principal,</span><br><span class="line">                                            <span class="string">&#x27;Early_Loss_Taker&#x27;</span>,</span><br><span class="line">                                            tol_hi,</span><br><span class="line">                                            tol_lo,</span><br><span class="line">                                            position_hi,</span><br><span class="line">                                            position_lo,</span><br><span class="line">                                            data,</span><br><span class="line">                                        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># gridspec_kw: https://stackoverflow.com/questions/10388462/matplotlib-different-size-subplots</span></span><br><span class="line">    fig,ax = plt.subplots(<span class="number">3</span>, <span class="number">1</span>, sharex=<span class="literal">True</span>, figsize=(<span class="number">14</span>,<span class="number">12</span>), gridspec_kw=&#123;<span class="string">&#x27;height_ratios&#x27;</span>: [<span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>]&#125;)</span><br><span class="line">    ax[<span class="number">0</span>].plot(data.index, data[<span class="string">&quot;Adj Close&quot;</span>])</span><br><span class="line">    ax[<span class="number">0</span>].set_ylabel(<span class="string">&quot;Price&quot;</span>)</span><br><span class="line">    ax[<span class="number">1</span>].plot(data.index, position)</span><br><span class="line">    ax[<span class="number">1</span>].plot(data.index, position_)</span><br><span class="line">    ax[<span class="number">1</span>].set_ylim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    ax[<span class="number">1</span>].legend([<span class="string">&quot;Early_Profit_Taker&quot;</span>, <span class="string">&quot;Early_Loss_Taker&quot;</span>])</span><br><span class="line">    ax[<span class="number">1</span>].set_ylabel(<span class="string">&quot;Position&quot;</span>)</span><br><span class="line">    ax[<span class="number">2</span>].plot(data.index, gain)</span><br><span class="line">    ax[<span class="number">2</span>].plot(data.index, gain_)</span><br><span class="line">    ax[<span class="number">2</span>].legend([<span class="string">&quot;Early_Profit_Taker&quot;</span>, <span class="string">&quot;Early_Loss_Taker&quot;</span>])</span><br><span class="line">    ax[<span class="number">2</span>].set_ylabel(<span class="string">&quot;Profits&quot;</span>)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="example-the-us-ten-year-treasury-future-2011.05---2012.03">Example: The US Ten year treasury future (2011.05 - 2012.03)</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_strategy_comparison(</span><br><span class="line">    principal=<span class="number">1e5</span>,</span><br><span class="line">    tol_hi=<span class="number">0.055</span>,</span><br><span class="line">    tol_lo=<span class="number">0.015</span>,</span><br><span class="line">    position_hi=<span class="number">0.8</span>,</span><br><span class="line">    position_lo = <span class="number">0.2</span>,    </span><br><span class="line">    data=USTTEN_Future_slice,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/12/26/Systematic-Trading-Early-Profit-Loss-Taker/Early_Profit_Taker_10_0.png" /></p>
<h3 id="example-baba">Example: BABA</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_strategy_comparison(</span><br><span class="line">    principal=<span class="number">1e5</span>,</span><br><span class="line">    tol_hi=<span class="number">0.2</span>,</span><br><span class="line">    tol_lo=<span class="number">0.1</span>,</span><br><span class="line">    position_hi=<span class="number">0.8</span>,</span><br><span class="line">    position_lo = <span class="number">0.2</span>,    </span><br><span class="line">    data=BABA,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/12/26/Systematic-Trading-Early-Profit-Loss-Taker/Early_Profit_Taker_12_0.png" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_strategy_comparison(</span><br><span class="line">    principal=<span class="number">1e5</span>,</span><br><span class="line">    tol_hi=<span class="number">0.25</span>,</span><br><span class="line">    tol_lo=<span class="number">0.1</span>,</span><br><span class="line">    position_hi=<span class="number">0.95</span>,</span><br><span class="line">    position_lo = <span class="number">0.2</span>,    </span><br><span class="line">    data=data.get_data_yahoo(<span class="string">&quot;INTC&quot;</span>, <span class="string">&quot;20160101&quot;</span>, <span class="string">&quot;20201224&quot;</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/12/26/Systematic-Trading-Early-Profit-Loss-Taker/Early_Profit_Taker_13_0.png" /></p>
<h2 id="comment">Comment</h2>
<p>The values of <code>tol_hi</code> and <code>tol_lo</code> as well as the ratio between them, are hard to be determined, and collectively influences the payoff. The conditions where the argument holds remain unclear.</p>
]]></content>
      <categories>
        <category>Finance</category>
      </categories>
      <tags>
        <tag>Systematic Trading</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/1970/01/01/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>The Matrix Cookbook</title>
    <url>/2020/10/31/The-Matrix-Cookbook/</url>
    <content><![CDATA[<p>在学习cs229-notes11 时遇到了这样一个等式</p>
<p><span class="math display">\[\nabla_W|W| = |W|(W^{-1})^T \]</span></p>
<p>于是找到了这个 UCI 的“矩阵菜谱”。其中列举了与矩阵有关的公式，在此上传供以后查阅。</p>
<p>Credit to: <a href="https://www.ics.uci.edu/~welling/teaching/KernelsICS273B/MatrixCookBook.pdf">Max Welling's CS 273B: Kernel-Based Learning, 2005</a></p>
<p>(可见这是一个比较古早的研究kernel的课程)</p>
<div class="pdfobject-container" data-target="./MatrixCookBook.pdf" data-height="1000px"></div>
]]></content>
      <categories>
        <category>Textbooks</category>
      </categories>
      <tags>
        <tag>矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title>二分查找</title>
    <url>/2020/09/03/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/</url>
    <content><![CDATA[<h2 id="链接">链接</h2>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzUyNjQxNjYyMg==&amp;mid=2247485570&amp;idx=2&amp;sn=7a4e2635aafcf1c9bd01642f6cedd409&amp;chksm=fa0e6703cd79ee15f692f31c488aac6787a08ffea2ffe457a037310df355c1e6a810919333e4&amp;scene=21#wechat_redirect">二分查找算法详解</a></p>
<h2 id="代码">代码</h2>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">binarysearch</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] a = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>&#125;;</span><br><span class="line">        System.out.println(right_bound(a, <span class="number">2</span>));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 默认数组有序</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">binarySearch</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> target)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> right = arr.length - <span class="number">1</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> (left &lt;= right)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = (left + right) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span> (target == arr[mid])&#123;</span><br><span class="line">                <span class="keyword">return</span> mid;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (arr[mid] &lt; target)&#123;</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (arr[mid] &gt; target)&#123;</span><br><span class="line">                right = mid - <span class="number">1</span>; </span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 寻找左侧边界</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">left_bound</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> target)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = arr.length;</span><br><span class="line">        <span class="keyword">int</span> left, right;</span><br><span class="line">        left = <span class="number">0</span>;</span><br><span class="line">        right = n;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(left &lt; right)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = (left + right) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span> (arr[mid] == target)&#123;</span><br><span class="line">                right = mid;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (arr[mid] &gt; target)&#123;</span><br><span class="line">                right = mid;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (arr[mid] &lt; target)&#123;</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (left == arr.length) <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">return</span> (arr[left] == target) ? left : -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 寻找右侧边界</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">right_bound</span><span class="params">(<span class="keyword">int</span>[]arr, <span class="keyword">int</span> target)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = arr.length;</span><br><span class="line">        <span class="keyword">int</span> left, right;</span><br><span class="line">        left = <span class="number">0</span>;</span><br><span class="line">        right = n;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(left &lt; right)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = (left + right) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span> (arr[mid] == target)&#123;</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (arr[mid] &gt; target)&#123;</span><br><span class="line">                right = mid;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (arr[mid] &lt; target)&#123;</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (left == <span class="number">0</span>) <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">return</span> (arr[left - <span class="number">1</span>] == target) ? left - <span class="number">1</span> : -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="注意点">注意点</h3>
<ol type="1">
<li>确定左右边界时, 初始区间[0, arr.length)。</li>
<li>确定左边界时, 关键在于</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (arr[mid] == target)&#123;</span><br><span class="line">    right = mid;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这是由于每次mid的判断将<code>[Left, right)</code> 分成两部分, 这样可以逐次剥除最右的等于target的元素</p>
<ol start="3" type="1">
<li>确定右边界时,</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (arr[mid] == target)&#123;</span><br><span class="line">    left = mid + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>同样的，如果剥除最左的等于target的元素，由于区间左闭，需要去除mid。</p>
]]></content>
      <categories>
        <category>Java数据结构与算法</category>
      </categories>
  </entry>
  <entry>
    <title>XGBoost</title>
    <url>/2020/11/06/XGBoost/</url>
    <content><![CDATA[<h2 id="参考资料">参考资料</h2>
<p><a href="https://arxiv.org/pdf/1603.02754.pdf?__hstc=133736337.1bb630f9cde2cb5f07430159d50a3c91.1513641600097.1513641600098.1513641600099.1&amp;__hssc=133736337.1.1513641600100&amp;__hsfp=528229161">XGBoost: A Scalable Tree Boosting System</a></p>
<p><a href="https://www.bilibili.com/video/BV1si4y1G7Jb">贪心学院： XGBoost的技术剖析</a></p>
<p>非常好的教学视频，在此将内容整理如下。</p>
<h2 id="目标函数">目标函数</h2>
<p>Boosting是多个弱分类器叠加，每一棵树拟合前面所有树加和与目标值的残差（递归定义）。假设共K棵树，<span class="math inline">\(f_k \sim F\)</span></p>
<p><span class="math display">\[obj = \sum_{i=1}^{n}{l(y_i, \hat{y_i})} + \sum_{k=1}^{K}{\Omega(f_k)}\]</span></p>
<p>后一项为树的复杂度，需要参数化；前一项为损失函数累积，损失函数的形式可以是指数或Squared error等等。</p>
<p>假设已知树的形状，在训练第K棵树时，前面的 K-1 都已知。目标函数中去除掉先前累计的常量。</p>
<p><span class="math display">\[\begin{aligned}obj &amp;= \sum_{i=1}^{N}{l(y_i, \sum_{j=1}^{K-1}{f_{j}(x_i)}+f_{K}(x_i))} + \Omega(f_K)\\
&amp;= \sum_{i=1}^{N}{l(y_i, \hat{y_i}^{(k-1)}+f_{K}(x_i))} + \Omega(f_K)\end{aligned}\]</span></p>
<p>其中 <span class="math inline">\(\hat{y_i}^{(k-1)}\)</span> 表示累计到前 K-1 棵树时的估计值。复杂度只计第 K 棵树。可以发现，损失函数中包含了前面的累积量，导致不能进一步简化。使用二阶泰勒展开。</p>
<h2 id="泰勒近似">泰勒近似</h2>
<p><span class="math display">\[f(x+\Delta x) \approx f(x) + f&#39;(x) \Delta x + \frac{f&#39;&#39;(x)}{2} (\Delta x)^2\]</span></p>
<p>将 <span class="math inline">\(l(y_i, \hat{y_i}^{(k-1)}+f_{K}(x_i))\)</span> 看作关于估计量 <span class="math inline">\(\hat{y_i} ^{(k-1)}\)</span> 的函数，最后一棵树提供的 <span class="math inline">\(f_{K}(x_i)\)</span> 看作增量， 则</p>
<p><span class="math display">\[\begin{aligned}l(y_i, \hat{y_i}^{(k-1)}+f_{K}(x_i)) &amp;\approx l(y_i, \hat{y_i}^{(k-1)}) + l&#39;(y_i, \hat{y_i}^{(k-1)})f_{K}(x_i) + \frac{l&#39;&#39;(y_i, \hat{y_i}^{(k-1)})}{2}(f_{K}(x_i))^2\\
&amp;=l(y_i, \hat{y_i}^{(k-1)}) + g_if_{K}(x_i)+ h_if_{K}^2(x_i)\end{aligned}\]</span></p>
<p>其中 <span class="math inline">\(g_i, h_i\)</span> 分别是 <span class="math inline">\(l(y_i, \hat{y_i}^{(k-1)})\)</span> 对 <span class="math inline">\(\hat{y_i}^{(k-1)}\)</span> 的一阶二阶导，都是已知参数。</p>
<p>再对目标函数做简化，去掉常数项</p>
<p><span class="math display">\[\begin{aligned}
obj &amp;= \sum_{i=1}^{N}{l(y_i, \hat{y_i}^{(k-1)}+f_{K}(x_i))} + \Omega(f_K) \\
&amp;= \sum_{i=1}^{N}{\bigg(l(y_i, \hat{y_i}^{(k-1)}) + g_if_{K}(x_i)+ \frac{h_i}2f_{K}^2(x_i)\bigg)} + \Omega(f_K) \\
&amp;\to\ \sum_{i=1}^{N}{\bigg(g_if_{K}(x_i)+ \frac{h_i}2f_{K}^2(x_i)\bigg)} + \Omega(f_K)
\end{aligned}\]</span></p>
<h2 id="树的参数化">树的参数化</h2>
<p>目前为止对损失函数部分进行了简化，接下来需要对树的复杂度进行研究，需要将树参数化。</p>
<p>这里树的复杂度用两个指标来量化，一个是叶节点的个数 <span class="math inline">\(T\)</span>, 另一个是每个叶节点的值 <span class="math inline">\(w_i\)</span> 。</p>
<p>定义函数 <span class="math inline">\(q(x_i) = k\)</span>，表示样本所在叶节点的序号。</p>
<p>定义函数 <span class="math inline">\(I(j) = \{i| q(x_i) = j\}\)</span>, 表示第 j 个叶节点中所有样本的序号集合。</p>
<p>则树的复杂度定义为</p>
<p><span class="math display">\[\Omega(f_k) = \gamma T + \frac12 \lambda \sum_{j=1}^T{w_j^2}\]</span></p>
<p>目标函数进一步表示为</p>
<p><span class="math display">\[\begin{aligned}
obj &amp;= \sum_{i=1}^{N}{\bigg(g_if_{K}(x_i)+ \frac{h_i}2f_{K}^2(x_i)\bigg)} + \Omega(f_K) \\
&amp;=\sum_{j=1}^{T}{\bigg((\sum_{i\in I(j)}{g_i})w_j+ (\sum_{i\in I(j)}{\frac{h_i}2})w_j^2\bigg)} + \gamma T + \frac12 \lambda \sum_{j=1}^T{w_j^2}\\
&amp;=\sum_{j=1}^{T}{\bigg((\sum_{i\in I(j)}{g_i})w_j+ \frac 1  2(\lambda + \sum_{i\in I(j)}h_i)w_j^2 + \gamma   \bigg)}  
\end{aligned}\]</span></p>
<p>因而目标函数的每一项是关于叶节点值 <span class="math inline">\(w_j\)</span> 的二项式。整体目标函数优化问题就变成了<strong>对各叶节点的取值<span class="math inline">\(w_j\)</span>的优化问题。</strong></p>
<p>我们知道二项式的critical point <span class="math inline">\(-\frac{b}{2a}\)</span>。</p>
<p><span class="math display">\[w_j^* = -\frac{\sum_{i\in I(j)}{g_i}}{\lambda + \sum_{i\in I(j)}h_i} = -\frac{G_j}{H_j + \lambda}\]</span></p>
<p><span class="math display">\[obj^* = -\frac12 \sum_{j=1}^{T}{\frac{G_j^2}{H_j + \lambda}} + \gamma T\]</span></p>
<p>于是我们有了最优目标值的 closed-form solution。</p>
<h2 id="树的结构">树的结构</h2>
<p>先前求解目标函数的最优值，基于已知树的结构这么一个前提。在不知道树的结构时如何获得全局最优值？</p>
<p>暴力(brute force)解法是遍历各种树结构，求出各自的最优目标函数值，并取全局最优。但遍历树复杂度过高（二叉树的组合数：卡特兰数）。</p>
<p>如果把 <span class="math inline">\(obj^*\)</span> 作为 Split 的 criterion (代替 Information Gain 或 Gini Impurity)，就可以<strong>贪心</strong>求解。每次分裂节点，比较新的 <span class="math inline">\(obj^*\)</span> 和 之前的 <span class="math inline">\(obj^*\)</span>，最终选取的 split 使 <span class="math inline">\(obj\)</span> 下降最快。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title>python股票分析</title>
    <url>/2020/09/02/python%E8%82%A1%E7%A5%A8%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p><a href="https://www.bilibili.com/video/BV1NW411W7FK">【Python学习】股票分析系列（中文自制） by 大力牛肉粉</a></p>
<p><a href="http://data.eastmoney.com/zjlx/detail.html">东方财富网资金流向</a></p>
<p><a href="https://curl.trillworks.com/">Convert curl syntax to Python, Ansible URI, MATLAB, Node.js, R, PHP, Strest, Go, Dart, JSON, Elixir, Rust</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> datetime <span class="keyword">as</span> dt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas_datareader <span class="keyword">as</span> pdr <span class="comment"># 获取在线数据</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> style <span class="comment"># style</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="获取数据">获取数据</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">start = dt.datetime(<span class="number">2019</span>, <span class="number">8</span>, <span class="number">30</span>)</span><br><span class="line">end = dt.datetime(<span class="number">2020</span>, <span class="number">8</span>, <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">df = pdr.get_data_yahoo(<span class="string">&quot;TSLA&quot;</span>, start, end)</span><br><span class="line">df.to_csv(<span class="string">&quot;tsla.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">&quot;tsla.csv&quot;</span>, parse_dates=<span class="literal">True</span>, index_col=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">style.use(<span class="string">&quot;ggplot&quot;</span>)</span><br><span class="line"></span><br><span class="line">df.plot(figsize=(<span class="number">16</span>,<span class="number">10</span>),fontsize=<span class="number">16</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;AxesSubplot:xlabel=&#39;Date&#39;&gt;</code></pre>
<figure>
<img src="pyFinance_files/pyFinance_4_1.svg" alt="svg" /><figcaption aria-hidden="true">svg</figcaption>
</figure>
<h1 id="十日均线">十日均线</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">&quot;tsla.csv&quot;</span>, parse_dates=<span class="literal">True</span>, index_col=<span class="number">0</span>)</span><br><span class="line">df[<span class="string">&quot;10ma&quot;</span>] = df[<span class="string">&quot;Adj Close&quot;</span>].rolling(window=<span class="number">10</span>, min_periods=<span class="number">0</span>).mean()</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
High
</th>
<th>
Low
</th>
<th>
Open
</th>
<th>
Close
</th>
<th>
Volume
</th>
<th>
Adj Close
</th>
<th>
10ma
</th>
</tr>
<tr>
<th>
Date
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
2019-08-30
</th>
<td>
46.487999
</td>
<td>
44.841999
</td>
<td>
45.830002
</td>
<td>
45.122002
</td>
<td>
46603000.0
</td>
<td>
45.122002
</td>
<td>
45.122002
</td>
</tr>
<tr>
<th>
2019-09-03
</th>
<td>
45.790001
</td>
<td>
44.632000
</td>
<td>
44.816002
</td>
<td>
45.001999
</td>
<td>
26770500.0
</td>
<td>
45.001999
</td>
<td>
45.062000
</td>
</tr>
<tr>
<th>
2019-09-04
</th>
<td>
45.692001
</td>
<td>
43.841999
</td>
<td>
45.377998
</td>
<td>
44.136002
</td>
<td>
28805000.0
</td>
<td>
44.136002
</td>
<td>
44.753334
</td>
</tr>
<tr>
<th>
2019-09-05
</th>
<td>
45.959999
</td>
<td>
44.169998
</td>
<td>
44.500000
</td>
<td>
45.916000
</td>
<td>
36976500.0
</td>
<td>
45.916000
</td>
<td>
45.044001
</td>
</tr>
<tr>
<th>
2019-09-06
</th>
<td>
45.928001
</td>
<td>
45.034000
</td>
<td>
45.439999
</td>
<td>
45.490002
</td>
<td>
20947000.0
</td>
<td>
45.490002
</td>
<td>
45.133201
</td>
</tr>
</tbody>
</table>
</div>
<h1 id="ax和subplot">ax和subplot</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]</span><br><span class="line"><span class="comment"># SimHei：微软雅黑</span></span><br><span class="line"><span class="comment"># FangSong：仿宋</span></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line">ax.plot(df.index, df[<span class="string">&quot;Adj Close&quot;</span>])</span><br><span class="line">ax.plot(df.index, df[<span class="string">&quot;10ma&quot;</span>])</span><br><span class="line">ax.legend([<span class="string">&quot;Adj Close&quot;</span>, <span class="string">&quot;10ma&quot;</span>])</span><br><span class="line">ax.set_title(<span class="string">&quot;Adj_Close &amp; 10ma&quot;</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&quot;Date&quot;</span>, fontsize=<span class="number">18</span>, fontfamily = <span class="string">&#x27;DejaVu Sans&#x27;</span>, fontstyle=<span class="string">&quot;italic&quot;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&quot;$&quot;</span>, fontsize = <span class="string">&quot;x-large&quot;</span>, fontstyle=<span class="string">&quot;oblique&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ax.minorticks_on()</span></span><br><span class="line">ax.grid(which=<span class="string">&quot;minor&quot;</span>, axis=<span class="string">&quot;both&quot;</span>)</span><br><span class="line"><span class="comment"># ax.margins(0)</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure>
<img src="pyFinance_files/pyFinance_8_0.svg" alt="svg" /><figcaption aria-hidden="true">svg</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig, axs = plt.subplots(<span class="number">2</span>, figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line">ax = axs[<span class="number">0</span>]</span><br><span class="line">ax2 = axs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">ax.plot(df.index, df[<span class="string">&quot;Adj Close&quot;</span>])</span><br><span class="line">ax.plot(df.index, df[<span class="string">&quot;10ma&quot;</span>])</span><br><span class="line">ax.legend([<span class="string">&quot;Adj Close&quot;</span>, <span class="string">&quot;10ma&quot;</span>])</span><br><span class="line">ax.set_title(<span class="string">&quot;Adj_Close &amp; 10ma&quot;</span>)</span><br><span class="line"><span class="comment"># ax.set_xlabel(&quot;Date&quot;,  fontfamily = &#x27;DejaVu Sans&#x27;, fontstyle=&quot;italic&quot;)</span></span><br><span class="line">ax.set_ylabel(<span class="string">&quot;$&quot;</span>, fontstyle=<span class="string">&quot;oblique&quot;</span>)</span><br><span class="line"></span><br><span class="line">ax2.bar(df.index, df[<span class="string">&quot;Volume&quot;</span>])</span><br><span class="line">ax2.set_ylabel(<span class="string">&quot;Volume&quot;</span>, fontstyle=<span class="string">&quot;oblique&quot;</span>)</span><br><span class="line">style.use(<span class="string">&quot;ggplot&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure>
<img src="pyFinance_files/pyFinance_9_0.svg" alt="svg" /><figcaption aria-hidden="true">svg</figcaption>
</figure>
<h1 id="蜡烛图">蜡烛图</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.dates <span class="keyword">as</span> mdates</span><br><span class="line"><span class="keyword">from</span> mpl_finance <span class="keyword">import</span> candlestick_ohlc <span class="comment">#蜡烛图</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">&quot;tsla.csv&quot;</span>, parse_dates=<span class="literal">True</span>, index_col=<span class="number">0</span>)</span><br><span class="line">df_ohlc = df[<span class="string">&quot;Adj Close&quot;</span>].resample(<span class="string">&quot;10D&quot;</span>).ohlc() <span class="comment">#open high low close</span></span><br><span class="line">df_volume = df[<span class="string">&quot;Volume&quot;</span>].resample(<span class="string">&quot;10D&quot;</span>).sum()</span><br><span class="line">df_ohlc.reset_index(inplace=<span class="literal">True</span>)</span><br><span class="line">df_ohlc.Date = df_ohlc.Date.map(mdates.date2num)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">16</span>,<span class="number">10</span>))</span><br><span class="line">ax1 = plt.subplot2grid((<span class="number">6</span>,<span class="number">1</span>), (<span class="number">0</span>,<span class="number">0</span>), rowspan=<span class="number">5</span>, colspan=<span class="number">1</span>)</span><br><span class="line">ax2 = plt.subplot2grid((<span class="number">6</span>,<span class="number">1</span>), (<span class="number">5</span>,<span class="number">0</span>), rowspan=<span class="number">1</span>, colspan=<span class="number">1</span>, sharex = ax1)</span><br><span class="line"></span><br><span class="line">ax1.xaxis_date()</span><br><span class="line">candlestick_ohlc(ax1, df_ohlc.values, width=<span class="number">5</span>, colorup=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">ax2.fill_between(df_volume.index.map(mdates.date2num), df_volume.values, <span class="number">0</span>)</span><br><span class="line">ax2.plot(df_volume)</span><br><span class="line"></span><br><span class="line">style.use(<span class="string">&quot;ggplot&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure>
<img src="pyFinance_files/pyFinance_13_0.svg" alt="svg" /><figcaption aria-hidden="true">svg</figcaption>
</figure>
<h1 id="标普500公司简称获取爬虫">标普500公司简称获取(爬虫)</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup <span class="keyword">as</span> bs <span class="comment"># 字典</span></span><br><span class="line"><span class="keyword">import</span> pickle <span class="comment"># 列表，序列化</span></span><br><span class="line"><span class="keyword">import</span> requests <span class="comment"># 网络请求</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_sp500_tickers</span>():</span></span><br><span class="line">    resp = requests.get(<span class="string">&quot;https://en.wikipedia.org/wiki/List_of_S%26P_500_companies&quot;</span>)</span><br><span class="line">    soup = bs(resp.text, <span class="string">&quot;lxml&quot;</span>)</span><br><span class="line">    table = soup.find(<span class="string">&quot;table&quot;</span>, &#123;<span class="string">&quot;class&quot;</span>:<span class="string">&quot;wikitable sortable&quot;</span>&#125;)</span><br><span class="line">    tickers = []</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> table.findAll(<span class="string">&#x27;tr&#x27;</span>)[<span class="number">1</span>:]:</span><br><span class="line">        ticker = row.findAll(<span class="string">&#x27;td&#x27;</span>)[<span class="number">0</span>].text.replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">        tickers.append(ticker)</span><br><span class="line">    <span class="comment"># print(tickers)</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">&quot;sp500tickers.pickle&quot;</span>,<span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f: <span class="comment"># 二进制只读</span></span><br><span class="line">        pickle.dump(tickers, f)</span><br><span class="line">    <span class="keyword">return</span> tickers</span><br><span class="line">        </span><br><span class="line"><span class="comment"># save_sp500_tickers()</span></span><br></pre></td></tr></table></figure>
<h1 id="下载sp500股票数据">下载SP500股票数据</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os <span class="comment"># 系统接口</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> pandas_datareader <span class="keyword">as</span> pdr</span><br><span class="line"><span class="keyword">import</span> datetime <span class="keyword">as</span> dt</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_from_yahoo</span>(<span class="params">isReload=False</span>):</span></span><br><span class="line">    <span class="keyword">if</span> isReload:</span><br><span class="line">        tickers = save_sp500_tickers()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">&quot;sp500tickers.pickle&quot;</span>,<span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            tickers = pickle.load(f)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&quot;stock_dfs&quot;</span>): <span class="comment">#当前目录无该子目录</span></span><br><span class="line">        os.makedirs(<span class="string">&quot;stock_dfs&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    start = dt.datetime(<span class="number">2016</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    end = dt.datetime(<span class="number">2019</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ticker <span class="keyword">in</span> tickers:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&quot;stock_dfs/&#123;&#125;.csv&quot;</span>.format(ticker)):</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                df = pdr.get_data_yahoo(ticker, start, end)</span><br><span class="line">                df.to_csv(<span class="string">&quot;stock_dfs/&#123;&#125;.csv&quot;</span>.format(ticker))</span><br><span class="line">                print(<span class="string">&quot;Download complete: &#123;&#125;&quot;</span>.format(ticker))</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">&quot;Already exist: &#123;&#125;&quot;</span>.format(ticker))</span><br><span class="line">    </span><br><span class="line">get_data_from_yahoo(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h1 id="整合数据">整合数据</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compile_data</span>():</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">&quot;sp500tickers.pickle&quot;</span>,<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        tickers = pickle.load(f)[:<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line">    main_df = pd.DataFrame()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compile</span></span><br><span class="line">    <span class="keyword">for</span> count, ticker <span class="keyword">in</span> enumerate(tickers):</span><br><span class="line">        df = pd.read_csv(<span class="string">&quot;stock_dfs/&#123;&#125;.csv&quot;</span>.format(ticker))</span><br><span class="line">        df.set_index(<span class="string">&quot;Date&quot;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">        df.rename(columns = &#123;<span class="string">&quot;Adj Close&quot;</span>: ticker&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line">        df.drop([<span class="string">&quot;Open&quot;</span>, <span class="string">&quot;High&quot;</span>, <span class="string">&quot;Low&quot;</span>, <span class="string">&quot;Close&quot;</span>, <span class="string">&quot;Volume&quot;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> main_df.empty:</span><br><span class="line">            main_df = df</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            main_df = main_df.join(df, how=<span class="string">&quot;left&quot;</span>)</span><br><span class="line">        print(count)</span><br><span class="line">        <span class="keyword">if</span> count == <span class="number">10</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    main_df.to_csv(<span class="string">&quot;sp500_joined.csv&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> main_df</span><br><span class="line"></span><br><span class="line">main_df = compile_data()</span><br></pre></td></tr></table></figure>
<pre><code>0
1
2
3
4
5
6
7
8
9</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">main_df</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
MMM
</th>
<th>
ABT
</th>
<th>
ABBV
</th>
<th>
ABMD
</th>
<th>
ACN
</th>
<th>
ATVI
</th>
<th>
ADBE
</th>
<th>
AMD
</th>
<th>
AAP
</th>
<th>
AES
</th>
</tr>
<tr>
<th>
Date
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
2016-01-04
</th>
<td>
128.033249
</td>
<td>
39.050327
</td>
<td>
46.141216
</td>
<td>
85.239998
</td>
<td>
93.774796
</td>
<td>
36.365803
</td>
<td>
91.970001
</td>
<td>
2.770000
</td>
<td>
150.250793
</td>
<td>
7.769172
</td>
</tr>
<tr>
<th>
2016-01-05
</th>
<td>
128.591339
</td>
<td>
39.041222
</td>
<td>
45.948997
</td>
<td>
85.000000
</td>
<td>
94.262863
</td>
<td>
35.901806
</td>
<td>
92.339996
</td>
<td>
2.750000
</td>
<td>
149.224380
</td>
<td>
7.876278
</td>
</tr>
<tr>
<th>
2016-01-06
</th>
<td>
126.001358
</td>
<td>
38.713760
</td>
<td>
45.956993
</td>
<td>
85.300003
</td>
<td>
94.078682
</td>
<td>
35.563477
</td>
<td>
91.019997
</td>
<td>
2.510000
</td>
<td>
145.276642
</td>
<td>
7.604397
</td>
</tr>
<tr>
<th>
2016-01-07
</th>
<td>
122.931786
</td>
<td>
37.785950
</td>
<td>
45.820843
</td>
<td>
81.919998
</td>
<td>
91.316017
</td>
<td>
35.060810
</td>
<td>
89.110001
</td>
<td>
2.280000
</td>
<td>
146.885345
</td>
<td>
7.414905
</td>
</tr>
<tr>
<th>
2016-01-08
</th>
<td>
122.513206
</td>
<td>
36.994564
</td>
<td>
44.571400
</td>
<td>
84.580002
</td>
<td>
90.431938
</td>
<td>
34.519478
</td>
<td>
87.849998
</td>
<td>
2.140000
</td>
<td>
143.658066
</td>
<td>
7.522010
</td>
</tr>
<tr>
<th>
...
</th>
<td>
...
</td>
<td>
...
</td>
<td>
...
</td>
<td>
...
</td>
<td>
...
</td>
<td>
...
</td>
<td>
...
</td>
<td>
...
</td>
<td>
...
</td>
<td>
...
</td>
</tr>
<tr>
<th>
2018-12-24
</th>
<td>
168.035690
</td>
<td>
63.701054
</td>
<td>
75.395958
</td>
<td>
281.079987
</td>
<td>
130.427887
</td>
<td>
43.354313
</td>
<td>
205.160004
</td>
<td>
16.650000
</td>
<td>
147.761047
</td>
<td>
13.031178
</td>
</tr>
<tr>
<th>
2018-12-26
</th>
<td>
175.222961
</td>
<td>
67.645943
</td>
<td>
79.767776
</td>
<td>
307.440002
</td>
<td>
135.638382
</td>
<td>
45.749195
</td>
<td>
222.949997
</td>
<td>
17.900000
</td>
<td>
153.823486
</td>
<td>
13.464924
</td>
</tr>
<tr>
<th>
2018-12-27
</th>
<td>
179.399902
</td>
<td>
68.627304
</td>
<td>
80.547180
</td>
<td>
315.670013
</td>
<td>
137.004425
</td>
<td>
46.360237
</td>
<td>
225.139999
</td>
<td>
17.490000
</td>
<td>
153.486160
</td>
<td>
13.474353
</td>
</tr>
<tr>
<th>
2018-12-28
</th>
<td>
178.148697
</td>
<td>
69.074249
</td>
<td>
81.631172
</td>
<td>
318.170013
</td>
<td>
136.428741
</td>
<td>
46.123703
</td>
<td>
223.130005
</td>
<td>
17.820000
</td>
<td>
154.250168
</td>
<td>
13.455495
</td>
</tr>
<tr>
<th>
2018-12-31
</th>
<td>
179.249359
</td>
<td>
70.279106
</td>
<td>
82.589752
</td>
<td>
325.040009
</td>
<td>
137.589874
</td>
<td>
45.897026
</td>
<td>
226.240005
</td>
<td>
18.459999
</td>
<td>
156.234573
</td>
<td>
13.634648
</td>
</tr>
</tbody>
</table>
<p>
754 rows × 10 columns
</p>
</div>
<h1 id="数据可视化">数据可视化</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 股票相关度？</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_data</span>():</span></span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;sp500_joined.csv&quot;</span>)</span><br><span class="line">    df_corr = df.corr()</span><br><span class="line">    df_corr.to_csv(<span class="string">&quot;sp500corr.csv&quot;</span>)</span><br><span class="line">    data1 = df_corr.values</span><br><span class="line">    fig1 = plt.figure()</span><br><span class="line">    ax1 = fig1.add_subplot(<span class="number">111</span>)</span><br><span class="line"></span><br><span class="line">    heatmap1 = ax1.pcolor(data1, cmap=plt.cm.RdYlGn)</span><br><span class="line">    fig1.colorbar(heatmap1)</span><br><span class="line"></span><br><span class="line">    ax1.set_xticks(np.arange(data1.shape[<span class="number">1</span>]))</span><br><span class="line">    ax1.set_yticks(np.arange(data1.shape[<span class="number">0</span>]))</span><br><span class="line">    ax1.invert_yaxis()</span><br><span class="line">    ax1.xaxis.tick_top()</span><br><span class="line">    column_labels = df_corr.columns</span><br><span class="line">    row_labels = df_corr.index</span><br><span class="line">    ax1.set_xticklabels(column_labels)</span><br><span class="line">    ax1.set_yticklabels(row_labels)</span><br><span class="line">    plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line">    heatmap1.set_clim(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">visualize_data()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure>
<img src="pyFinance_files/pyFinance_24_0.svg" alt="svg" /><figcaption aria-hidden="true">svg</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 股票相关度？</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_data</span>():</span></span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;sp500_joined.csv&quot;</span>)</span><br><span class="line">    df_corr = df.corr()</span><br><span class="line">    df_corr.to_csv(<span class="string">&quot;sp500corr.csv&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    f, axes = plt.subplots(<span class="number">1</span>, figsize=(<span class="number">10</span>,<span class="number">8</span>))</span><br><span class="line">    <span class="comment"># sns.set(style=&quot;whitegrid&quot;)</span></span><br><span class="line">    sns.despine(left=<span class="literal">True</span>)</span><br><span class="line">    sns.heatmap(df_corr, cmap= <span class="string">&quot;RdYlGn&quot;</span>, cbar=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">visualize_data()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure>
<img src="pyFinance_files/pyFinance_26_0.svg" alt="svg" /><figcaption aria-hidden="true">svg</figcaption>
</figure>
<h1 id="获取7日内股价波动">获取7日内股价波动</h1>
<ul>
<li>买/卖/持有？</li>
<li>7日内表现</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_data_for_labels</span>(<span class="params">ticker</span>):</span></span><br><span class="line">    hm_days = <span class="number">7</span></span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;sp500_joined.csv&quot;</span>, index_col=<span class="number">0</span>)</span><br><span class="line">    tickers = df.columns.values.tolist()</span><br><span class="line">    df.fillna(<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, hm_days+<span class="number">1</span>):</span><br><span class="line">        df[<span class="string">&quot;&#123;&#125;_&#123;&#125;d&quot;</span>.format(ticker, i)] = (df[ticker].shift(-i)-df[ticker])/df[ticker]</span><br><span class="line">    df.fillna(<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> tickers, df</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">buy_sell_hold</span>(<span class="params">*args</span>):</span></span><br><span class="line">    cols = [c <span class="keyword">for</span> c <span class="keyword">in</span> args]</span><br><span class="line">    requirement = <span class="number">0.02</span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> cols:</span><br><span class="line">        <span class="keyword">if</span> col &gt; requirement:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> col &lt; -requirement:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<h1 id="获取特征集合">获取特征集合</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_featuresets</span>(<span class="params">ticker</span>):</span></span><br><span class="line">    tickers, df = process_data_for_labels(ticker)</span><br><span class="line">    df[<span class="string">&quot;&#123;&#125;_target&quot;</span>.format(ticker)] = list(map(buy_sell_hold, </span><br><span class="line">                                    df[<span class="string">&quot;&#123;&#125;_1d&quot;</span>.format(ticker)],</span><br><span class="line">                                    df[<span class="string">&quot;&#123;&#125;_2d&quot;</span>.format(ticker)],</span><br><span class="line">                                    df[<span class="string">&quot;&#123;&#125;_3d&quot;</span>.format(ticker)],</span><br><span class="line">                                    df[<span class="string">&quot;&#123;&#125;_4d&quot;</span>.format(ticker)],</span><br><span class="line">                                    df[<span class="string">&quot;&#123;&#125;_5d&quot;</span>.format(ticker)],</span><br><span class="line">                                    df[<span class="string">&quot;&#123;&#125;_6d&quot;</span>.format(ticker)],</span><br><span class="line">                                    df[<span class="string">&quot;&#123;&#125;_7d&quot;</span>.format(ticker)],</span><br><span class="line">                                    ))</span><br><span class="line"></span><br><span class="line">    vals = df[<span class="string">&quot;&#123;&#125;_target&quot;</span>.format(ticker)].values.tolist()</span><br><span class="line">    str_vals = [str(i) <span class="keyword">for</span> i <span class="keyword">in</span> vals]</span><br><span class="line">    <span class="comment"># print(&quot;Data spread&quot;, Counter(str_vals))</span></span><br><span class="line"></span><br><span class="line">    df.fillna(<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    df.replace([np.inf, -np.inf], np.nan, inplace=<span class="literal">True</span>)</span><br><span class="line">    df.dropna(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    df_vals = df[[t <span class="keyword">for</span> t <span class="keyword">in</span> tickers]].pct_change() <span class="comment"># 环比</span></span><br><span class="line">    df_vals.replace([np.inf, -np.inf], np.nan)</span><br><span class="line">    df_vals.fillna(<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    X = df_vals.values</span><br><span class="line">    y = df[<span class="string">f&quot;<span class="subst">&#123;ticker&#125;</span>_target&quot;</span>].values</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X, y, df</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="机器学习">机器学习</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, y, _ = extract_featuresets(<span class="string">&quot;MMM&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm, neighbors <span class="comment">#cross_validation deprecated </span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier, RandomForestClassifier<span class="comment"># 综合不同算法</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_ml</span>(<span class="params">ticker</span>):</span></span><br><span class="line">    X,y,df</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
  </entry>
  <entry>
    <title>分治</title>
    <url>/2020/09/04/%E5%88%86%E6%B2%BB/</url>
    <content><![CDATA[<p><a href="https://mp.weixin.qq.com/s?__biz=MzUyNjQxNjYyMg==&amp;mid=2247485228&amp;idx=1&amp;sn=9f48aee51dcb2b98b56b1827cc658439&amp;chksm=fa0e68adcd79e1bbcd0183ef30a79ede4e46c5835ce05ee6644169c3cc9454073019ccd85d3d&amp;scene=21#wechat_redirect">浅谈分治算法</a></p>
<h2 id="使用场景">使用场景</h2>
<p>可以分解为若干 MECE 的相同子问题，即具有最优子结构</p>
<h2 id="步骤">步骤</h2>
<ol type="1">
<li>分解</li>
<li>求解：直接/递归</li>
<li>合并</li>
</ol>
<h2 id="案例">案例</h2>
<h3 id="二分查找基本">二分查找(基本)</h3>
<ul>
<li>算法流程：
<ul>
<li>选择一个标志位 i 将集合分为两个子集合</li>
<li>判断 <code>arr[i]</code> 和 <code>target</code> 关系，相等则返回</li>
<li>否则判断在左侧还是右侧查找</li>
<li>递归直到找到或退出</li>
</ul></li>
<li>代码： <a href="https://hy2632.github.io/2020/09/02/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/">二分查找</a></li>
</ul>
<h3 id="全排列问题">全排列问题</h3>
<ul>
<li>问题描述：
<ul>
<li>有1，2，3，4个数，问你有多少种排列方法，并输出排列。</li>
</ul></li>
<li>问题分析：
<ul>
<li>确定第一位后，对后续序列进行全排列。直到只有一个数字时停止。</li>
</ul></li>
<li>代码：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;</span><br><span class="line">        fullSort(arr, <span class="number">0</span>, arr.length - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">fullSort</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> start, <span class="keyword">int</span> end)</span></span>&#123;</span><br><span class="line">        <span class="comment">// 递归终止条件</span></span><br><span class="line">        <span class="keyword">if</span> (start == end)&#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i : arr)&#123;</span><br><span class="line">                System.out.print(i);</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.prinln();</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = start; i &lt;= end; i++)&#123;</span><br><span class="line">            swap(arr, i, start);</span><br><span class="line">            fullSort(arr, start + <span class="number">1</span>, end); <span class="comment">// 把该位放到开头，对剩下的位数全排列</span></span><br><span class="line">            swap(arr, i, start;)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> tmp = arr[i];</span><br><span class="line">        arr[i] = arr[j];</span><br><span class="line">        arr[j] = tmp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="归并排序">归并排序</h3>
<ul>
<li>问题分析：
<ul>
<li>一个序列从中点分为两个有序子序列，然后合并。</li>
</ul></li>
<li>代码：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">guibing</span> </span>&#123;</span><br><span class="line">     <span class="keyword">static</span> <span class="keyword">int</span>[] mergeSort(<span class="keyword">int</span>[] sourceArray)&#123;</span><br><span class="line">        <span class="keyword">if</span> (sourceArray.length &lt;= <span class="number">1</span>) <span class="keyword">return</span> sourceArray;</span><br><span class="line">        <span class="keyword">int</span>[] arr = Arrays.copyOf(sourceArray, sourceArray.length);</span><br><span class="line">        <span class="keyword">int</span> mid = arr.length &gt;&gt; <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span>[] left = Arrays.copyOfRange(arr, <span class="number">0</span>, mid);</span><br><span class="line">        <span class="keyword">int</span>[] right = Arrays.copyOfRange(arr, mid, arr.length);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> merge(mergeSort(left), mergeSort(right));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// a1 &amp; a2 are both sorted</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span>[] merge(<span class="keyword">int</span>[] a1, <span class="keyword">int</span>[] a2)&#123;</span><br><span class="line">        <span class="keyword">int</span>[] result = <span class="keyword">new</span> <span class="keyword">int</span>[a1.length + a2.length];</span><br><span class="line">        <span class="keyword">int</span> sortedIndex = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">0</span>, j = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(i &lt; a1.length &amp;&amp; j &lt; a2.length)&#123;</span><br><span class="line">            <span class="keyword">if</span> (a1[i] &lt;= a2[j])&#123;</span><br><span class="line">                result[sortedIndex++] = a1[i++];</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                result[sortedIndex++] = a2[j++];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(i &lt; a1.length)&#123;</span><br><span class="line">            result[sortedIndex++] = a1[i++];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(j &lt; a2.length)&#123;</span><br><span class="line">            result[sortedIndex++] = a2[j++];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = &#123;<span class="number">3</span>,<span class="number">76</span>,<span class="number">4</span>,<span class="number">32</span>,<span class="number">643</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>&#125;;</span><br><span class="line">        System.out.println(Arrays.toString(mergeSort(arr)));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="快速排序">快速排序</h3>
<ul>
<li>问题分析：
<ul>
<li>partition: 一个序列，选定一个 pivot ，调整 pivot 位置使得它左边的值小于它，右边的值大于它，并确定该 pivot 的位置</li>
<li>对于左右的序列分别进行快速排序</li>
</ul></li>
<li>代码：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">kuaipai</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> a, <span class="keyword">int</span> b)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> temp = arr[a];</span><br><span class="line">        arr[a] = arr[b];</span><br><span class="line">        arr[b] = temp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span>[] quickSort(<span class="keyword">int</span>[] sourceArray) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = Arrays.copyOf(sourceArray, sourceArray.length);</span><br><span class="line">        <span class="keyword">return</span> quickSortRecursion(arr, <span class="number">0</span>, arr.length - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span>[] quickSortRecursion(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> left, <span class="keyword">int</span> right) &#123;</span><br><span class="line">        <span class="keyword">if</span> (left &lt; right) &#123;</span><br><span class="line">            <span class="keyword">int</span> mid = partition(arr, left, right);</span><br><span class="line">            quickSortRecursion(arr, left, mid - <span class="number">1</span>);</span><br><span class="line">            quickSortRecursion(arr, mid + <span class="number">1</span>, right);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> arr;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> pivot = left;</span><br><span class="line">        <span class="keyword">int</span> index = pivot + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> i = index;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (i &lt;= right) &#123;</span><br><span class="line">            <span class="keyword">if</span> (arr[i] &lt;= arr[pivot]) &#123;</span><br><span class="line">                swap(arr, i, index);</span><br><span class="line">                index++;</span><br><span class="line">            &#125;</span><br><span class="line">            i++;</span><br><span class="line">        &#125;</span><br><span class="line">        swap(arr, pivot, index - <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> index - <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = &#123; <span class="number">3</span>, <span class="number">76</span>, <span class="number">4</span>, <span class="number">32</span>, <span class="number">643</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span> &#125;;</span><br><span class="line">        System.out.println(Arrays.toString(quickSort(arr)));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="汉诺塔">汉诺塔</h3>
<ul>
<li>问题分析：
<ul>
<li>原问题：n 个盘子，从 source 到 target ，中途可借助 temp</li>
<li>子问题：上面 n-1 个盘子放到temp，最底下盘子到target，然后 temp 上的 n-1 个盘子借助 source 移到 target 上。 如果 n = 1， 直接移。</li>
</ul></li>
<li>代码：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">hanoi</span><span class="params">(<span class="keyword">int</span> n, String source, String temp, String target)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">1</span>)&#123;</span><br><span class="line">        move (n, source, target);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        hanoi (n - <span class="number">1</span>, source, target, temp);</span><br><span class="line">        move (n, source, target);</span><br><span class="line">        hanoi (n - <span class="number">1</span>, temp, source, target);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">move</span><span class="params">(<span class="keyword">int</span> n, String source, String target)</span></span>&#123;</span><br><span class="line">    System.out.printf(<span class="string">&quot;第 %d 号盘子 move: %s ---&gt; %s \n&quot;</span>, n, source, target);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Java数据结构与算法</category>
      </categories>
  </entry>
  <entry>
    <title>哈希</title>
    <url>/2020/09/25/%E5%93%88%E5%B8%8C/</url>
    <content><![CDATA[<h3 id="找到最长无重复字串长度">找到最长无重复字串长度</h3>
<ul>
<li>题目描述
<ul>
<li>给定一个数组arr，返回arr的最长无的重复子串的长度(无重复指的是所有数字都不相同</li>
<li>示例1 输入 <code>[2,3,4,5]</code>, 输出 <code>4</code></li>
</ul></li>
<li>考点： 哈希，双指针，字符串</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> arr int整型一维数组 the array</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> int整型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NowCoder_MaxLength</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 滑动窗口是一个set，如果没有就添加，否则比较长度，start增加1</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">maxLength</span><span class="params">(<span class="keyword">int</span>[] arr)</span> </span>&#123;</span><br><span class="line">        HashMap&lt;Integer, Integer&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="keyword">int</span> max = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> start = <span class="number">0</span>, end = <span class="number">0</span>; end &lt; arr.length; end++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (map.containsKey(arr[end])) &#123;</span><br><span class="line">                start = Math.max(start, map.get(arr[end]) + <span class="number">1</span>); <span class="comment">// 注意点</span></span><br><span class="line">            &#125;</span><br><span class="line">            max = Math.max(max, end - start + <span class="number">1</span>);</span><br><span class="line">            map.put(arr[end], end);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> max;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] a = &#123; <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span> &#125;;</span><br><span class="line">        System.out.println(maxLength(a));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="nc61-twosum">NC61 TwoSum</h3>
<ul>
<li><p>题目描述</p>
<p>给出一个整数数组，请在数组中找出两个加起来等于目标值的数， 你给出的函数twoSum 需要返回这两个数字的下标（index1，index2），需要满足 index1 小于index2</p>
<p>注意：下标是从1开始的 假设给出的数组中只存在唯一解</p>
<p>例如： 给出的数组为 {20, 70, 110, 150},目标值为90</p>
<p>输出 index1=1, index2=2</p></li>
<li><p>代码</p></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NC61</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * </span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> numbers int整型一维数组</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> target  int整型</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> int整型一维数组</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] twoSum(<span class="keyword">int</span>[] numbers, <span class="keyword">int</span> target) &#123;</span><br><span class="line">        <span class="comment">// write code here</span></span><br><span class="line">        HashMap&lt;Integer, Integer&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; numbers.length; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (map.containsKey(target - numbers[i])) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[] &#123; map.get(target - numbers[i]) + <span class="number">1</span>, i + <span class="number">1</span> &#125;;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                map.put(numbers[i], i);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] nums = &#123; <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span> &#125;;</span><br><span class="line">        <span class="keyword">int</span> tgt = <span class="number">6</span>;</span><br><span class="line">        System.out.println(Arrays.toString(twoSum(nums, tgt)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Java数据结构与算法</category>
      </categories>
  </entry>
  <entry>
    <title>布朗运动与伊藤引理</title>
    <url>/2020/12/07/%E5%B8%83%E6%9C%97%E8%BF%90%E5%8A%A8%E4%B8%8E%E4%BC%8A%E8%97%A4%E5%BC%95%E7%90%86/</url>
    <content><![CDATA[<h2 id="引用">引用:</h2>
<p><a href="https://zhuanlan.zhihu.com/p/38293827">布朗运动、伊藤引理、BS 公式（前篇） - 石川的文章 - 知乎</a></p>
<p><a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-070j-advanced-stochastic-processes-fall-2013/lecture-notes/MIT15_070JF13_Lec7.pdf">Lec7 Brownian Motion - MIT advanced-stochastic-processes-fall-2013</a></p>
<p><a href="https://financetrainingcourse.com/education/wp-content/uploads/2011/03/Understanding.pdf">P13, Understanding N(d1) and N(d2): Risk-Adjusted Probabilities in the Black-Scholes Model</a></p>
<h2 id="布朗运动">布朗运动</h2>
<p>定义布朗运动</p>
<p><span class="math display">\[B(t+\Delta t) - B(t) \sim N(0, \Delta t)\]</span> <span class="math display">\[B(0) = 0\]</span></p>
<!-- 布朗运动是一个 Markov Process，因为$B(t^+)$ 的值仅与 -->
<p>定义到t时间点的最大值 <span class="math inline">\(M(t)\)</span>。该极值的分布有性质 <span class="math inline">\(P(M(t)\geq a) = 2P(B(t)\geq a)\)</span>.</p>
<p>证明: (the reflection principle，反射性)</p>
<p><span class="math display">\[\begin{aligned}
P(B(t)\geq a) &amp;= P(B(t)\geq a, M(t)\geq a) + P(B(t)\geq a, M(t)&lt; a) \\
&amp;= P(B(t)\geq a, M(t)\geq a) \\
&amp;= P(B(t)\geq a| M(t)\geq a) P(M(t)\geq a)\\
&amp;= P(B(t)\geq a| T_a \leq t) P(M(t)\geq a)\\
&amp;= P(B(t) - B(T_a)\geq 0|T_a \leq t) P(M(t)\geq a)\\
&amp;= \frac12 P(M(t)\geq a)
\end{aligned}\]</span></p>
<p><span class="math display">\[P(M(t)\geq a) = 2 - 2\Phi(\frac{a}{\sqrt{t}}) \]</span></p>
<p>根据此性质我们可以量化极值的概率分布。</p>
<h2 id="二次变分">二次变分</h2>
<p>可微函数 <span class="math inline">\(f(x)\)</span> 在区间 <span class="math inline">\([0, T]\)</span> 和该区间的一个划分 <span class="math inline">\(\Pi\)</span> 上的二次变分定义为：</p>
<p><span class="math display">\[\sum_{i=0}^{N-1}{ [f(t_{i+1}) - f(t_{i})]^2  }\]</span></p>
<p>利用中值定理</p>
<p><span class="math display">\[\begin{aligned}
\sum_{i=0}^{N-1}{ [f(t_{i+1}) - f(t_{i})]^2  } &amp;\leq \sum_{i=0}^{N-1}{ f&#39;(s_i)^2(t_{i+1} - t_i)^2 } \\
&amp;\leq  \max_{s\in [0,T]}{f&#39;(s)^2} \cdot T\cdot \max_i{(t_{i+1} - t_i)}
\end{aligned}\]</span></p>
<p>可见该二次变分可以随着划分的更细而趋于0。</p>
<p>而布朗运动的二次变分在足够细的划分下则等于 <span class="math inline">\(T\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\lim_{|\Pi|\to 0}\sum_{i=0}^{N-1}{ [B(t_{i+1}) - B(t_{i})]^2  } &amp;= T\\
\end{aligned}\]</span></p>
<p>根据独立同分布的随机变量的大数定律可以证明。例如 <span class="math inline">\([0, t]\)</span> 区间上,</p>
<p><span class="math display">\[\begin{aligned}
E[B(t)^2] &amp;= E[B(t)]^2 + Var(B(t)) \\ 
&amp;= 0 + t\\
&amp;= t
\end{aligned}\]</span></p>
<p>由于布朗运动的波动性，任意细分的区间 <span class="math inline">\(T\)</span> 上的位移差平方量都不能为0，而是和这个区间长度相同的量。</p>
<p>可以记为 <span class="math inline">\((dB)^2 = dt\)</span></p>
<h2 id="几何布朗运动">几何布朗运动</h2>
<p>在标准布朗运动的基础上增加漂移项(drift)和扩散(diffusion)系数，</p>
<p><span class="math display">\[X(t) = \mu t + \sigma B(t)\]</span></p>
<p><span class="math display">\[dX(t) = \mu dt + \sigma dB(t)\]</span></p>
<p><span class="math inline">\(X(t)\)</span> 不宜直接描述股价，因为可能取到负值，但可以描述收益率。令股价变化率 <span class="math inline">\(\frac{dS(t）}{S(t)} = X(t)\)</span>,</p>
<p><span class="math display">\[\frac{dS(t）}{S(t)}= \mu dt + \sigma dB(t)\]</span></p>
<p><span class="math display">\[dS(t)= \mu S(t)dt + \sigma S(t)dB(t)\]</span></p>
<p>满足该随机微分方程的股价 <span class="math inline">\(S(t)\)</span> 是一个 <strong>几何布朗运动</strong> 。其求解用到了 <strong>伊藤积分</strong>。</p>
<p>注意到 <span class="math inline">\(E[X(t)] = \mu t\)</span>，长期漂移量主导。价值投资。</p>
<h2 id="伊藤引理">伊藤引理</h2>
<p><span class="math inline">\(f(B(t))\)</span> 的微分不能简单地写作</p>
<p><span class="math display">\[df = \frac{df}{dB} \frac{dB}{dt} dt\]</span></p>
<p>这是由于 <span class="math inline">\(B(t)\)</span> 不可微， <span class="math inline">\(\frac{dB}{dt}\)</span> 不存在。</p>
<p>那么能否写为 <span class="math inline">\(df = \frac{df}{dB}dB\)</span> 呢？同样也不可以。该表达式来自于泰勒展开的一次项，对于一般可微函数二阶往上都是高阶小量可忽略，但是对于布朗运动并非如此。根据之前布朗运动二次变分的结论，第二阶 <span class="math inline">\((dB)^2 = dt\)</span>。</p>
<p><span class="math display">\[df = \frac{df}{dB} dB + \frac12 \frac{d^2f}{dB^2}dt\]</span></p>
<p>此为伊藤引理的<strong>基本形式</strong>。二阶之后仍然是更高阶的小量，可以忽略。</p>
<p>带漂移项的布朗运动还是关于时间 <span class="math inline">\(t\)</span> 的函数，这种情况下</p>
<p><span class="math display">\[df = \frac{df}{dB} dB + (\frac{df}{dt} + \frac12 \frac{d^2f}{dB^2})dt\]</span></p>
<h2 id="伊藤引理-一般形式">伊藤引理-一般形式</h2>
<p>带漂移和扩散的布朗运动</p>
<p><span class="math display">\[dX(t) = \mu dt + \sigma dB(t)\]</span></p>
<p>假设漂移和扩散参数也是关于随机过程 <span class="math inline">\(X(t)\)</span> 和 <span class="math inline">\(t\)</span> 的函数，满足以下 <strong>随机微分方程(SDE)</strong> 的随机过程为 <strong>伊藤漂移扩散过程 (Ito drift-diffusion process)</strong>， 简称伊藤过程。</p>
<p><span class="math display">\[dX = a(X,t)dt + b(X,t)dB\]</span></p>
<p>关于 <span class="math inline">\(X(t)\)</span> 和 <span class="math inline">\(t\)</span> 的二阶连续可导函数 <span class="math inline">\(f(X(t),t)\)</span> 由伊藤引理</p>
<p><span class="math display">\[df = \frac{\partial f}{\partial X}dX + \frac{\partial f}{\partial t}dt + \frac12 \frac{\partial^2f}{\partial X^2}(dX)^2\]</span></p>
<p>代入并略去高阶小量</p>
<p><span class="math display">\[\begin{aligned}
df &amp;= \frac{\partial f}{\partial X}(adt+bdB) + \frac{\partial f}{\partial t}dt + \frac12 \frac{\partial^2f}{\partial X^2}(b^2dt) \\
&amp;= \bigg(\frac{\partial f}{\partial X}a + \frac{\partial f}{\partial t} + \frac12 \frac{\partial^2f}{\partial X^2}b^2 \bigg)dt + \frac{\partial f}{\partial X}bdB
\end{aligned}\]</span></p>
<p>可见 <span class="math inline">\(f(X(t), t)\)</span> 和 <span class="math inline">\(X(t)\)</span> 一样也是一个伊藤过程。且其布朗运动不变。</p>
<h2 id="几何布朗运动的随机微分方程求解">几何布朗运动的随机微分方程求解</h2>
<p><span class="math display">\[dS= \mu Sdt + \sigma SdB\]</span></p>
<p><span class="math inline">\(S\)</span> 是一个伊藤过程， <span class="math inline">\(a=\mu S, b=\sigma S\)</span>。</p>
<p>令 <span class="math inline">\(f = \ln{S}\)</span>, 应用伊藤引理的一般形式，</p>
<p><span class="math display">\[\begin{aligned}
df &amp;= \bigg(\frac{\partial f}{\partial S}\mu S + 0 + \frac12 \frac{\partial^2f}{\partial S^2}\sigma^2 S^2 \bigg)dt + \frac{\partial f}{\partial S}\sigma SdB \\
&amp;= \bigg(\mu - \frac12 \sigma^2 \bigg)dt +\sigma dB 
\end{aligned}\]</span></p>
<p>则 <span class="math inline">\(f\)</span> 是一个带漂移项的布朗运动。其漂移系数为 <span class="math inline">\(\mu -\frac12 \sigma^2\)</span>，扩散系数为 <span class="math inline">\(\sigma\)</span>。</p>
<p><span class="math display">\[\ln S(T) \sim N(\ln S(0) + (\mu -\frac12 \sigma^2)T, \sigma^2T)\]</span></p>
<p>股价函数 <span class="math inline">\(S(T)\)</span> 满足对数正态分布，且通过积分，其表达式为</p>
<p><span class="math display">\[S(T) = S(0) \exp \bigg( \bigg(\mu -\frac12 \sigma^2 \bigg)T + \sigma B(T)   \bigg)\]</span></p>
<p>股价的 <strong>连续复利期望收益率</strong> 并非 <span class="math inline">\(\mu\)</span> 而是 <span class="math inline">\(\mu -\frac12 \sigma^2\)</span>。后者是几何平均，考虑了波动。连续复利收益率 <span class="math inline">\(x = \frac{\ln S(T) - \ln S(0)}{T}\)</span> 的分布是</p>
<p><span class="math display">\[ x \sim N(\mu -\frac12 \sigma^2, \frac{\sigma^2}T)\]</span></p>
<h2 id="black-scholes-微分方程">Black-Scholes 微分方程</h2>
<p>前提： 欧式期权，股价 <span class="math inline">\(S\)</span> 是几何布朗运动 (股价满足对数正态分布), ...</p>
<p><span class="math inline">\(C\)</span> : 欧式看涨期权的价格</p>
<p><span class="math inline">\(S\)</span> : 股价，是一个伊藤过程</p>
<p><span class="math inline">\(C\)</span> 是关于 <span class="math inline">\(S\)</span> 和 <span class="math inline">\(t\)</span> 的函数，记为 <span class="math inline">\(C(S, t)\)</span></p>
<p>应用伊藤引理的,</p>
<p><span class="math display">\[dC = \bigg(\frac{\partial C}{\partial S}\mu S + \frac{\partial C}{\partial t} + \frac12 \frac{\partial^2C}{\partial S^2} \sigma^2 S^2 \bigg)dt + \frac{\partial C}{\partial S} \sigma S dB\]</span></p>
<p>离散化，取一个小区间 <span class="math inline">\(\Delta t\)</span></p>
<p><span class="math display">\[\Delta C = \bigg(\frac{\partial C}{\partial S}\mu S + \frac{\partial C}{\partial t} + \frac12 \frac{\partial^2C}{\partial S^2} \sigma^2 S^2 \bigg)\Delta t + \frac{\partial C}{\partial S} \sigma S \Delta B\]</span></p>
<p><span class="math display">\[\Delta S = \mu S \Delta t + \sigma S \Delta B\]</span></p>
<p><strong>Delta-对冲</strong> :把布朗运动 <span class="math inline">\(\Delta B\)</span> 对冲掉, <span class="math inline">\(-1\)</span> 份的 call option 加上 <span class="math inline">\(\frac{\partial C}{\partial S}\)</span> 份的股票，得到一个无风险的投资组合。</p>
<p><span class="math display">\[\Delta P = - \Delta C + \frac{\partial C}{\partial S} \Delta S = - \bigg(\frac{\partial C}{\partial S}\mu S + \frac{\partial C}{\partial t} + \frac12 \frac{\partial^2C}{\partial S^2} \sigma^2 S^2 - \frac{\partial C}{\partial S}\mu S \bigg)\Delta t \\
= - \bigg(\frac{\partial C}{\partial t} + \frac12 \frac{\partial^2C}{\partial S^2} \sigma^2 S^2 \bigg)\Delta t \]</span></p>
<p>不存在无风险套利的市场中，</p>
<p><span class="math display">\[\Delta P = rP\Delta t\]</span></p>
<p><span class="math display">\[\therefore - \bigg(\frac{\partial C}{\partial t} + \frac12 \frac{\partial^2C}{\partial S^2} \sigma^2 S^2 \bigg) = rP = r(-C + \frac{\partial C}{\partial S} S)\]</span></p>
<p>于是得到 <strong>Black-Scholes 微分方程</strong>。非随机的微分方程。</p>
<p><span class="math display">\[rC = \frac{\partial C}{\partial t} + \frac12 \frac{\partial^2C}{\partial S^2} \sigma^2 S^2 + r\frac{\partial C}{\partial S} S\]</span></p>
<p>该微分方程存在边界条件</p>
<p><span class="math display">\[C = \max {(S(T)-K, 0)}\]</span></p>
<p>该方程的核心在于，<span class="math inline">\(C\)</span> 和 <span class="math inline">\(S\)</span> 的不确定性来自于同一个布朗运动 <span class="math inline">\(B\)</span>，因而可以对冲。</p>
<h2 id="风险中性定价理论">风险中性定价理论</h2>
<p>利用边界条件求解BS微分方程就可以给欧式看涨期权 <span class="math inline">\(C\)</span> 定价。另一种方法是 <strong>风险中性定价</strong>，可以不求解BS。</p>
<p>在BS中，注意到漂移系数 <span class="math inline">\(\mu\)</span> 被约去。这个值和风险偏好有关，越 Risk-averse 该值越高。因为这个值不影响BS，计算 C 时可以假设所有投资者风险中性，所要求的回报率都是无风险利率 <span class="math inline">\(r\)</span>。于是可以用这个值贴现。</p>
<h2 id="black-scholes-期权定价公式">Black-Scholes 期权定价公式</h2>
<p>风险中性定价，</p>
<p><span class="math display">\[C = e^{-rT}E[\max {(S(T)-K, 0)}]\]</span></p>
<!-- BS微分方程，

$$rC = \frac{\partial C}{\partial t} + \frac12 \frac{\partial^2C}{\partial S^2} \sigma^2 S^2 + r\frac{\partial C}{\partial S} S$$ -->
<p>股价 <span class="math inline">\(S\)</span> 满足对数正态分布，</p>
<p><span class="math display">\[\ln S(T) \sim N(\ln S(0) + (\mu -\frac12 \sigma^2)T, \sigma^2T)\]</span></p>
<p><span class="math display">\[\begin{aligned}
E[\max{(S_T-K, 0)}] &amp;= E[S_T-K | S_T&gt;K] \\
&amp;= E[S_T|S_T&gt;K] - KP(S_T&gt;K) \\
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
P(S_T&gt;K) &amp;=  P(\ln{S_T} &gt; \ln{K}) \\
&amp;= P(\frac{\ln{S_T} - \ln{S_0} - (\mu-\frac{\sigma^2}{2})T}{\sigma\sqrt{T}} &gt; \frac{\ln{K} - \ln{S_0} - (\mu-\frac{\sigma^2}{2})T}{\sigma\sqrt{T}}) \\
&amp;= P(\frac{\ln{S_T} - \ln{S_0} - (\mu-\frac{\sigma^2}{2})T}{\sigma\sqrt{T}} &lt; \frac{\ln{S_0/K} + (\mu-\frac{\sigma^2}{2})T}{\sigma\sqrt{T}})\\
&amp;= N(\frac{\ln{S_0/K} + (\mu-\frac{\sigma^2}{2})T}{\sigma\sqrt{T}}) \\
&amp;= N(d_2)
\end{aligned}\]</span></p>
<p><span class="math inline">\(E[S_T|S_T&gt;K]\)</span> 的求解用到了该<a href="https://financetrainingcourse.com/education/wp-content/uploads/2011/03/Understanding.pdf">引理：</a></p>
<hr />
<p>If <span class="math inline">\(\log S \sim N(\mu, \sigma^2)\)</span>, the expectation of truncated <span class="math inline">\(\hat{S}(K)\)</span> is</p>
<p><span class="math display">\[E\hat{S} = \exp(\mu + \frac{\sigma^2}2) N(\sigma - D),\]</span></p>
<p>where</p>
<p><span class="math display">\[D = \frac{log{K}-\mu}{\sigma}\]</span></p>
<p>Proof: Using integral...</p>
<hr />
<p>在本例中， <span class="math display">\[\begin{aligned}
D  &amp;= \frac{\ln{K/S_0} - (\mu-\frac{\sigma^2}{2})T}{\sigma\sqrt{T}}\\
\sigma-D &amp;=\frac{\ln{S_0/K} + (\mu-\frac{\sigma^2}{2})T + \sigma^2T}{\sigma\sqrt{T}} = \frac{\ln{S_0/K} + (\mu+\frac{\sigma^2}{2})T}{\sigma\sqrt{T}} \text{ The sigma on both sides are different } \\
\mu + \frac{\sigma^2}2 &amp;= \ln{S_0} + (\mu-\frac{\sigma^2}2)T + \frac{\sigma^2}{2}T = \ln{S_0} + \mu T \\
E\hat{S} &amp;= S_0e^{\mu T}N(\frac{\ln{S_0/K} + (\mu+\frac{\sigma^2}{2})T}{\sigma\sqrt{T}}) = S_0e^{\mu T}N(d_1) = E[S_T|S_T&gt;K] \end{aligned}\]</span></p>
<p>代入 C 的表达式, <span class="math inline">\(r=\mu\)</span></p>
<p><span class="math display">\[\begin{aligned}
C &amp;= e^{-rT}E[\max {(S(T)-K, 0)}] \\
&amp;= e^{-rT}(E[S_T|S_T&gt;K] - KP(S_T&gt;K) ) \\
&amp;= S_0N(d_1) - Ke^{-rT}N(d_2) \end{aligned}\]</span></p>
<p>整理结果，Black-Scholes 公式：</p>
<p><img src="布朗运动与伊藤引理\equation.jpg" /></p>
<p><span class="math inline">\(N(d_2)\)</span> 表示该期权被行权的概率。</p>
<p><span class="math display">\[N(d_2) = P(S_T&gt;K)\]</span></p>
<p><span class="math inline">\(N(d_1)\)</span>更为复杂，没有比较直观的表述</p>
<p><span class="math display">\[N(d_1) = \frac{E[S_T|S_T&gt;K]}{e^{rT}S_0}\]</span></p>
<p><span class="math display">\[N(d_1)S_0 = \frac{E[S_T|S_T&gt;K]}{e^{rT}}\]</span></p>
<p>"Thus, N(d1) is the factor by which the present value of contingent receipt of the stock exceeds the current stock price."</p>
<p><span class="math display">\[N(d_1) = \frac{\partial C}{\partial S_0}\]</span></p>
<p>所以 <span class="math inline">\(N(d_1)\)</span> 衡量 call 和 stock 价格的敏感性。</p>
<h2 id="类风险敞口">5类风险敞口</h2>
<p><span class="math display">\[Vega = \frac{\partial C}{\partial S} = N(d_1)\]</span></p>
<p><span class="math display">\[\gamma = \frac{\partial^2 C}{\partial S^2}\]</span></p>
<p><span class="math display">\[\Delta = \frac{\Delta C}{\Delta S}\]</span></p>
<p><span class="math display">\[\rho = \frac{\partial^2 C}{\partial r^2}\]</span></p>
<p><span class="math display">\[\theta = \frac{\partial^2 C}{\partial T^2}\]</span></p>
<h2 id="其他期权定价">其他期权定价</h2>
<p>欧式看跌 european put 直接使用期权平价公式 put-call parity.</p>
<h2 id="understanding-nd1-and-nd2-risk-adjusted-probabilities-in-the-black-scholes-model">Understanding N(d1) and N(d2): Risk-Adjusted Probabilities in the Black-Scholes Model</h2>
<div class="pdfobject-container" data-target="./Understanding_N(d1)_and_N(d2).pdf" data-height="1000px"></div>
]]></content>
      <categories>
        <category>Finance</category>
      </categories>
      <tags>
        <tag>Financial Engineering</tag>
      </tags>
  </entry>
  <entry>
    <title>十大排序算法</title>
    <url>/2020/08/25/%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h1 id="链接">链接：</h1>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzUyNjQxNjYyMg==&amp;mid=2247484184&amp;idx=1&amp;sn=62965b401aa42107b3c17d1d8ea17454&amp;chksm=fa0e6c99cd79e58f298e9026f677f912bd8c8e55edb48fc509b2b5834f05e529a9b47d59d202&amp;scene=21#wechat_redirect">十大经典排序算法动画与解析，看我就够了！（配代码完全版）</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/52884590">视频 | 手撕九大经典排序算法，看我就够了！ - 力扣（LeetCode）的文章 - 知乎</a></p>
<h1 id="复杂度">复杂度</h1>
<figure>
<img src="https://pic2.zhimg.com/v2-b9b0c6526d416d8e8e81916ac1dd63b8_b.jpg" alt="复杂度" /><figcaption aria-hidden="true">复杂度</figcaption>
</figure>
<!-- | Sorting Algorithm | Avg. Time | Best Case  | Worst Case | Space | Inplace | Stability |
| ----------------- | --------- | ---------- | ---------- | ----- | ------- | --------- |
| BubbleSort        | O(n2)     | O(n)       | O(n2)      | O(1)  | Y       | Y         |
| SelectionSort     | O(n2)     | O(n2)      | O(n2)      | O(1)  | Y       | N         |
| InsertSort        | O(n2)     | O(n)       | O(n)       | O(1)  | Y       | N         |
| ShellSort         | O(nlogn)  | O(nlogn) |            | O(1)  | Y       | N         |
| MergeSort         |           |            |            |       | N       |           |
| QuickSort         |           |            |            |       |         |           |
| HeapSort          |           |            |            |       |         |           |
| CountingSort      |           |            |            |       | N       |           |
| BucketSort        |           |            |            |       | N       |           |
| RadixSort         |           |            |            |       | N       |           | -->
<p><a href="https://www.zhihu.com/question/24516934/answer/28076722">很多高效排序算法的代价是 nlogn，难道这是排序算法的极限了吗？ - 曾加的回答 - 知乎</a></p>
<h1 id="排序算法">排序算法</h1>
<h2 id="冒泡排序">冒泡排序</h2>
<p>基础版本： <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> Java.util.Arrays;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] BubbleSort(<span class="keyword">int</span>[] sourceArray) &#123;</span><br><span class="line">    <span class="keyword">int</span>[] arr = Arrays.copyOf(sourceArray, sourceArray.length);</span><br><span class="line">    <span class="keyword">int</span> temp;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; arr.length; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; arr.length - i - <span class="number">1</span>; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (arr[j + <span class="number">1</span>] &lt; arr[j]) &#123;</span><br><span class="line">                temp = arr[j];</span><br><span class="line">                arr[j] = arr[j + <span class="number">1</span>];</span><br><span class="line">                arr[j + <span class="number">1</span>] = temp;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> arr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>优化版本： 如果第<code>i</code>个循环，从第<code>0</code>个到第<code>arr.length-1-i</code>都顺序，则可以提前结束 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] BubbleSort(<span class="keyword">int</span>[] sourceArray) &#123;</span><br><span class="line">    <span class="keyword">int</span>[] arr = Arrays.copyOf(sourceArray, sourceArray.length);</span><br><span class="line">    <span class="keyword">int</span> temp;</span><br><span class="line">    <span class="keyword">boolean</span> flag;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; arr.length; i++) &#123;</span><br><span class="line">        flag = <span class="keyword">true</span>; <span class="comment">// 默认存在逆序对，会发生交换</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; arr.length - i - <span class="number">1</span>; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (arr[j + <span class="number">1</span>] &lt; arr[j]) &#123;</span><br><span class="line">                flag = <span class="keyword">false</span>;</span><br><span class="line">                temp = arr[j];</span><br><span class="line">                arr[j] = arr[j + <span class="number">1</span>];</span><br><span class="line">                arr[j + <span class="number">1</span>] = temp;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (flag) </span><br><span class="line">            <span class="keyword">break</span>; <span class="comment">//如果全部顺序就提前结束外循环</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> arr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="选择排序">选择排序</h2>
<p>每一步找出未排序序列中的最小值 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] SelectionSort(<span class="keyword">int</span>[] sourceArray)&#123;</span><br><span class="line">    <span class="keyword">int</span>[] arr = Arrays.copyOf(sourceArray, sourceArray.length);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; arr.length; i++)&#123;</span><br><span class="line">        <span class="keyword">int</span> argmin = i;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = i; j &lt; arr.length; j++)&#123;</span><br><span class="line">            <span class="keyword">if</span> (arr[j] &lt; arr[argmin])&#123;</span><br><span class="line">                argmin = j;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> temp = arr[i];</span><br><span class="line">        arr[i] = arr[argmin];</span><br><span class="line">        arr[argmin] = temp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> arr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="插入排序">插入排序</h2>
<p>把下一个元素插入到有序序列的合适位置 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] InsertSort(<span class="keyword">int</span>[] sourceArray)&#123;</span><br><span class="line">    <span class="keyword">int</span>[] arr = Arrays.copyOf(sourceArray, sourceArray.length);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; arr.length; i++)&#123;</span><br><span class="line">        <span class="keyword">int</span> j = i;</span><br><span class="line">        <span class="keyword">int</span> temp = arr[i];</span><br><span class="line">        <span class="keyword">while</span> (j - <span class="number">1</span> &gt;= <span class="number">0</span> &amp;&amp; arr[j - <span class="number">1</span>] &gt; temp)&#123;</span><br><span class="line">            arr[j] = arr[j-<span class="number">1</span>];</span><br><span class="line">            j--;</span><br><span class="line">        &#125;</span><br><span class="line">        arr[j] = temp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> arr;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h2 id="希尔排序">希尔排序</h2>
<p>优化版的插入排序 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] ShellSort(<span class="keyword">int</span>[] sourceArray) &#123;</span><br><span class="line">    <span class="keyword">int</span>[] arr = Arrays.copyOf(sourceArray, sourceArray.length);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> gap = arr.length &gt;&gt; <span class="number">1</span>; gap &gt;= <span class="number">1</span>; gap &gt;&gt;= <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = gap; i &lt; arr.length; i++) &#123;</span><br><span class="line">            <span class="keyword">int</span> temp = arr[i];</span><br><span class="line">            <span class="keyword">int</span> j = i;</span><br><span class="line">            <span class="keyword">while</span> (j - gap &gt;= <span class="number">0</span> &amp;&amp; arr[j - gap] &gt; temp) &#123;</span><br><span class="line">                arr[j] = arr[j - gap];</span><br><span class="line">                j -= gap;</span><br><span class="line">            &#125;</span><br><span class="line">            arr[j] = temp;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> arr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="归并排序">归并排序</h2>
<p>递归，如果用原位迭代空间复杂度为o(1)但时间复杂度到o(n2) <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] MergeSort(<span class="keyword">int</span>[] sourceArray) &#123;</span><br><span class="line">    <span class="keyword">int</span>[] arr = Arrays.copyOf(sourceArray, sourceArray.length);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (arr.length &lt; <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> arr;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> middle = arr.length &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span>[] left = Arrays.copyOfRange(arr, <span class="number">0</span>, middle);</span><br><span class="line">    <span class="keyword">int</span>[] right = Arrays.copyOfRange(arr, middle, arr.length);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> merge(MergeSort(left), MergeSort(right));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">int</span>[] merge(<span class="keyword">int</span>[] left, <span class="keyword">int</span>[] right) &#123;<span class="comment">// 两个有序序列</span></span><br><span class="line">    <span class="keyword">int</span>[] result = <span class="keyword">new</span> <span class="keyword">int</span>[left.length + right.length];</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (left.length &gt; <span class="number">0</span> &amp;&amp; right.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (left[<span class="number">0</span>] &lt;= right[<span class="number">0</span>]) &#123;</span><br><span class="line">            result[i] = left[<span class="number">0</span>];</span><br><span class="line">            i++;</span><br><span class="line">            left = Arrays.copyOfRange(left, <span class="number">1</span>, left.length);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            result[i] = right[<span class="number">0</span>];</span><br><span class="line">            i++;</span><br><span class="line">            right = Arrays.copyOfRange(right, <span class="number">1</span>, right.length);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (left.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        result[i] = left[<span class="number">0</span>];</span><br><span class="line">        i++;</span><br><span class="line">        left = Arrays.copyOfRange(left, <span class="number">1</span>, left.length);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (right.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        result[i] = right[<span class="number">0</span>];</span><br><span class="line">        i++;</span><br><span class="line">        right = Arrays.copyOfRange(right, <span class="number">1</span>, right.length);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h2 id="快速排序">快速排序</h2>
<ul>
<li>快速排序是对冒泡排序的一种改进。</li>
<li>时间复杂度并不固定，如果在最坏情况下（元素刚好是反向的）速度比较慢，达到 O(n^2)（和选择排序一个效率），但是如果在比较理想的情况下时间复杂度 O(nlogn)。</li>
</ul>
<figure>
<img src="https://mmbiz.qpic.cn/mmbiz_gif/D67peceibeISwc3aGibUlvZ0XqVnbWtBRiaAY3VU8iaziaYcxAasTdrIu69BOVPYtfvqdvicmlJDS94cG2tjwZhVkdHA/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="QuickSort" /><figcaption aria-hidden="true">QuickSort</figcaption>
</figure>
<ul>
<li>从数列中挑出一个元素，称为 “基准”（pivot）;</li>
<li>重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。</li>
<li>如果基准值<code>pivot</code>在最左，从<code>i=index=pivot+1</code>开始，每次遇到比<code>arr[index]</code>小的元素，就<code>swap(arr, i, index); index++</code>。<code>index</code>记录分隔符向右移动的情况，但此时pivot的位置还未改变，直到最后再<code>swap(arr), pivot, index-1</code>。</li>
<li>在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作；</li>
<li>递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序；</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] QuickSort(<span class="keyword">int</span>[] sourceArray) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = Arrays.copyOf(sourceArray, sourceArray.length);</span><br><span class="line">        <span class="keyword">return</span> quicksort(arr, <span class="number">0</span>, arr.length - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">int</span>[] quicksort(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> left, <span class="keyword">int</span> right) &#123;</span><br><span class="line">    <span class="keyword">if</span> (left &lt; right) &#123; <span class="comment">// 约束，防止partitionIndex到0</span></span><br><span class="line">        <span class="keyword">int</span> partitionIndex = partition(arr, left, right); <span class="comment">// partition实际执行排序操作</span></span><br><span class="line">        quicksort(arr, left, partitionIndex - <span class="number">1</span>); <span class="comment">// 不含中间</span></span><br><span class="line">        quicksort(arr, partitionIndex + <span class="number">1</span>, right);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> arr;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// partition方法对arr进行修改</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> pivot = left;</span><br><span class="line">    <span class="keyword">int</span> index = pivot + <span class="number">1</span>; <span class="comment">// 从左端开始</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = index; i &lt;= right; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (arr[i] &lt; arr[pivot]) &#123;</span><br><span class="line">            swap(arr, i, index); <span class="comment">// pivot右边一位作为index不断地和后续比arr[pivot]小的元素交换位置，</span></span><br><span class="line">            index++; <span class="comment">// index位向右挪</span></span><br><span class="line">            <span class="comment">// 最终pivot往右直到index-1都是小于基准的，再把pivot放到中间位置index-1</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    swap(arr, pivot, index - <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> index - <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> temp = arr[i];</span><br><span class="line">    arr[i] = arr[j];</span><br><span class="line">    arr[j] = temp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="堆排序">堆排序</h2>
<ul>
<li>堆：相较于完全二叉树，所有父节点的值大于（小于）子节点</li>
<li>最大堆的最大元素值出现在根结点（堆顶）</li>
<li>稳定性：不稳定</li>
<li>完全二叉树、二叉堆属于数据结构。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">heapify</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> i, <span class="keyword">int</span> len)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> left = <span class="number">2</span> * i + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> right = <span class="number">2</span> * i + <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">int</span> largest = i;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (left &lt; len &amp;&amp; arr[left] &gt; arr[largest] )&#123; <span class="comment">//注意次序，先判断index</span></span><br><span class="line">        largest = left;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (right &lt; len &amp;&amp; arr[right] &gt; arr[largest])&#123;</span><br><span class="line">        largest = right;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (i &lt; largest)&#123;</span><br><span class="line">        swap(arr, i, largest);</span><br><span class="line">        heapify(arr, largest, len);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">buildMaxHeap</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> len)</span></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = len &gt;&gt; <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--)&#123; <span class="comment">//i &gt;= 0</span></span><br><span class="line">        heapify(arr, i, len);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">int</span>[] heapSort(<span class="keyword">int</span>[] sourceArray)&#123;</span><br><span class="line">    <span class="keyword">int</span>[] arr = Arrays.copyOf(sourceArray, sourceArray.length);</span><br><span class="line">    <span class="keyword">int</span> len = arr.length;</span><br><span class="line"></span><br><span class="line">    buildMaxHeap(arr, len);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = len - <span class="number">1</span>; i &gt; <span class="number">0</span>; i--)&#123;</span><br><span class="line">        swap(arr, <span class="number">0</span>, i); <span class="comment">// 右下子节点和根节点交换，根节点最大值放到最后</span></span><br><span class="line">        len--;</span><br><span class="line">        heapify(arr, <span class="number">0</span>, len); <span class="comment">//调整堆</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> arr;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> temp = arr[i];</span><br><span class="line">    arr[i] = arr[j];</span><br><span class="line">    arr[j] = temp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="计数排序">计数排序</h2>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">int</span>[] CountingSort(<span class="keyword">int</span>[] sourceArray)&#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = Arrays.copyOf(sourceArray, sourceArray.length);</span><br><span class="line">        <span class="keyword">return</span> (countingsort(arr, getMaxValue(arr)));</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">static</span> <span class="keyword">int</span>[] countingsort(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> maxValue)&#123;</span><br><span class="line">    <span class="keyword">int</span> bucketlen = maxValue + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span>[] bucketarr = <span class="keyword">new</span> <span class="keyword">int</span>[bucketlen];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> value: arr)&#123;</span><br><span class="line">        bucketarr[value] ++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> sortedIndex = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; bucketlen; i++)&#123; <span class="comment">// i &lt; bucketlen</span></span><br><span class="line">        <span class="keyword">while</span> (bucketarr[i]&gt;<span class="number">0</span>)&#123;</span><br><span class="line">            arr[sortedIndex++] = i;</span><br><span class="line">            bucketarr[i]--;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> arr;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">getMaxValue</span><span class="params">(<span class="keyword">int</span>[] arr)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> maxValue = arr[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; arr.length; i++)&#123;</span><br><span class="line">        maxValue = arr[i] &gt; maxValue ? arr[i] : maxValue;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> maxValue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="桶排序">桶排序</h2>
<figure>
<img src="https://mmbiz.qpic.cn/mmbiz_gif/D67peceibeISwc3aGibUlvZ0XqVnbWtBRia0umOurgFRv8ESYzcK6bXknufLgJGLaiaTrDTXUk09R6ia2DhbCPAJx0A/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="BucketSort" /><figcaption aria-hidden="true">BucketSort</figcaption>
</figure>
<ul>
<li>设置固定数量的空桶。</li>
<li>把数据放到对应的桶中</li>
<li>对每个不为空的桶中数据进行排序</li>
<li>拼接不为空的桶中数据，得到结果</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BucketSort</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] a = &#123; <span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span> &#125;;</span><br><span class="line">        System.out.println(Arrays.toString(sort(a)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] sort(<span class="keyword">int</span>[] sourceArray) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = Arrays.copyOf(sourceArray, sourceArray.length);</span><br><span class="line">        <span class="keyword">return</span> bucketSort(arr, <span class="number">5</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//桶排序主体部分 </span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span>[] bucketSort(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> bucketSize) &#123;</span><br><span class="line">        <span class="keyword">if</span> (arr.length == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> arr;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> maxValue = arr[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">int</span> minValue = arr[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> value : arr) &#123;</span><br><span class="line">            maxValue = value &gt; maxValue ? value : maxValue;</span><br><span class="line">            minValue = value &lt; minValue ? value : minValue;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> bucketCount = (<span class="keyword">int</span>) Math.floor((maxValue - minValue) / bucketSize) + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span>[][] buckets = <span class="keyword">new</span> <span class="keyword">int</span>[bucketCount][<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> value : arr) &#123;</span><br><span class="line">            <span class="keyword">int</span> bucketIndex = (<span class="keyword">int</span>) Math.floor((value - minValue) / bucketSize);</span><br><span class="line">            buckets[bucketIndex] = arrayAppend(buckets[bucketIndex], value);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> sortedIndex = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; bucketCount; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (buckets[i].length == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">int</span>[] bucket = buckets[i];</span><br><span class="line">            bucket = InsertSort(bucket);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; bucket.length; j++) &#123;</span><br><span class="line">                arr[sortedIndex++] = bucket[j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> arr;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 每个桶内部用到插入排序</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span>[] InsertSort(<span class="keyword">int</span>[] sourceArray) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = Arrays.copyOf(sourceArray, sourceArray.length);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; arr.length; i++) &#123;</span><br><span class="line">            <span class="keyword">int</span> j = i;</span><br><span class="line">            <span class="keyword">int</span> temp = arr[j];</span><br><span class="line">            <span class="keyword">while</span> (j &gt; <span class="number">0</span> &amp;&amp; arr[j - <span class="number">1</span>] &gt; temp) &#123;</span><br><span class="line">                arr[j] = arr[j - <span class="number">1</span>];</span><br><span class="line">                j--;</span><br><span class="line">            &#125;</span><br><span class="line">            arr[j] = temp;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> arr;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> a, <span class="keyword">int</span> b)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> temp = arr[a];</span><br><span class="line">        arr[a] = arr[b];</span><br><span class="line">        arr[b] = temp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// array扩容</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span>[] arrayAppend(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> a) &#123;</span><br><span class="line">        arr = Arrays.copyOf(arr, arr.length + <span class="number">1</span>);</span><br><span class="line">        arr[arr.length - <span class="number">1</span>] = a;</span><br><span class="line">        <span class="keyword">return</span> arr;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="基数排序">基数排序</h2>
<ul>
<li>首先通过<code>getMaxValue</code>方法得出最大数</li>
<li>通过<code>getDigitCount</code>方法得出最大数的数位数，也就是全arr的最大digits数</li>
<li><code>getDigitValue</code>方法可以给出给定数的某数位的值</li>
<li>从低位（个位）开始，新建0-9十个空桶，按照arr中每个数的个位放入桶中，然后再依次放回到arr中</li>
<li>再进入下一位，新建十个空桶，重复操作</li>
<li>直到遍历MaxDigit</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RadixSort</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span>[] sort(<span class="keyword">int</span>[] sourceArray) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = Arrays.copyOf(sourceArray, sourceArray.length);</span><br><span class="line">        <span class="keyword">int</span> maxDigit = getMaxDigit(arr);</span><br><span class="line">        <span class="keyword">return</span> radixSort(arr, maxDigit);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span>[] radixSort(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> maxDigit) &#123;</span><br><span class="line">        <span class="comment">// 从个位比较开始数位 (低位)</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; maxDigit; i++) &#123;</span><br><span class="line">            <span class="comment">// 每次比较数位时初始化新的10个空桶</span></span><br><span class="line">            <span class="keyword">int</span>[][] buckets = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">10</span>][<span class="number">0</span>]; <span class="comment">// 0-9, 10 buckets, each store several values;</span></span><br><span class="line">            <span class="comment">// arr的值进入10个桶</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> value : arr) &#123;</span><br><span class="line">                buckets[getDigitValue(value, i)] = arrayAppend(buckets[getDigitValue(value, i)], value);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 从桶中把值拿出来放回arr</span></span><br><span class="line">            <span class="keyword">int</span> sortedIndex = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">10</span>; j++) &#123;</span><br><span class="line">                <span class="comment">// System.out.printf(&quot;buckets[%d]: %s&quot;, j, Arrays.toString(buckets[j]));</span></span><br><span class="line">                <span class="keyword">if</span> (buckets[j].length == <span class="number">0</span>)</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> value : buckets[j]) &#123;</span><br><span class="line">                    arr[sortedIndex++] = value;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> arr;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 个位为第0位</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">getDigitValue</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> digit)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> digitCount = getDigitCount(a);</span><br><span class="line">        <span class="keyword">if</span> (digit &gt; digitCount - <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; digit; i++) &#123;</span><br><span class="line">            a /= <span class="number">10</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> a % <span class="number">10</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">getMaxDigit</span><span class="params">(<span class="keyword">int</span>[] arr)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> maxValue = getMaxValue(arr);</span><br><span class="line">        <span class="keyword">return</span> getDigitCount(maxValue);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">getDigitCount</span><span class="params">(<span class="keyword">int</span> a)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> digitCount = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (a &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            a /= <span class="number">10</span>;</span><br><span class="line">            digitCount++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> digitCount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">getMaxValue</span><span class="params">(<span class="keyword">int</span>[] arr)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> maxValue = arr[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> value : arr) &#123;</span><br><span class="line">            maxValue = value &gt; maxValue ? value : maxValue;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> maxValue;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span>[] arrayAppend(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> a) &#123;</span><br><span class="line">        arr = Arrays.copyOf(arr, arr.length + <span class="number">1</span>);</span><br><span class="line">        arr[arr.length - <span class="number">1</span>] = a;</span><br><span class="line">        <span class="keyword">return</span> arr;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] a = &#123; <span class="number">9</span>, <span class="number">28</span>, <span class="number">7</span>, <span class="number">345</span>, <span class="number">579</span>, <span class="number">42</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">101</span> &#125;;</span><br><span class="line">        System.out.println(Arrays.toString(sort(a)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Java数据结构与算法</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>跨式Straddle</title>
    <url>/2021/01/01/%E8%B7%A8%E5%BC%8FStraddle/</url>
    <content><![CDATA[<h2 id="跨式期权straddle简述">跨式期权(Straddle)简述</h2>
<p>Systematic Trading P41 提及，做空 <strong>跨式期权(Straddle)</strong> 是一种负偏的交易策略。</p>
<p>Barings 的 Nick Leeson 在1995年 short straddle 损失 $1B。 <a href="https://www.investopedia.com/ask/answers/08/nick-leeson-barings-bank.asp">How Did Nick Leeson Contribute To The Fall of Barings Bank?</a></p>
<p>使用场景：预测股价有大幅波动，但不知道方向。当价格波动大时获利。但也要考虑到在出现可能的价格波动时期权价格也会上升。</p>
<p><img src="https://upload.wikimedia.org/wikipedia/en/5/57/Longstraddle.png" /></p>
<p>以上的"V"字形跨式期权称作 底部跨式期权(bottom straddle) 或 买入跨式 (straddle purchase)。 做空这种期权叫做 顶部跨式期权(top straddle) / 卖出跨式 (straddle write)，呈倒V形，显然是负偏的交易策略 (损失有长尾)。</p>
<h2 id="参考">参考</h2>
<ul>
<li><p><a href="https://xueqiu.com/1216599649/43051495">什么是跨式期权(Straddle)策略</a></p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Straddle">Straddle - Wiki</a></p></li>
</ul>
]]></content>
      <categories>
        <category>Finance</category>
      </categories>
      <tags>
        <tag>Financial Engineering</tag>
      </tags>
  </entry>
  <entry>
    <title>动态规划</title>
    <url>/2020/11/18/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
    <content><![CDATA[<h2 id="题目">题目</h2>
<h3 id="买卖股票的最佳时机">(0908)买卖股票的最佳时机</h3>
<p>知识点：数组，动态规划</p>
<p>难度：简单</p>
<p>题目描述：</p>
<ul>
<li>假设你有一个数组，其中第 i 个元素是股票在第 i 天的价格。</li>
<li>你有一次买入和卖出的机会。（只有买入了股票以后才能卖出）。请你设计一个算法来计算可以获得的最大收益。</li>
<li>示例1</li>
<li>输入 [1,4,2]</li>
<li>输出 3</li>
</ul>
<p>代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">maxProfit</span><span class="params">(<span class="keyword">int</span>[] prices)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> buy = Integer.MIN_VALUE;</span><br><span class="line">    <span class="keyword">int</span> sell = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; prices.length; i++)&#123;</span><br><span class="line">        buy = Math.max(buy, -prices[i]);</span><br><span class="line">        sell = Math.max(sell, prices[i] + buy);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> sell;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="带权值的最小路径和">(0908)带权值的最小路径和</h3>
<p>知识点：数组，动态规划（矩阵）</p>
<p>难度：中等</p>
<p>题目描述：</p>
<ul>
<li>给定一个由非负整数填充的m x n的二维数组，现在要从二维数组的左上角走到右下角，请找出路径上的所有数字之和最小的路径。注意：你每次只能向下或向右移动。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">minPathSum</span><span class="params">(<span class="keyword">int</span>[][] grid)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// write code here</span></span><br><span class="line">    <span class="keyword">int</span> rows = grid.length;</span><br><span class="line">    <span class="keyword">int</span> columns = grid[<span class="number">0</span>].length;</span><br><span class="line">    <span class="keyword">int</span>[][] dp = <span class="keyword">new</span> <span class="keyword">int</span>[rows + <span class="number">1</span>][columns + <span class="number">1</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= rows; i++) &#123;</span><br><span class="line">        dp[i][<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= columns; j++) &#123;</span><br><span class="line">        dp[<span class="number">0</span>][j] = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; rows; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; columns; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (i != <span class="number">0</span> &amp;&amp; j != <span class="number">0</span>) &#123;</span><br><span class="line">                dp[i + <span class="number">1</span>][j + <span class="number">1</span>] = Math.min(dp[i][j + <span class="number">1</span>], dp[i + <span class="number">1</span>][j]) + grid[i][j];</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (i == <span class="number">0</span>) &#123;</span><br><span class="line">                dp[i + <span class="number">1</span>][j + <span class="number">1</span>] = dp[i + <span class="number">1</span>][j] + grid[i][j];</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (j == <span class="number">0</span>) &#123;</span><br><span class="line">                dp[i + <span class="number">1</span>][j + <span class="number">1</span>] = dp[i][j + <span class="number">1</span>] + grid[i][j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[rows][columns];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="子数组的最大累加和">(0909)子数组的最大累加和</h3>
<ul>
<li><p>知识点：分治，动态规划</p></li>
<li><p>难度：简单</p></li>
<li><p>题目描述：</p>
<ul>
<li>给定一个数组arr，返回子数组的最大累加和</li>
<li>例如，arr = [1, -2, 3, 5, -2, 6, -1]，所有子数组中，[3, 5, -2, 6]可以累加出最大的和12，所以返回12.</li>
<li>要求时间复杂度为<span class="math inline">\(O(n)\)</span>，空间复杂度为<span class="math inline">\(O(1)\)</span></li>
</ul></li>
<li><p>理解：问题分为：到某一位为止的最优累加和，和整个数列不同位置为终点的累加和中的最大值(max)。</p></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">maxsumofSubarray</span> <span class="params">(<span class="keyword">int</span>[] arr)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// write code here</span></span><br><span class="line">    <span class="keyword">if</span> (arr.length == <span class="number">1</span>) <span class="keyword">return</span> arr[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">int</span> currSum = arr[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">int</span> maxSum = Integer.MIN_VALUE;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; arr.length; i++)&#123;</span><br><span class="line">        <span class="keyword">if</span> (currSum &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">            currSum = arr[i];</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            currSum += arr[i];</span><br><span class="line">        &#125;</span><br><span class="line">        maxSum = currSum &gt; maxSum ? currSum : maxSum;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> maxSum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="lis最长递增子序列现解法超时">(0911)LIS最长递增子序列（现解法超时）</h3>
<ul>
<li><p>知识点：<strong>二分</strong>，动态规划</p></li>
<li><p>难度：中等</p></li>
<li><p>题目描述：</p>
<ul>
<li>给定数组arr，设长度为n，输出arr的最长递增子序列。（如果有多个答案，请输出其中字典序最小的）</li>
</ul></li>
<li><p>目前解法：</p>
<ul>
<li><code>arrayCompare</code>: 先比较长度，再比较字典序</li>
<li><code>DP[length]</code>: 逐步更新，长度为length的所有子序列中最优解</li>
<li>从arr第一位到最后一位，每次检查<strong>以当前位为最后一位</strong>的最长递增子序列</li>
<li>检查的方法<code>searchInsert</code>：对之前的所有<code>DP[length] (1&lt;=length&lt;=maxlength)</code>遍历，插入当前位<code>arr[i]</code>并舍去大于当前位的部分。这部分用到<code>二分查找</code>。</li>
</ul></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NowCoder_LIS</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] LIS(<span class="keyword">int</span>[] arr) &#123;</span><br><span class="line">        <span class="keyword">int</span>[][] DP = <span class="keyword">new</span> <span class="keyword">int</span>[arr.length][];</span><br><span class="line">        <span class="comment">// 括号内表示LIS长度</span></span><br><span class="line">        DP[<span class="number">0</span>] = <span class="keyword">new</span> <span class="keyword">int</span>[] &#123;&#125;;</span><br><span class="line">        DP[<span class="number">1</span>] = <span class="keyword">new</span> <span class="keyword">int</span>[] &#123; arr[<span class="number">0</span>] &#125;;</span><br><span class="line">        <span class="keyword">int</span> maxlength = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span>[] bestarr = DP[maxlength];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; arr.length; i++) &#123;</span><br><span class="line">            bestarr = searchInsert(DP[maxlength], arr[i]);</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = maxlength-<span class="number">1</span>; j &gt;= <span class="number">1</span>; j--)&#123;</span><br><span class="line">                bestarr = arrayCompare(bestarr, searchInsert(DP[j], arr[i]));</span><br><span class="line">            &#125;</span><br><span class="line">            DP[bestarr.length] = bestarr;</span><br><span class="line">            <span class="keyword">if</span> (bestarr.length &gt; maxlength) maxlength=bestarr.length;</span><br><span class="line">            System.out.printf(<span class="string">&quot;arr[%d]: %d, DP[%d] : %s \n&quot;</span>, i, arr[i], maxlength, Arrays.toString(DP[maxlength]));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> DP[maxlength];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] arrayCompare(<span class="keyword">int</span>[] a1, <span class="keyword">int</span>[] a2) &#123;</span><br><span class="line">        <span class="keyword">if</span> (a1.length != a2.length) &#123;</span><br><span class="line">            <span class="keyword">return</span> a1.length &gt; a2.length ? a1 : a2;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// return (Arrays.compare(a1, a2) == 0) ? a1 : a2;</span></span><br><span class="line">            <span class="keyword">return</span> (Arrays.toString(a1).compareTo(Arrays.toString(a2)) == <span class="number">0</span>) ? a1 : a2;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] searchInsert(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> a) &#123;</span><br><span class="line">        <span class="keyword">if</span> (arr.length == <span class="number">0</span> || a &lt; arr[<span class="number">0</span>]) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[] &#123; a &#125;;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (a &gt;= arr[arr.length - <span class="number">1</span>]) &#123;</span><br><span class="line">            <span class="keyword">return</span> arrayAppend(arr, a);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">int</span> right = arr.length - <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">while</span> (left + <span class="number">1</span> != right)&#123;</span><br><span class="line">                <span class="keyword">int</span> mid = (left + right) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">if</span> (arr[mid] &gt; a)&#123;</span><br><span class="line">                    right = mid;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    left = mid;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">int</span>[] result = Arrays.copyOfRange(arr, <span class="number">0</span>, right+<span class="number">1</span>);</span><br><span class="line">            result[result.length - <span class="number">1</span>] = a;</span><br><span class="line">            <span class="keyword">return</span> result;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] arrayAppend(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> a) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] result = Arrays.copyOf(arr, arr.length + <span class="number">1</span>);</span><br><span class="line">        result[result.length - <span class="number">1</span>] = a;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = &#123;<span class="number">5</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">13</span>, <span class="number">12</span>, <span class="number">11</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">15</span>, <span class="number">14</span>, <span class="number">7</span>,&#125;;</span><br><span class="line">        System.out.println(Arrays.toString(LIS(arr)));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="最大正方形">(0911)最大正方形</h3>
<ul>
<li><p>初始化，<code>DP = mat</code></p></li>
<li><p>遍历，<code>包含第[i][j]个结点的最大正方形</code></p></li>
<li><p>状态转移方程:</p>
<p><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">if mat[<span class="string">i</span>][<span class="symbol">j</span>] == 1&#123;</span><br><span class="line"><span class="code">    dp[i][j] = min(dp[i-1][j-1], dp[i-1][j], dp[i][j-1]) + 1;</span></span><br><span class="line"><span class="code">&#125;</span></span><br></pre></td></tr></table></figure></p></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NowCoder_BiggestSquare</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">char</span>[][] mat = &#123; &#123; <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;0&#x27;</span> &#125;, &#123; <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;1&#x27;</span> &#125;, &#123; <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;1&#x27;</span> &#125;,</span><br><span class="line">                &#123; <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;0&#x27;</span> &#125; &#125;;</span><br><span class="line">        System.out.println(solve(mat));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">solve</span><span class="params">(<span class="keyword">char</span>[][] matrix)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// write code here</span></span><br><span class="line">        <span class="keyword">if</span> (matrix.length == <span class="number">0</span> || matrix[<span class="number">0</span>].length == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> m = matrix.length;</span><br><span class="line">        <span class="keyword">int</span> n = matrix[<span class="number">0</span>].length;</span><br><span class="line">        <span class="keyword">int</span> max = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span>[][] dp = <span class="keyword">new</span> <span class="keyword">int</span>[m][n];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; n; j++) &#123;</span><br><span class="line">                dp[i][j] = matrix[i][j] - <span class="string">&#x27;0&#x27;</span>;</span><br><span class="line">                <span class="keyword">if</span> (dp[i][j] == <span class="number">1</span>) &#123;</span><br><span class="line">                    max = <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; m; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt; n; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (matrix[i][j] == <span class="string">&#x27;1&#x27;</span>) &#123;</span><br><span class="line">                    dp[i][j] = Math.min(dp[i - <span class="number">1</span>][j - <span class="number">1</span>], Math.min(dp[i - <span class="number">1</span>][j], dp[i][j - <span class="number">1</span>])) + <span class="number">1</span>;</span><br><span class="line">                    max = Math.max(dp[i][j], max);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i++)&#123;</span><br><span class="line">            System.out.println(Arrays.toString(dp[i]));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> max * max;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="nc127最长公共子串-lcs">(1117) NC127最长公共子串 LCS</h3>
<ul>
<li>DP[i][j]: 以第i、j个字符为结尾的最长公共子串</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NC127</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * longest common substring</span></span><br><span class="line"><span class="comment">     * </span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> str1 string字符串 the string</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> str2 string字符串 the string</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> string字符串</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">LCS</span><span class="params">(String str1, String str2)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// write code here</span></span><br><span class="line">        <span class="keyword">int</span>[][] DP = <span class="keyword">new</span> <span class="keyword">int</span>[str1.length() + <span class="number">1</span>][str2.length() + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span> startIdx = <span class="number">0</span>, endIdx = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> maxLen = <span class="number">0</span>;</span><br><span class="line">        String result;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; str1.length() + <span class="number">1</span>; i++) &#123;</span><br><span class="line">            DP[i][<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; str2.length() + <span class="number">1</span>; i++) &#123;</span><br><span class="line">            DP[<span class="number">0</span>][i] = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; str1.length() + <span class="number">1</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt; str2.length() + <span class="number">1</span>; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (str1.charAt(i - <span class="number">1</span>) == str2.charAt(j - <span class="number">1</span>)) &#123;</span><br><span class="line">                    DP[i][j] = DP[i - <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">1</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    DP[i][j] = Math.min(DP[i - <span class="number">1</span>][j], DP[i][j - <span class="number">1</span>]);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (DP[i][j] &gt;= maxLen) &#123;</span><br><span class="line">                    maxLen = DP[i][j];</span><br><span class="line">                    endIdx = i;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        startIdx = endIdx - maxLen + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        System.out.printf(<span class="string">&quot;startIdx: %d, endIdx: %d \n&quot;</span>, startIdx, endIdx);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; str1.length() + <span class="number">1</span>; i++) &#123;</span><br><span class="line">            System.out.println(Arrays.toString(DP[i]));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        result = str1.substring(startIdx - <span class="number">1</span>, endIdx);</span><br><span class="line">        <span class="keyword">if</span> (result.length() == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;-1&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String str1 = <span class="string">&quot;1AB23456HCDEFG&quot;</span>, str2 = <span class="string">&quot;123456QCDEFG&quot;</span>;</span><br><span class="line">        System.out.println(LCS(str1, str2));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>样例结果 <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">startIdx:</span> <span class="number">10</span><span class="string">,</span> <span class="attr">endIdx:</span> <span class="number">14</span> </span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>]</span><br><span class="line"><span class="string">CDEFG</span></span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>Java数据结构与算法</category>
      </categories>
      <tags>
        <tag>Dynamic Programming (DP)</tag>
      </tags>
  </entry>
  <entry>
    <title>递归</title>
    <url>/2020/09/19/%E9%80%92%E5%BD%92/</url>
    <content><![CDATA[<h3 id="集合的所有子集">(0918) 集合的所有子集</h3>
<p>题目描述</p>
<p>现在有一个没有重复元素的整数集合S，求S的所有子集</p>
<p>注意： - 你给出的子集中的元素必须按升序排列 - 给出的解集中不能出现重复的元素 - 例如：如果S=[1,2,3], 给出的解集应为： [ [3], [1], [2], [1,2,3], [1,3], [2,3], [1,2], []]</p>
<p>解法： - 递归，集合每增加一个新元素，先前所有非空子集 + 这些非空子集加入新元素生成的新子集 + 该元素本身形成的集合 - 边界条件：空集合需要包含一个空子集 - 注意：递归时非空子集的运算屏蔽空子集 (continue) ，但每次返回时增加一个空子集。</p>
<p>代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NowCoder_subsets</span> </span>&#123;</span><br><span class="line">    <span class="comment">/** 集合的所有子集 */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> ArrayList&lt;ArrayList&lt;Integer&gt;&gt; subsets(<span class="keyword">int</span>[] S) &#123;</span><br><span class="line">        <span class="comment">// 包含空子集</span></span><br><span class="line">        <span class="keyword">if</span> (S.length == <span class="number">0</span>) &#123;</span><br><span class="line">            ArrayList&lt;Integer&gt; empty = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            ArrayList&lt;ArrayList&lt;Integer&gt;&gt; result = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            result.add(empty);</span><br><span class="line">            <span class="keyword">return</span> result;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;ArrayList&lt;Integer&gt;&gt; result = <span class="keyword">new</span> ArrayList&lt;ArrayList&lt;Integer&gt;&gt;();</span><br><span class="line">        <span class="keyword">int</span>[] S_m1 = Arrays.copyOfRange(S, <span class="number">0</span>, S.length - <span class="number">1</span>);</span><br><span class="line">        ArrayList&lt;ArrayList&lt;Integer&gt;&gt; result_m1 = subsets(S_m1);</span><br><span class="line">        <span class="keyword">for</span> (ArrayList&lt;Integer&gt; subset_m1 : result_m1) &#123;</span><br><span class="line">            <span class="keyword">if</span> (subset_m1.size() == <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">continue</span>; <span class="comment">// 空子集暂时不管</span></span><br><span class="line">            result.add(subset_m1); <span class="comment">// 之前的也要加上</span></span><br><span class="line">            ArrayList&lt;Integer&gt; subset = (ArrayList&lt;Integer&gt;) subset_m1.clone();</span><br><span class="line">            subset.add(S[S.length - <span class="number">1</span>]);</span><br><span class="line">            Collections.sort(subset);</span><br><span class="line">            result.add(subset);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 加最后一位的一元子集</span></span><br><span class="line">        ArrayList&lt;Integer&gt; lastDigit = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        lastDigit.add(S[S.length - <span class="number">1</span>]);</span><br><span class="line">        result.add(lastDigit);</span><br><span class="line">        <span class="comment">// 加空集</span></span><br><span class="line">        ArrayList&lt;Integer&gt; empty = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        result.add(empty);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] S = &#123; <span class="number">1</span> &#125;;</span><br><span class="line">        ArrayList&lt;ArrayList&lt;Integer&gt;&gt; subset = subsets(S);</span><br><span class="line">        <span class="keyword">for</span> (ArrayList&lt;Integer&gt; i : subset)</span><br><span class="line">            System.out.println(Arrays.toString(i.toArray()));</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="二叉树层序遍历">(0919) 二叉树层序遍历</h3>
<p>题目描述 给定一个二叉树，返回该二叉树层序遍历的结果，（从左到右，一层一层地遍历）例如：给定的二叉树是{3,9,20,#,#,15,7},该二叉树层序遍历的结果是</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">[</span><br><span class="line">[<span class="number">3</span>],</span><br><span class="line">[<span class="number">9</span>,<span class="number">20</span>],</span><br><span class="line">[<span class="number">15</span>,<span class="number">7</span>]</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * public class TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *   int val = 0;</span></span><br><span class="line"><span class="comment"> *   TreeNode left = null;</span></span><br><span class="line"><span class="comment"> *   TreeNode right = null;</span></span><br><span class="line"><span class="comment"> * &#125;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NowCoder_LevelOrder</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * </span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> root TreeNode类</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> int整型ArrayList&lt;ArrayList&lt;&gt;&gt;</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> ArrayList&lt;ArrayList&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123;</span><br><span class="line">        <span class="comment">// write code here</span></span><br><span class="line">        ArrayList&lt;Integer&gt; width = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        ArrayList&lt;ArrayList&lt;Integer&gt;&gt; result = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> result;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (root.left == <span class="keyword">null</span> &amp;&amp; root.right == <span class="keyword">null</span>) &#123;</span><br><span class="line">            width.add(root.val);</span><br><span class="line">            result.add(width);</span><br><span class="line">            <span class="keyword">return</span> result;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> depth = treeDepth(root);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; depth; i++) &#123;</span><br><span class="line">            result.add(ithlevel(root, i));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 确定二叉树深度</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">treeDepth</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">return</span> Math.max(treeDepth(root.left), treeDepth(root.right)) + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 递归确定二叉树每层width的所有结点</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ArrayList&lt;Integer&gt; <span class="title">ithlevel</span><span class="params">(TreeNode root, <span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">        ArrayList&lt;Integer&gt; width = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">return</span> width;</span><br><span class="line">        <span class="keyword">if</span> (i == <span class="number">0</span>) &#123;</span><br><span class="line">            width.add(root.val);</span><br><span class="line">            <span class="keyword">return</span> width;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            ArrayList&lt;Integer&gt; leftwidth = ithlevel(root.left, i - <span class="number">1</span>);</span><br><span class="line">            ArrayList&lt;Integer&gt; rightwidth = ithlevel(root.right, i - <span class="number">1</span>);</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> val : rightwidth) &#123;</span><br><span class="line">                leftwidth.add(val);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> leftwidth;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Java数据结构与算法</category>
      </categories>
  </entry>
  <entry>
    <title>链表题(持续更新)</title>
    <url>/2020/09/03/%E9%93%BE%E8%A1%A8%E9%A2%98-%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0/</url>
    <content><![CDATA[<h2 id="链表类listnode定义">链表类<code>ListNode</code>定义</h2>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ListNode</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> val;</span><br><span class="line">    ListNode next;</span><br><span class="line">    ListNode(<span class="keyword">int</span> x) &#123;</span><br><span class="line">        val = x;</span><br><span class="line">        next = <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] toArray(ListNode head)&#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = &#123;&#125;;</span><br><span class="line">        <span class="keyword">while</span> (head != <span class="keyword">null</span>)&#123;</span><br><span class="line">            arr = Arrays.copyOf(arr, arr.length+<span class="number">1</span>);</span><br><span class="line">            arr[arr.length-<span class="number">1</span>] = head.val;</span><br><span class="line">            head = head.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> arr;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ListNode <span class="title">fromArray</span><span class="params">(<span class="keyword">int</span>[] arr)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (arr.length == <span class="number">0</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        ListNode ll = <span class="keyword">new</span> ListNode(arr[<span class="number">0</span>]);</span><br><span class="line">        ListNode head = ll;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; arr.length; i++)&#123;</span><br><span class="line">            ll.next = <span class="keyword">new</span> ListNode(arr[i]);</span><br><span class="line">            ll = ll.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> head;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>提供了导入导出为数组的内置静态方法。</p>
<h2 id="问题">问题</h2>
<h3 id="反转链表">反转链表</h3>
<p>题目描述：输入一个链表，反转链表后，输出新链表的表头。</p>
<p>代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">public class ListNode &#123;</span></span><br><span class="line"><span class="comment">    int val;</span></span><br><span class="line"><span class="comment">    ListNode next = null;</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    ListNode(int val) &#123;</span></span><br><span class="line"><span class="comment">        this.val = val;</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">&#125;*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> ListNode <span class="title">ReverseList</span><span class="params">(ListNode head)</span> </span>&#123;</span><br><span class="line">        ListNode prev, curr, next, result;</span><br><span class="line">        prev = <span class="keyword">null</span>;</span><br><span class="line">        curr = head;</span><br><span class="line">        result = curr;</span><br><span class="line">        <span class="keyword">while</span> (curr != <span class="keyword">null</span>)&#123;</span><br><span class="line">            next = curr.next;</span><br><span class="line">            curr.next = prev;</span><br><span class="line">            result = curr;</span><br><span class="line">            prev = curr;</span><br><span class="line">            curr = next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 更简洁的</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ListNode <span class="title">reverseList</span><span class="params">(ListNode head)</span></span>&#123;</span><br><span class="line">    ListNode prev, curr, next;</span><br><span class="line">    prev = <span class="keyword">null</span>; curr = head; </span><br><span class="line">    <span class="keyword">while</span>(curr != <span class="keyword">null</span>)&#123;</span><br><span class="line">        next = curr.next;</span><br><span class="line">        curr.next= prev; </span><br><span class="line">        prev = curr;</span><br><span class="line">        curr = next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> prev;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="链表有环">链表有环</h3>
<p>题目描述：判断给定的链表中是否有环 扩展：你能给出空间复杂度<span class="math inline">\(O(1)\)</span>的解法么？</p>
<p>解法: 快慢指针</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"> * class ListNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     ListNode next;</span></span><br><span class="line"><span class="comment"> *     ListNode(int x) &#123;</span></span><br><span class="line"><span class="comment"> *         val = x;</span></span><br><span class="line"><span class="comment"> *         next = null;</span></span><br><span class="line"><span class="comment"> *     &#125;</span></span><br><span class="line"><span class="comment"> * &#125;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasCycle</span><span class="params">(ListNode head)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (head == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        ListNode fast, slow;</span><br><span class="line">        fast = head;</span><br><span class="line">        slow = head;</span><br><span class="line">        <span class="keyword">while</span>(fast != <span class="keyword">null</span> &amp;&amp; fast.next != <span class="keyword">null</span>)&#123;</span><br><span class="line">            fast = fast.next.next;</span><br><span class="line">            slow = slow.next;</span><br><span class="line">            <span class="keyword">if</span> (fast == slow) <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="合并有序列表">合并有序列表</h3>
<p>题目描述：将两个有序的链表合并为一个新链表，要求新的链表是通过拼接两个链表的节点来生成的</p>
<p>解法: 类似归并排序的思路</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LL</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ListNode <span class="title">mergeTwoLists</span> <span class="params">(ListNode l1, ListNode l2)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// write code here</span></span><br><span class="line">        <span class="keyword">if</span> (l1 == <span class="keyword">null</span>) <span class="keyword">return</span> l2;</span><br><span class="line">        <span class="keyword">if</span> (l2 == <span class="keyword">null</span>) <span class="keyword">return</span> l1;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Initialize </span></span><br><span class="line">        ListNode last;</span><br><span class="line">        <span class="keyword">if</span> (l1.val &lt;= l2.val) &#123;</span><br><span class="line">            last = l1;</span><br><span class="line">            l1 = l1.next;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            last = l2;</span><br><span class="line">            l2 = l2.next;</span><br><span class="line">        &#125;</span><br><span class="line">        ListNode result = last;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(l1 != <span class="keyword">null</span> &amp;&amp; l2 != <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="comment">// System.out.printf(&quot;l1.val &#123;%d&#125;, l2.val &#123;%d&#125;\n&quot;, l1.val, l2.val);</span></span><br><span class="line">            <span class="keyword">if</span> (l1.val &lt;= l2.val)&#123;</span><br><span class="line">                last.next = l1;</span><br><span class="line">                l1 = l1.next;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                last.next = l2;</span><br><span class="line">                l2 = l2.next;</span><br><span class="line">            &#125;</span><br><span class="line">            last = last.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(l1 != <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="comment">// System.out.printf(&quot;l1.val &#123;%d&#125;\n&quot;, l1.val);</span></span><br><span class="line">            last.next = l1;</span><br><span class="line">            l1 = l1.next;</span><br><span class="line">            last = last.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(l2 != <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="comment">// System.out.printf(&quot;l2.val &#123;%d&#125;\n&quot;, l2.val);</span></span><br><span class="line">            last.next = l2;</span><br><span class="line">            l2 = l2.next;</span><br><span class="line">            last = last.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] a1 = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>&#125;;</span><br><span class="line">        <span class="keyword">int</span>[] a2 = &#123;<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</span><br><span class="line"></span><br><span class="line">        ListNode l1 = ListNode.fromArray(a1);</span><br><span class="line">        ListNode l2 = ListNode.fromArray(a2);</span><br><span class="line">        ListNode merged = mergeTwoLists(l1, l2);</span><br><span class="line">        System.out.println(Arrays.toString(ListNode.toArray(merged)));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="删除链表的倒数第n个节点">(9.7)删除链表的倒数第n个节点</h3>
<p>题目描述：</p>
<ul>
<li><p>给定一个链表，删除链表的倒数第n个节点并返回链表的头指针</p></li>
<li><p>例如， 给出的链表为:1-&gt;2-&gt;3-&gt;4-&gt;5, n= 2. 删除了链表的倒数第n个节点之后,链表变为1-&gt;2-&gt;3-&gt;5.</p></li>
<li><p>备注：题目保证n一定是有效的</p></li>
<li><p>请给出时间复杂度为 <span class="math inline">\(O(n)\)</span> 的算法</p></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ListNode <span class="title">removeNthFromEnd</span><span class="params">(ListNode head, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// write code here</span></span><br><span class="line">    ListNode prev, curr, next;</span><br><span class="line">    <span class="keyword">int</span> len = <span class="number">0</span>;</span><br><span class="line">    curr = head;</span><br><span class="line">    <span class="keyword">while</span> (curr != <span class="keyword">null</span>) &#123;</span><br><span class="line">        curr = curr.next;</span><br><span class="line">        len++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 去除head</span></span><br><span class="line">    <span class="keyword">if</span> (n == len) &#123;</span><br><span class="line">        <span class="keyword">return</span> head.next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    curr = head;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len - n - <span class="number">1</span>; i++) &#123;</span><br><span class="line">        curr = curr.next;</span><br><span class="line">    &#125;</span><br><span class="line">    prev = curr;</span><br><span class="line">    curr = curr.next;</span><br><span class="line">    next = curr.next;</span><br><span class="line">    prev.next = next;</span><br><span class="line">    <span class="keyword">return</span> head;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="两个链表生成相加链表">(9.7)两个链表生成相加链表</h3>
<p>题目描述：</p>
<ul>
<li>假设链表中每一个节点的值都在 0 - 9 之间，那么链表整体就可以代表一个整数。</li>
<li>给定两个这种链表，请生成代表两个整数相加值的结果链表。</li>
<li>例如：链表 1 为 9-&gt;3-&gt;7，链表 2 为 6-&gt;3，最后生成新的结果链表为 1-&gt;0-&gt;0-&gt;0。</li>
</ul>
<p>分析：</p>
<p>用到了之前的<strong>反转链表</strong>，以及数组相加的策略（进位）。链表增加结点需要考虑初始化的情况。最重要的还是确定哪一步同步current和result两个链表。 讨论区反映java卡75%算例，超时。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">addInList0907</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ListNode <span class="title">addInList</span><span class="params">(ListNode head1, ListNode head2)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// write code here</span></span><br><span class="line">        ListNode head1Rev = reverseList(head1);</span><br><span class="line">        ListNode head2Rev = reverseList(head2);</span><br><span class="line">        ListNode mergeRevCurr = <span class="keyword">null</span>;</span><br><span class="line">        ListNode mergeRev = mergeRevCurr;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> cache = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> digit = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">boolean</span> init = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (head1Rev != <span class="keyword">null</span> &amp;&amp; head2Rev != <span class="keyword">null</span>) &#123;</span><br><span class="line">            digit = head1Rev.val + head2Rev.val + cache;</span><br><span class="line">            <span class="keyword">if</span> (digit &gt;= <span class="number">10</span>) &#123;</span><br><span class="line">                digit -= <span class="number">10</span>;</span><br><span class="line">                cache = <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                cache = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            ListNode currDigit = <span class="keyword">new</span> ListNode(digit);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (init) &#123;</span><br><span class="line">                init = <span class="keyword">false</span>;</span><br><span class="line">                mergeRevCurr = currDigit; <span class="comment">// 这一步同步</span></span><br><span class="line">                mergeRev = mergeRevCurr;</span><br><span class="line">                <span class="comment">// System.out.println(Arrays.toString(ListNode.toArray(mergeRev)));</span></span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                mergeRevCurr.next = currDigit;</span><br><span class="line">                mergeRevCurr = mergeRevCurr.next;</span><br><span class="line">            &#125;</span><br><span class="line">            head1Rev = head1Rev.next;</span><br><span class="line">            head2Rev = head2Rev.next;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (head1Rev != <span class="keyword">null</span>) &#123;</span><br><span class="line">            digit = head1Rev.val + cache;</span><br><span class="line">            <span class="keyword">if</span> (digit &gt;= <span class="number">10</span>) &#123;</span><br><span class="line">                digit -= <span class="number">10</span>;</span><br><span class="line">                cache = <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                cache = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            ListNode currDigit = <span class="keyword">new</span> ListNode(digit);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (init)</span><br><span class="line">                mergeRevCurr = currDigit;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                mergeRevCurr.next = currDigit;</span><br><span class="line">                mergeRevCurr = mergeRevCurr.next;</span><br><span class="line">            &#125;</span><br><span class="line">            head1Rev = head1Rev.next;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (head2Rev != <span class="keyword">null</span>) &#123;</span><br><span class="line">            digit = head2Rev.val + cache;</span><br><span class="line">            <span class="keyword">if</span> (digit &gt;= <span class="number">10</span>) &#123;</span><br><span class="line">                digit -= <span class="number">10</span>;</span><br><span class="line">                cache = <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                cache = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            ListNode currDigit = <span class="keyword">new</span> ListNode(digit);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (init)</span><br><span class="line">                mergeRevCurr = currDigit;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                mergeRevCurr.next = currDigit;</span><br><span class="line">                mergeRevCurr = mergeRevCurr.next;</span><br><span class="line">            &#125;</span><br><span class="line">            head2Rev = head2Rev.next;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (cache == <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="comment">// System.out.println(Arrays.toString(ListNode.toArray(mergeRev)));</span></span><br><span class="line">            ListNode currDigit = <span class="keyword">new</span> ListNode(<span class="number">1</span>);</span><br><span class="line">            mergeRevCurr.next = currDigit;</span><br><span class="line">            mergeRevCurr = mergeRevCurr.next;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> reverseList(mergeRev);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ListNode <span class="title">reverseList</span><span class="params">(ListNode head)</span> </span>&#123;</span><br><span class="line">        ListNode prev, curr, next;</span><br><span class="line">        prev = <span class="keyword">null</span>;</span><br><span class="line">        curr = head;</span><br><span class="line">        <span class="keyword">while</span> (curr != <span class="keyword">null</span>) &#123;</span><br><span class="line">            next = curr.next;</span><br><span class="line">            curr.next = prev;</span><br><span class="line">            prev = curr;</span><br><span class="line">            curr = next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> prev;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        ListNode l1 = ListNode.fromArray(<span class="keyword">new</span> <span class="keyword">int</span>[] &#123; <span class="number">5</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">9</span>,</span><br><span class="line">                <span class="number">5</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">0</span> &#125;);</span><br><span class="line">        ListNode l2 = ListNode.fromArray(<span class="keyword">new</span> <span class="keyword">int</span>[] &#123; <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">6</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">7</span>,</span><br><span class="line">                <span class="number">8</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">6</span> &#125;);</span><br><span class="line">        <span class="comment">// System.out.println(Arrays.toString(ListNode.toArray(reverseList(l1))));</span></span><br><span class="line">        System.out.println(Arrays.toString(ListNode.toArray(addInList(l2, l1))));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="链表中环的入口节点">(9/14) 链表中环的入口节点</h3>
<p>题目描述</p>
<ul>
<li>对于一个给定的链表，返回环的入口节点，如果没有环，返回null</li>
</ul>
<p>解题思路</p>
<ul>
<li>首先用快慢指针判断是否存在环</li>
<li>如存在环，假设环前长度为 d , 环长度为 l , 相遇点在环入口后 s 处</li>
</ul>
<p><span class="math display">\[ d+l+s = 2*(d+s) \]</span> <span class="math display">\[ d+s = l \]</span></p>
<ul>
<li>快指针归零，再走 (l-s)</li>
<li>慢指针</li>
</ul>
<p><span class="math display">\[ d+s+(l-s)=d+l \]</span></p>
<ul>
<li>快指针</li>
</ul>
<p><span class="math display">\[ l-s = d\]</span></p>
<ul>
<li>二者在 d 处相遇</li>
</ul>
<p>代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NowCoder_DetectCycle</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ListNode <span class="title">detectCycle</span><span class="params">(ListNode head)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (head == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        ListNode ptr1 = head;</span><br><span class="line">        ListNode ptr2 = head;</span><br><span class="line">        ListNode meetpoint = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">while</span>(ptr1 != <span class="keyword">null</span> &amp;&amp; ptr1.next != <span class="keyword">null</span>)&#123;</span><br><span class="line">            ptr1 = ptr1.next.next;</span><br><span class="line">            ptr2 = ptr2.next;</span><br><span class="line">            <span class="keyword">if</span> (ptr2 == ptr1)&#123;</span><br><span class="line">                meetpoint = ptr1;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 判断是否有环</span></span><br><span class="line">        <span class="keyword">if</span> (meetpoint == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        <span class="comment">// 快指针从头出发，慢指针接着走</span></span><br><span class="line">        ptr1 = head;</span><br><span class="line">        <span class="keyword">while</span>(ptr1 != ptr2)&#123;</span><br><span class="line">            ptr1 = ptr1.next;</span><br><span class="line">            ptr2 = ptr2.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ptr1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        ListNode l1 = ListNode.fromArray(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>&#125;);</span><br><span class="line">        l1.next.next.next.next.next = l1.next.next;</span><br><span class="line">        ListNode l2 = <span class="keyword">new</span> ListNode(<span class="number">1</span>);</span><br><span class="line">        System.out.println(detectCycle(l2).val);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="两个链表的第一个公共节点">(9/14) 两个链表的第一个公共节点</h3>
<p>题目描述</p>
<ul>
<li>输入两个链表，找出它们的第一个公共结点。（注意因为传入数据是链表，所以错误测试数据的提示是用其他方式显示的，保证传入数据是正确的）</li>
</ul>
<p>解题思路1——<strong>链表成环</strong></p>
<ul>
<li>两链表应该呈现出 “Y” 形。把第二个链表接到第一个链表尾巴上形成一个环，若存在公共节点，则必形成环。</li>
<li>如果保留 p1, 最后的节点 temp -&gt; p2。用 detectCycle(p1) 找出节点 result。<strong>再将temp -&gt; p2 断开</strong>。返回result。</li>
<li>代码如下</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ListNode <span class="title">FindFirstCommonNode</span><span class="params">(ListNode pHead1, ListNode pHead2)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(pHead1 == <span class="keyword">null</span> || pHead2 == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    ListNode curr = pHead1;</span><br><span class="line">    <span class="keyword">while</span> (curr.next != <span class="keyword">null</span>) curr = curr.next;</span><br><span class="line">    curr.next = pHead2;</span><br><span class="line">    <span class="comment">// 把两个链表接起来找第一个</span></span><br><span class="line"></span><br><span class="line">    ListNode firstCommon = detectCycle(pHead1);</span><br><span class="line">    <span class="comment">// 断开两个链表</span></span><br><span class="line">    curr.next = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">return</span> firstCommon;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ListNode <span class="title">detectCycle</span><span class="params">(ListNode head)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (head == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    ListNode ptr1 = head;</span><br><span class="line">    ListNode ptr2 = head;</span><br><span class="line">    ListNode meetpoint = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">while</span>(ptr1 != <span class="keyword">null</span> &amp;&amp; ptr1.next != <span class="keyword">null</span>)&#123;</span><br><span class="line">        ptr1 = ptr1.next.next;</span><br><span class="line">        ptr2 = ptr2.next;</span><br><span class="line">        <span class="keyword">if</span> (ptr2 == ptr1)&#123;</span><br><span class="line">            meetpoint = ptr1;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (meetpoint == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    <span class="comment">// 快指针从头出发，慢指针接着走</span></span><br><span class="line">    ptr1 = head;</span><br><span class="line">    <span class="keyword">while</span>(ptr1 != ptr2)&#123;</span><br><span class="line">        ptr1 = ptr1.next;</span><br><span class="line">        ptr2 = ptr2.next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ptr1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>解题思路2</p>
<ul>
<li>先遍历两链表，分别得出长度</li>
<li>较长者截取掉更长的部分，因为共同部分肯定比较短的链表更短</li>
<li>然后两链表同时向后遍历直到二者相同</li>
<li>代码如下</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 解法：截取长度并依次比较。复杂度O(m+n)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ListNode <span class="title">FindFirstCommonNode</span><span class="params">(ListNode pHead1, ListNode pHead2)</span> </span>&#123;</span><br><span class="line">    ListNode p1 = pHead1;</span><br><span class="line">    ListNode p2 = pHead2;</span><br><span class="line">    <span class="keyword">int</span> len1 = <span class="number">0</span>, len2 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (p1 != <span class="keyword">null</span>)&#123;</span><br><span class="line">        p1 = p1.next;</span><br><span class="line">        len1++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (p2 != <span class="keyword">null</span>)&#123;</span><br><span class="line">        p2 = p2.next;</span><br><span class="line">        len2++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (len1 &gt;= len2)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len1 - len2; i++)&#123;</span><br><span class="line">            pHead1 = pHead1.next;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len2 - len1; i++)&#123;</span><br><span class="line">            pHead2 = pHead2.next;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (pHead1 != pHead2)&#123;</span><br><span class="line">        pHead1 = pHead1.next;</span><br><span class="line">        pHead2 = pHead2.next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> pHead1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Java数据结构与算法</category>
      </categories>
  </entry>
  <entry>
    <title>Data-Mining-HW1</title>
    <url>/2020/10/06/Data-Mining-HW1/</url>
    <content><![CDATA[<h1 id="description">Description</h1>
<h2 id="problem-1-softmax-kernel-approximators-50-points">1 Problem 1: Softmax-kernel approximators [50 points]</h2>
<p>The goal of this task is to test different random feature map based estimators of the softmax-kernel. Choose two feature vectors <span class="math inline">\(x, y \in \mathbb{R}^{20}\)</span> of unit <span class="math inline">\(L_2\)</span>-norm and with angle <span class="math inline">\(\frac{π}{3}\)</span> between them. Consider three estimators of the value of the softmax-kernel in these points: the regular one using independent sin/cos-random features (leveraging random feature maps for the Gaussian kernel) and its two modifications: the one using Givens random rotations and the one applying random Hadamard matrices (with three <strong>HD</strong>-blocks). Describe in detail each estimator. Then for the number of random projections <span class="math inline">\(m = 5, 10, 20, 30, 50, 100\)</span> and each estimator, compute its value as an average over <span class="math inline">\(s = 10\)</span> independent trials. Propose how to use those trials to compute empirical mean squared error for each estimator. Prepare plots where on <span class="math inline">\(x\)</span>-axis you put the number of random projections m used and on the <span class="math inline">\(y\)</span>-axis the average value of the estimator. Show also computed mean squared error in the form of the error-bars. Remark: If <span class="math inline">\(m &gt; d\)</span> the orthogonal trick can be still applied by constructing independent ensembles of samples such that within each ensemble samples are exactly orthogonal.</p>
<h2 id="problem-2-softmax-kernel-is-positive-semidefinite-30-points">Problem 2 Softmax-kernel is positive semidefinite [30 points]</h2>
<p><span class="math inline">\(K : \mathbb{R}^{d\times d} \to \mathbb{R}^{d}\)</span> is positive semidefinite (PSD) if:</p>
<p><span class="math display">\[v^T\kappa(X)v \geq 0\]</span></p>
<p>for any <span class="math inline">\(N&gt;0\)</span>, any set of feature vectors <span class="math inline">\(\chi = \{x_1, ..., x_N\}\subseteq \mathbb{R}^d\)</span>, matrix <span class="math inline">\(\kappa(X) := [K(x_i, x_j)]_{i,j=1,...,N} \in \mathbb{R}^{N\times N}\)</span> and any vector <span class="math inline">\(v \in \mathbb{R}^{N}\)</span>. Show that softmax-kernel is PSD.</p>
<h3 id="proof"><strong>Proof</strong>:</h3>
<!-- $$\kappa(X)_{ij} = K_{SM}(x_i, x_j) = e^{x_i^Tx_j}$$

$$v = \begin{bmatrix}v_1, v_2, ..., v_N\end{bmatrix} ,v_i \in \mathbb{R}$$

$$v^T\kappa(X)v = \begin{bmatrix}v_1, v_2, ..., v_N\end{bmatrix}\kappa(X) \begin{bmatrix}v_1\\v_2\\...\\v_N\end{bmatrix}, $$

$$ = \begin{bmatrix}e^{x_1^Tx_1}v_1 + e^{x_2^Tx_1}v_2+...+e^{x_N^Tx_1}v_N\\e^{x_1^Tx_2}v_1 + e^{x_2^Tx_2}v_2+...+e^{x_N^Tx_2}v_N\\...\\e^{x_1^Tx_N}v_1 + e^{x_2^Tx_N}v_2+...+e^{x_N^Tx_N}v_N\end{bmatrix}^T \begin{bmatrix}v_1\\v_2\\...\\v_N\end{bmatrix}$$

$$=\sum_{i=1}^{N}\sum_{j=1}^{N}{e^{x_j^Tx_i}v_iv_j}$$ -->
<p><span class="math display">\[X \in \mathbb{R}^{d\times N},\]</span> <span class="math display">\[\kappa(X)_{ij} = K_{SM}(x_i, x_j) = e^{x_i^Tx_j}\]</span> <span class="math display">\[ \kappa(X) = \exp{(X^TX)} \in \mathbb{R}^{N\times N}\]</span></p>
<h4 id="lemma1-xtx-in-mathbbrn-and-xxt-in-mathbbrd-are-psd">Lemma1: <span class="math inline">\(X^TX \in \mathbb{R}^N and \ XX^T \in \mathbb{R}^d\)</span> are PSD</h4>
<p>Proof:</p>
<p><span class="math display">\[v^T(X^TX)v = (Xv)^T(Xv) = y^Ty \geq 0\]</span></p>
<p><span class="math display">\[v \in \mathbb{R}^{N}\]</span></p>
<p><span class="math display">\[y = Xv \in \mathbb{R}^{d}\]</span></p>
<p>Similarly, $XX^T $ is PSD</p>
<h4 id="lemma2-xtxk-and-xxtk-k012...-are-psd.">Lemma2: <span class="math inline">\((X^TX)^k \ and \  (XX^T)^k, k=0,1,2,...\)</span> are PSD.</h4>
<p>Proof:</p>
<p>For <span class="math inline">\(k=0\)</span>, obviously <span class="math inline">\(I\)</span> is PSD.</p>
<p>From above, this holds when <span class="math inline">\(k=1\)</span>. Use mathematical induction, if this is true for <span class="math inline">\(k=n-1\)</span>, then for <span class="math inline">\(k=n\)</span> <span class="math display">\[ v^T(X^TX)^{n}v = (Xv)^T(XX^T)^{n-1}(Xv) = y^T(XX^T)^{n-1}y \geq 0\]</span> <span class="math display">\[ v\in \mathbb{R}^N, y = Xv \in \mathbb{R}^d\]</span> <span class="math display">\[ u^T(XX^T)^{n}u = (X^Tu)^T(X^TX)^{n-1}(X^Tu) = z^T(X^TX)^{n-1}z \geq 0,\  z\in \mathbb{R}^N \]</span> <span class="math display">\[ u\in \mathbb{R}^d, z=X^Tu\in \mathbb{R}^N\]</span></p>
<p>Therefore this holds for arbitrary <span class="math inline">\(k \in \{0,1,2,3...\}\)</span>.</p>
<h4 id="lemma3-matrix-exponential">Lemma3: Matrix exponential</h4>
<p>The exponential of a <span class="math inline">\(N\times N\)</span> matrix <span class="math inline">\(X\)</span> is: <span class="math display">\[e^{X} = \sum_{k=0}^{\infty}{\frac{1}{k!}X^k}\]</span></p>
<h4 id="using-results-above">Using results above,</h4>
<p><span class="math display">\[ \therefore \kappa(X) = \exp{(X^TX)} = \sum_{k=0}^{\infty}{\frac{1}{k!}(X^TX)^k}\]</span></p>
<p><span class="math display">\[ v^T\kappa(X)v = \sum_{k=0}^{\infty}{\frac{1}{k!}v^T(X^TX)^kv} \geq 0\]</span></p>
<p><span class="math inline">\(\therefore\)</span> Softmax-kernel is PSD.</p>
<h1 id="notebook-for-problem-1">Notebook for Problem 1</h1>
<h2 id="import-packages">Import packages</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> norm</span><br><span class="line"><span class="keyword">from</span> scipy.spatial.transform <span class="keyword">import</span> Rotation <span class="keyword">as</span> R</span><br><span class="line"><span class="keyword">import</span> scipy <span class="keyword">as</span> sp</span><br></pre></td></tr></table></figure>
<h2 id="problem-1-softmax-kernel-approximators-50-points-1">Problem 1: Softmax-kernel approximators [50 points]</h2>
<h3 id="choose-two-feature-vectors-x-y-in-mathbbr20-of-unit-l_2-norm-and-with-angle-fracπ3-between-them.">Choose two feature vectors <span class="math inline">\(x, y \in \mathbb{R}^{20}\)</span> of unit <span class="math inline">\(L_2\)</span>-norm and with angle <span class="math inline">\(\frac{π}{3}\)</span> between them.</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateXY</span>(<span class="params">d = <span class="number">20</span></span>):</span></span><br><span class="line">    <span class="comment"># init X</span></span><br><span class="line">    x = np.array([<span class="number">1</span>] + [<span class="number">0</span>]*(d<span class="number">-1</span>))</span><br><span class="line">    y = np.array([<span class="number">0.5</span>, <span class="number">3</span>**<span class="number">0.5</span>/<span class="number">2</span>] + [<span class="number">0</span>]*(d<span class="number">-2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Orthogonal Matrix Q</span></span><br><span class="line">    A = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, (d,d))</span><br><span class="line">    Q, _ = np.linalg.qr(A, mode = <span class="string">&#x27;complete&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># x^Ty=0.5 =&gt; (Qx)^T(Qy)=0.5, where Q is orthogonal</span></span><br><span class="line">    x = np.matmul(Q, x)</span><br><span class="line">    y = np.matmul(Q, y)</span><br><span class="line">    <span class="keyword">return</span> x,y</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x, y = generateXY(<span class="number">20</span>)</span><br><span class="line">x, y</span><br></pre></td></tr></table></figure>
<pre><code>(array([-0.29894506, -0.66638541, -0.18786531, -0.08032163,  0.01395138,
         0.002299  ,  0.1465464 ,  0.32123403, -0.13995845,  0.17208692,
        -0.30362139, -0.20299325, -0.00676445, -0.17788199, -0.00389046,
         0.11739096,  0.13262346, -0.07173246,  0.18018864, -0.12911171]),
 array([ 0.05091238, -0.50959723,  0.39335244, -0.36683427,  0.11539508,
        -0.07330203,  0.07384118,  0.13288575, -0.47794303,  0.09279718,
        -0.08177025,  0.01269871, -0.01592124,  0.1074321 , -0.0916396 ,
         0.05875871,  0.19641505, -0.27930029,  0.14967152,  0.01007921]))</code></pre>
<p><span class="math inline">\(K_{SM}(x,y) = e^{x^Ty} = e^{0.5}\)</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">true_value = np.exp(np.dot(x, y))</span><br><span class="line">true_value</span><br></pre></td></tr></table></figure>
<pre><code>1.6487212707001284</code></pre>
<h3 id="consider-three-estimators-of-the-value-of-the-softmax-kernel-in-these-points-describe-in-detail-each-estimator.">Consider three estimators of the value of the softmax-kernel in these points; Describe in detail each estimator.</h3>
<ul>
<li>the regular one using independent sin/cos-random features (leveraging random feature maps for the Gaussian kernel)</li>
<li>and its two modifications: the one using Givens random rotations</li>
<li>and the one applying random Hadamard matrices (with three <strong>HD</strong>-blocks).</li>
</ul>
<h4 id="answer">Answer</h4>
<ul>
<li>Regular one:
<ul>
<li>Generate matrix <span class="math inline">\(G:N(0,I_d) \in \mathbb{R}^{m\times d}\)</span></li>
<li>caculate the norms of each row(<span class="math inline">\(|w_i|^2\sim \chi^2(d)\)</span>)</li>
<li>Map through <span class="math inline">\(G\)</span>'s <span class="math inline">\(m\)</span> rows and orthogonalize <span class="math inline">\(G\)</span>'s each <span class="math inline">\(d\times d\)</span> block(or the remainder part <span class="math inline">\((m\%d) \times d\)</span>)</li>
<li>Renormalize the orthogonalized matrix with its original row norms. <span class="math display">\[ \phi_{SM}(x) = e^{\frac{||x||^2}{2}} \frac{1}{\sqrt{m}} \begin{bmatrix}\cos(w_1^Tx)\\...\\\cos(w_m^Tx)\\\sin(w_1^Tx)\\...\\\sin(w_m^Tx) \end{bmatrix} \]</span></li>
<li>Calculate <span class="math inline">\(\phi(x)\)</span> and <span class="math inline">\(\phi(y)\)</span>, and thus <span class="math inline">\(\phi(x)^T\phi(y)\)</span></li>
</ul></li>
<li>Givens random rotations:
<ul>
<li>Generate random Givens Matrices <span class="math inline">\(\{Giv_i \in \mathbb{R}^{d\times d}\}, i=1,...,k = d\log{d}\)</span></li>
<li>Generate orthogonal <span class="math inline">\(d\times d\)</span> matrix <span class="math inline">\(G_{ort} = Giv_1,...Giv_k\)</span></li>
<li>Generate diagnoal matrix <span class="math inline">\(S\)</span> with entries <span class="math inline">\(S_{ii}\)</span> such that <span class="math inline">\(S_{ii} \sim \chi(d)\)</span></li>
<li>Renormalize <span class="math inline">\(G_{ort}\)</span> by left multiplying <span class="math inline">\(S\)</span></li>
<li>Generate several such <span class="math inline">\(G_{ort} \in \mathbb{R}^{d\times d}\)</span> and concatenate to get <span class="math inline">\(G_{ort} \in \mathbb{R}^{m\times d}\)</span></li>
<li>Apply the softmax random feature function <span class="math inline">\(\phi_{SM}\)</span> like the regular one</li>
</ul></li>
<li>Hadamard Matrices
<ul>
<li>Calculate the padded size <span class="math inline">\(d&#39; = 2^T\)</span></li>
<li>Pad the input <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> to dimension of <span class="math inline">\(d&#39;\)</span></li>
<li>Generate Hadamard Matrix <span class="math inline">\(H: d&#39; \times d&#39;\)</span></li>
<li>Generate 3 random diagonal "sign-flipping" matrices <span class="math inline">\(D_i\)</span></li>
<li>Generate diagnoal matrix <span class="math inline">\(S\)</span> with entries <span class="math inline">\(S_{ii}\)</span> such that <span class="math inline">\(S_{ii} \sim \chi(d&#39;)\)</span></li>
<li><span class="math inline">\(G_{ort} = S(\frac{1}{\sqrt{d&#39;}}HD_1)(\frac{1}{\sqrt{d&#39;}}HD_2)(\frac{1}{\sqrt{d&#39;}}HD_3)\)</span></li>
<li>Generate several such <span class="math inline">\(G_{ort} \in \mathbb{R}^{d&#39;\times d&#39;}\)</span> and concatenate to get <span class="math inline">\(G_{ort} \in \mathbb{R}^{m\times d&#39;}\)</span></li>
<li>Apply the softmax random feature function <span class="math inline">\(\phi_{SM}\)</span> like the regular one</li>
</ul></li>
</ul>
<h3 id="then-for-the-number-of-random-projections-m-5-10-20-30-50-100-and-each-estimator-compute-its-value-as-an-average-over-s-10-independent-trials.">Then for the number of random projections <span class="math inline">\(m = 5, 10, 20, 30, 50, 100\)</span> and each estimator, compute its value as an average over <span class="math inline">\(s = 10\)</span> independent trials.</h3>
<h4 id="estimator1-regular-one">Estimator1: Regular one</h4>
<p><span class="math display">\[ \phi_{SM}(x) = e^{\frac{||x||^2}{2}}\phi_{Gauss}(x) = e^{\frac{||x||^2}{2}} \frac{1}{\sqrt{m}} \begin{bmatrix}\cos(w_1^Tx)\\...\\\cos(w_m^Tx)\\\sin(w_1^Tx)\\...\\\sin(w_m^Tx) \end{bmatrix} \]</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_schmidt_columns</span>(<span class="params">X</span>):</span></span><br><span class="line">    Q, R = np.linalg.qr(X)</span><br><span class="line">    <span class="keyword">return</span> Q</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">orthgonalize</span>(<span class="params">V</span>):</span></span><br><span class="line">    N = V.shape[<span class="number">0</span>]</span><br><span class="line">    d = V.shape[<span class="number">1</span>]</span><br><span class="line">    turns = int(N/d)</span><br><span class="line">    remainder = N%d</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#V =  np.random.normal(size=[(turns+1)*d, d])</span></span><br><span class="line">    <span class="comment">#V =  np.random.normal(size =[N, d])</span></span><br><span class="line">    <span class="comment">#print(V.shape)</span></span><br><span class="line">    V_ = np.zeros_like(V)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(turns):</span><br><span class="line">        v = gram_schmidt_columns(V[i*d:(i+<span class="number">1</span>)*d, :].T).T</span><br><span class="line">        V_[i*d:(i+<span class="number">1</span>)*d, :] = v</span><br><span class="line">    <span class="keyword">if</span> remainder != <span class="number">0</span>:</span><br><span class="line">        V_[turns*d:,:] = gram_schmidt_columns(V[turns*d:,:].T).T</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> V_</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateGMatrix</span>(<span class="params">m = <span class="number">5</span>, d = <span class="number">20</span></span>) -&gt; np.array:</span></span><br><span class="line">    G = np.random.normal(<span class="number">0</span>,<span class="number">1</span>, (m,d))</span><br><span class="line">    <span class="comment"># Renormalize</span></span><br><span class="line">    norms = np.linalg.norm(G, axis=<span class="number">1</span>).reshape([m,<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> orthgonalize(G) * norms</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Softmax with sin/cos random features</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">baseline_SM</span>(<span class="params">x, G, m</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Calculate the result of baseline random feature mapping</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        x: array, dimension = d</span></span><br><span class="line"><span class="string">            The data point to input to the baseline mapping</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        G: matrix, dimension = m*d</span></span><br><span class="line"><span class="string">            The matrix in the baseline random feature mapping</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">        m: integer</span></span><br><span class="line"><span class="string">            The number of dimension that we want to reduce to</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    left = np.cos(np.dot(G, x).astype(np.float32))</span><br><span class="line">    right = np.sin(np.dot(G, x).astype(np.float32))</span><br><span class="line">    <span class="keyword">return</span> np.exp( (np.linalg.norm(x))**<span class="number">2</span> / <span class="number">2</span>) * ((<span class="number">1</span>/m)**<span class="number">0.5</span>) * np.append(left, right)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">baseline_SM_approximation</span>(<span class="params">x, y, m, d=<span class="number">20</span>, s=<span class="number">10</span></span>):</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(s):</span><br><span class="line">        G = generateGMatrix(m=m, d=d)</span><br><span class="line">        phi_x = baseline_SM(x, G, m)</span><br><span class="line">        phi_y = baseline_SM(y, G, m)</span><br><span class="line">        xTy = np.dot(phi_x, phi_y)</span><br><span class="line">        sum += xTy</span><br><span class="line">    <span class="keyword">return</span> sum / s</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">arr_m = [<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">50</span>, <span class="number">100</span>]</span><br><span class="line">arr_estimation = [baseline_SM_approximation(x, y, i, <span class="number">20</span>, <span class="number">10</span>) <span class="keyword">for</span> i <span class="keyword">in</span> arr_m]</span><br><span class="line">arr_estimation</span><br></pre></td></tr></table></figure>
<pre><code>[1.8072113811969757,
 1.69626122713089,
 1.6249037623405456,
 1.670891809463501,
 1.6728131294250488,
 1.640375828742981]</code></pre>
<h4 id="estimator2-givens-random-rotations">Estimator2: Givens random rotations</h4>
<p><span class="math inline">\(G_{ort} = Giv_1,...Giv_k, k=O(d\log{d})\)</span></p>
<p><span class="math inline">\(Giv_i \in \mathbb{R}^{d\times d}\)</span></p>
<p><img src="https://www.zhihu.com/equation?tex=G%28i%2Cj%2C%5Ctheta%29+%3D++%5Cbegin%7Bbmatrix%7D+1+%26+%5Cdots+%26+0+%26+%5Cdots+%26+0+%26+%5Cdots+%26+0%5C%5C+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots%5C%5C+0+%26+%5Cdots+%26+cos%5Ctheta+%26+%5Cdots+%26+sin%5Ctheta+%26+%5Cdots+%26+0%5C%5C+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots%5C%5C+0+%26+%5Cdots+%26+-sin%5Ctheta+%26+%5Cdots+%26+cos%5Ctheta+%26+%5Cdots+%26+0%5C%5C+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots%5C%5C+0+%26+%5Cdots+%26+0+%26+%5Cdots+%26+0+%26+%5Cdots+%26+1%5C%5C+%5Cend%7Bbmatrix%7D+%5Ctag%7B8%7D" /></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+i+%26%3C+j+%5C%5C+G_%7Bi%2Ci%7D+%26%3D+c%5C%5C+G_%7Bj%2Cj%7D+%26%3D+c%5C%5C+G_%7Bi%2Cj%7D+%26%3D+s%5C%5C+G_%7Bj%2Ci%7D+%26%3D+-s%5C%5C+G_%7Bk%2Ck%7D+%26%3D+1%2C%5C+for%5C+k%5Cne+i%5C+or%5C+j%5C%5C+G_%7Bt%2Cs%7D+%26%3D+0%2C%5C+otherwise+%5Cend%7Baligned%7D+%5Ctag%7B9%7D" /></p>
<p><a href="http://papers.nips.cc/paper/6246-orthogonal-random-features.pdf">Orthogonal Random Features</a></p>
<p>The idea of Orthogonal Random Features (ORF) is to impose orthogonality on the matrix on the linear transformation matrix <span class="math inline">\(G\)</span>. Note that one cannot achieve unbiased kernel estimation by simply replacing <span class="math inline">\(G\)</span> by an orthogonal matrix, since the norms of the rows of <span class="math inline">\(G\)</span> follow the -distribution, while rows of an orthogonal matrix have the unit norm. The linear transformation matrix of ORF has the following form <span class="math display">\[W_{ORF} = \frac{1}{\sigma}
SQ\]</span> where <span class="math inline">\(Q\)</span> is a uniformly distributed random orthogonal matrix. The set of rows of <span class="math inline">\(Q\)</span> forms a bases in <span class="math inline">\(R^d\)</span>. <span class="math inline">\(S\)</span> is a diagonal matrix, with diagonal entries sampled i.i.d. from the <span class="math inline">\(\chi\)</span>-distribution</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateGiv</span>(<span class="params">d</span>):</span></span><br><span class="line">    Giv = np.identity(d)</span><br><span class="line">    i = np.random.randint(<span class="number">0</span>, d<span class="number">-1</span>)</span><br><span class="line">    j = np.random.randint(i+<span class="number">1</span>, d)</span><br><span class="line">    theta = np.random.uniform(<span class="number">0</span>, <span class="number">2</span>*np.pi)</span><br><span class="line">    c = np.cos(theta)</span><br><span class="line">    s = np.sin(theta)</span><br><span class="line">    Giv[i,i] = c</span><br><span class="line">    Giv[j,j] = c</span><br><span class="line">    Giv[i,j] = s</span><br><span class="line">    Giv[j, i] = -s</span><br><span class="line">    <span class="keyword">return</span> Giv</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateS</span>(<span class="params">d</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.diag(np.sqrt(np.random.chisquare(d, (d))))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Givens_generateGort_dd</span>(<span class="params">d</span>):</span></span><br><span class="line">    k = int(d * np.log(d))</span><br><span class="line">    Gort_dd = generateGiv(d)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k<span class="number">-1</span>):</span><br><span class="line">        Gort_dd = np.matmul(Gort_dd, generateGiv(d))</span><br><span class="line">    <span class="keyword">return</span> np.matmul(generateS(d), Gort_dd)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Givens_generateGort_md</span>(<span class="params">m, d</span>):</span></span><br><span class="line">    arr_Gort_dd = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m//d + <span class="number">1</span>):</span><br><span class="line">        arr_Gort_dd.append(Givens_generateGort_dd(d)) <span class="comment"># (m//d+1, d, d)</span></span><br><span class="line">    Gort_md = np.concatenate(arr_Gort_dd, axis=<span class="number">0</span>)</span><br><span class="line">    Gort_md = Gort_md[:m, :]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Gort_md</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GivensRandomRotations_SM_approximation</span>(<span class="params">x, y, m, d=<span class="number">20</span>, s=<span class="number">10</span></span>):</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(s):</span><br><span class="line">        G = Givens_generateGort_md(m,<span class="number">20</span>)</span><br><span class="line">        phi_x = baseline_SM(x, G, m)</span><br><span class="line">        phi_y = baseline_SM(y, G, m)</span><br><span class="line">        xTy = np.dot(phi_x, phi_y)</span><br><span class="line">        sum += xTy</span><br><span class="line">    <span class="keyword">return</span> sum / s</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">arr_m = [<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">50</span>, <span class="number">100</span>]</span><br><span class="line">arr_estimation = [GivensRandomRotations_SM_approximation(x, y, i, <span class="number">20</span>, <span class="number">10</span>) <span class="keyword">for</span> i <span class="keyword">in</span> arr_m]</span><br><span class="line">arr_estimation</span><br></pre></td></tr></table></figure>
<pre><code>[1.3707373946905137,
 1.7438804149627685,
 1.6728084802627563,
 1.7161020755767822,
 1.689209222793579,
 1.6569807291030885]</code></pre>
<h4 id="estimator3-hadamard-matriceswith-3-hd-blocks">Estimator3: Hadamard matrices(with 3 HD-blocks)</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Hadamard</span>(<span class="params">d</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;return: Hadamard matrix of shape(2^d, 2^d)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> d==<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> np.ones((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ul = Hadamard(d<span class="number">-1</span>)</span><br><span class="line">        ur = Hadamard(d<span class="number">-1</span>)</span><br><span class="line">        bl = Hadamard(d<span class="number">-1</span>)</span><br><span class="line">        br = -Hadamard(d<span class="number">-1</span>)</span><br><span class="line">        u = np.concatenate((ul,ur),axis=<span class="number">1</span>)</span><br><span class="line">        b = np.concatenate((bl,br),axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> np.concatenate((u,b),axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalized_Hadamard</span>(<span class="params">d</span>):</span></span><br><span class="line">    H = Hadamard(d)</span><br><span class="line">    norms = np.linalg.norm(H, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> H / norms</span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateDi</span>(<span class="params">d</span>):</span></span><br><span class="line">    D_i = np.diag([np.random.choice([<span class="number">1</span>, <span class="number">-1</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, d+<span class="number">1</span>)])</span><br><span class="line">    <span class="keyword">return</span> D_i</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Uncomment to get Structured Orthogonal Random Features (SORF) solution</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Hadamard_generateGort_dd</span>(<span class="params">d</span>):</span> </span><br><span class="line">    t = np.ceil(np.log2(d)).astype(int)</span><br><span class="line">    d_ = <span class="number">2</span>**t</span><br><span class="line">    <span class="comment"># H = normalized_Hadamard(t)</span></span><br><span class="line">    H = Hadamard(t)</span><br><span class="line">    Gort_dd = np.identity(d_)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        <span class="comment"># Gort_dd = np.matmul(Gort_dd ,np.matmul(H, generateDi(d_)))</span></span><br><span class="line">        Gort_dd = np.matmul(Gort_dd ,np.matmul(<span class="number">1</span> / np.sqrt(d_) * H, generateDi(d_)))</span><br><span class="line">    <span class="comment"># return np.sqrt(d_)*Gort_dd</span></span><br><span class="line">    <span class="keyword">return</span> np.matmul(generateS(d_), Gort_dd)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Hadamard_generateGort_md</span>(<span class="params">m, d</span>):</span></span><br><span class="line">    t = np.floor(np.log2(d)).astype(int) + <span class="number">1</span></span><br><span class="line">    d_ = <span class="number">2</span>**t</span><br><span class="line">    arr_Gort_dd = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m//d_ + <span class="number">1</span>):</span><br><span class="line">        arr_Gort_dd.append(Hadamard_generateGort_dd(d))</span><br><span class="line">    Gort_md = np.concatenate(arr_Gort_dd, axis=<span class="number">0</span>)</span><br><span class="line">    Gort_md = Gort_md[:m, :]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Gort_md</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Hadamard_SM_approximation</span>(<span class="params">x, y, m, d=<span class="number">20</span>, s=<span class="number">10</span></span>):</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(s):</span><br><span class="line">        G = Hadamard_generateGort_md(m,<span class="number">20</span>)</span><br><span class="line">        d_ = G.shape[<span class="number">1</span>]</span><br><span class="line">        x_padded = np.pad(x, (<span class="number">0</span>, d_-d))</span><br><span class="line">        y_padded = np.pad(y, (<span class="number">0</span>, d_-d))</span><br><span class="line">        phi_x = baseline_SM(x_padded, G, m)</span><br><span class="line">        phi_y = baseline_SM(y_padded, G, m)</span><br><span class="line">        xTy = np.dot(phi_x, phi_y)</span><br><span class="line">        sum += xTy</span><br><span class="line">    <span class="keyword">return</span> sum / s</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">arr_m = [<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">50</span>, <span class="number">100</span>]</span><br><span class="line">arr_estimation = [Hadamard_SM_approximation(x, y, i, <span class="number">20</span>, <span class="number">10</span>) <span class="keyword">for</span> i <span class="keyword">in</span> arr_m]</span><br><span class="line">arr_estimation</span><br></pre></td></tr></table></figure>
<pre><code>[1.7086819410324097,
 1.617677903175354,
 1.6517545461654664,
 1.645971429347992,
 1.6519848704338074,
 1.6540889978408813]</code></pre>
<h3 id="propose-how-to-use-those-trials-to-compute-empirical-mean-squared-error-for-each-estimator.">Propose how to use those trials to compute empirical mean squared error for each estimator.</h3>
<h4 id="answer-1">Answer</h4>
<ul>
<li>The true value: <span class="math inline">\(K_{SM}(x,y) = e^{x^Ty} = e^{0.5}\)</span></li>
<li>For each <span class="math inline">\(m\)</span>, run 10 trials. <span class="math display">\[MSE = \frac{\sum_{i=1}^{10}{(est_i - e^{0.5}})^2}{10}\]</span></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MSE_iid</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">    SM_approximation=Hadamard_SM_approximation,</span></span></span><br><span class="line"><span class="function"><span class="params">    episode=<span class="number">1</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">    epoch=<span class="number">10</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">    arr_m=[<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">50</span>, <span class="number">100</span>],</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line"></span><br><span class="line">    estimation = np.zeros((len(arr_m), episode, epoch))</span><br><span class="line">    MSE_iid = []</span><br><span class="line">    list_of_samples = []</span><br><span class="line"></span><br><span class="line">    x, y = generateXY(<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># for e in tqdm(range(episode), position=0, leave=True):</span></span><br><span class="line">    <span class="comment">#     x, y = generateXY(20)</span></span><br><span class="line">    <span class="comment">#     true_value = np.exp(0.5)</span></span><br><span class="line">    <span class="comment">#     list_of_samples.append((x, y))</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(len(arr_m)):</span><br><span class="line">        mse_m = []</span><br><span class="line">        m = arr_m[n]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> e <span class="keyword">in</span> tqdm(range(episode), position=<span class="number">0</span>, leave=<span class="literal">True</span>):        </span><br><span class="line">            <span class="comment"># x = list_of_samples[e][0]</span></span><br><span class="line">            <span class="comment"># y = list_of_samples[e][1]</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</span><br><span class="line">                np.random.seed()</span><br><span class="line">                estimation[n, e, i] = SM_approximation(x, y, m, d=<span class="number">20</span>, s=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            mse_m.append(mean_squared_error(np.repeat(true_value, epoch), estimation[n, e])) <span class="comment"># compare 10 values</span></span><br><span class="line">        </span><br><span class="line">        MSE_iid.append(mse_m)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> estimation.mean(axis=(<span class="number">1</span>,<span class="number">2</span>)) ,MSE_iid</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">estimation_regular, MSE_iid_regular = MSE_iid(SM_approximation=baseline_SM_approximation)</span><br><span class="line">df_MSE_Regular = pd.DataFrame(MSE_iid_regular, index = arr_m, columns = [<span class="string">&quot;Regular&quot;</span>])</span><br><span class="line">df_estimation_Regular = pd.DataFrame(estimation_regular, index = arr_m, columns = [<span class="string">&quot;Regular&quot;</span>])</span><br></pre></td></tr></table></figure>
<pre><code>100%|██████████| 1/1 [00:00&lt;00:00, 101.45it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 119.49it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 95.60it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 112.89it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 89.43it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 74.54it/s]</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">estimation_Givens, MSE_iid_Givens = MSE_iid(SM_approximation=GivensRandomRotations_SM_approximation)</span><br><span class="line">df_MSE_Givens = pd.DataFrame(MSE_iid_Givens, index = arr_m, columns = [<span class="string">&quot;GivensRotation&quot;</span>])</span><br><span class="line">df_estimation_Givens = pd.DataFrame(estimation_Givens, index = arr_m, columns = [<span class="string">&quot;GivensRotation&quot;</span>])</span><br></pre></td></tr></table></figure>
<pre><code>100%|██████████| 1/1 [00:00&lt;00:00, 25.62it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 30.17it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 13.01it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 16.68it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 14.39it/s]
100%|██████████| 1/1 [00:00&lt;00:00,  6.19it/s]</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">estimation_Hadamard, MSE_iid_Hadamard = MSE_iid(SM_approximation=Hadamard_SM_approximation)</span><br><span class="line">df_MSE_Hadamard = pd.DataFrame(MSE_iid_Hadamard, index = arr_m, columns = [<span class="string">&quot;Hadamard&quot;</span>])</span><br><span class="line">df_estimation_Hadamard = pd.DataFrame(estimation_Hadamard, index = arr_m, columns = [<span class="string">&quot;Hadamard&quot;</span>])</span><br></pre></td></tr></table></figure>
<pre><code>100%|██████████| 1/1 [00:00&lt;00:00, 12.08it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 11.96it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 12.45it/s]
100%|██████████| 1/1 [00:00&lt;00:00, 12.02it/s]
100%|██████████| 1/1 [00:00&lt;00:00,  7.46it/s]
100%|██████████| 1/1 [00:00&lt;00:00,  3.86it/s]</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_MSE_total = pd.concat([df_MSE_Regular, df_MSE_Givens, df_MSE_Hadamard], axis=<span class="number">1</span>)</span><br><span class="line">df_estimation_total = pd.concat([df_estimation_Regular, df_estimation_Givens, df_estimation_Hadamard], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_MSE_total</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Regular
</th>
<th>
GivensRotation
</th>
<th>
Hadamard
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
5
</th>
<td>
0.172311
</td>
<td>
0.188588
</td>
<td>
0.162290
</td>
</tr>
<tr>
<th>
10
</th>
<td>
0.066876
</td>
<td>
0.065423
</td>
<td>
0.077146
</td>
</tr>
<tr>
<th>
20
</th>
<td>
0.007488
</td>
<td>
0.017989
</td>
<td>
0.032074
</td>
</tr>
<tr>
<th>
30
</th>
<td>
0.020978
</td>
<td>
0.027808
</td>
<td>
0.007083
</td>
</tr>
<tr>
<th>
50
</th>
<td>
0.011074
</td>
<td>
0.003390
</td>
<td>
0.004394
</td>
</tr>
<tr>
<th>
100
</th>
<td>
0.001614
</td>
<td>
0.001725
</td>
<td>
0.001746
</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_estimation_total</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Regular
</th>
<th>
GivensRotation
</th>
<th>
Hadamard
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
5
</th>
<td>
1.709679
</td>
<td>
1.735700
</td>
<td>
1.481327
</td>
</tr>
<tr>
<th>
10
</th>
<td>
1.560758
</td>
<td>
1.708040
</td>
<td>
1.657001
</td>
</tr>
<tr>
<th>
20
</th>
<td>
1.659077
</td>
<td>
1.617314
</td>
<td>
1.574311
</td>
</tr>
<tr>
<th>
30
</th>
<td>
1.615470
</td>
<td>
1.572253
</td>
<td>
1.635823
</td>
</tr>
<tr>
<th>
50
</th>
<td>
1.603345
</td>
<td>
1.656838
</td>
<td>
1.642844
</td>
</tr>
<tr>
<th>
100
</th>
<td>
1.617124
</td>
<td>
1.648726
</td>
<td>
1.638272
</td>
</tr>
</tbody>
</table>
</div>
<h3 id="prepare-plots-where-on-x-axis-you-put-the-number-of-random-projections-m-used-and-on-the-y-axis-the-average-value-of-the-estimator.-show-also-computed-mean-squared-error-in-the-form-of-the-error-bars">Prepare plots where on x-axis you put the number of random projections m used and on the y-axis the average value of the estimator. Show also computed mean squared error in the form of the error-bars</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.set_style(<span class="string">&quot;darkgrid&quot;</span>)    </span><br><span class="line">df_estimation_total.plot(</span><br><span class="line">    kind = <span class="string">&#x27;line&#x27;</span>, </span><br><span class="line">    title = <span class="string">&#x27;Softmax-kernel approximators comparison&#x27;</span>, </span><br><span class="line">    grid=<span class="literal">True</span>, </span><br><span class="line">    legend=[df_estimation_total.columns],</span><br><span class="line">    xlabel=<span class="string">&#x27;m&#x27;</span>, </span><br><span class="line">    ylabel=<span class="string">&#x27;Average of Estimation&#x27;</span>,</span><br><span class="line">    figsize = (<span class="number">12</span>,<span class="number">8</span>),</span><br><span class="line">    yerr=df_MSE_total,</span><br><span class="line">    )</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/10/06/Data-Mining-HW1/DataMining_HW1_39_0.png" /></p>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>谈判案例模拟：Leckenby Company</title>
    <url>/2020/10/09/%E8%B0%88%E5%88%A4%E6%A1%88%E4%BE%8B%E6%A8%A1%E6%8B%9F%EF%BC%9ALeckenby-Company/</url>
    <content><![CDATA[<h1 id="概述">概述</h1>
<p>Managerial Negotiation 第五周的案例，强调了在谈判前要发现双方共同利益、maximize value creation。在本案例情境中，谈判轮数增加会导致双方可变成本增加，且谈判核心-<span class="math inline">\(w\)</span>值的增加会导致双方总福利的降低。在设计本模拟工具时也考虑了两种情况：</p>
<ul>
<li>君子条约: 在罢工（即造成双方成本增加）开始前确定<span class="math inline">\(w\)</span>在 <span class="math inline">\(0.5-0.52\)</span> 范围内，对两者较为公平且最大化总价值</li>
<li>消耗战(War of Attrition): 通过模拟发现作为Kunzler(希望w小)，利用前两轮压价、第三轮结束通常能获得最好结果，但该结果仍比君子条约差；作为Arnold(希望w大)，第六轮后结果比君子条约差，但在此之前有较大的议价空间。没有达成君子条约默契并消耗至第六轮，最终结果<span class="math inline">\(w\)</span>通常在0.6左右，Arnold小赚而Kunzler较亏</li>
</ul>
<p>针对消耗战情况，又设计了一个函数<code>find_Case</code>来针对当前已知信息（轮数，己方出价，对方出价），找出一个模拟情况，从而对未来局势有所判断。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_Case</span>(<span class="params">round, w_K, w_A</span>)</span></span><br></pre></td></tr></table></figure>
<h1 id="案例材料">案例材料</h1>
<div class="pdfobject-container" data-target="./Leckenby_Company1.pdf" data-height="1000px"></div>
<hr />
<h1 id="代码部分">代码部分</h1>
<h1 id="import">Import</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> scipy.stats <span class="keyword">as</span> spst</span><br></pre></td></tr></table></figure>
<h1 id="simulation1">Simulation1</h1>
<h2 id="distribution-triangular001">Distribution: triangular(0,0,1)</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">h = plt.hist(np.random.triangular(<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">10000</span>),bins = <span class="number">200</span>, density=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/10/09/%E8%B0%88%E5%88%A4%E6%A1%88%E4%BE%8B%E6%A8%A1%E6%8B%9F%EF%BC%9ALeckenby-Company/LeckenbySimulation_files/LeckenbySimulation_4_0.png" /></p>
<h2 id="simulation-code">Simulation Code</h2>
<p>As two parties approaching the final <span class="math inline">\(w\)</span>, they do not calculate their gain&amp;loss in each round. The next bid is solely subject to the triangular distribution</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># bid_Kunzler = 10</span></span><br><span class="line"><span class="comment"># bid_Arnold = 11</span></span><br><span class="line"><span class="comment"># round = 0</span></span><br><span class="line"><span class="comment"># while ((bid_Kunzler+0.0004 &lt; bid_Arnold) &amp; (round &lt;=22)):</span></span><br><span class="line"><span class="comment">#     round += 1</span></span><br><span class="line"><span class="comment">#     bid_Kunzler += (bid_Arnold - bid_Kunzler)*np.random.triangular(0, 0.4, 1)</span></span><br><span class="line"><span class="comment">#     bid_Arnold -= (bid_Arnold - bid_Kunzler)*np.random.triangular(0, 0.2, 1)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># np.around(bid_Kunzler, 3,), np.around(bid_Arnold, 3,)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Kunzler</span>(<span class="params">w, d</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-5</span>*w -(<span class="number">0.1</span>*d + <span class="number">0.015</span>*d**<span class="number">2</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Arnold</span>(<span class="params">w, d</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span>*w - (<span class="number">0.05</span>*d + <span class="number">0.005</span>*d**<span class="number">2</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">simulation</span>():</span></span><br><span class="line">    bid_Kunzler = <span class="number">10</span></span><br><span class="line">    bid_Arnold = <span class="number">11</span></span><br><span class="line">    round = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> ((bid_Kunzler+<span class="number">0.0004</span> &lt; bid_Arnold) &amp; (round &lt;=<span class="number">22</span>)):</span><br><span class="line">        round += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> (np.random.random()&gt;<span class="number">0.2</span>): <span class="comment"># 20% chance stick to the previous bid</span></span><br><span class="line">            bid_Kunzler += (bid_Arnold - bid_Kunzler)*np.random.triangular(<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> (np.random.random()&gt;<span class="number">0.2</span>):</span><br><span class="line">            bid_Arnold -= (bid_Arnold - bid_Kunzler)*np.random.triangular(<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    round -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> round==<span class="number">22</span>:</span><br><span class="line">        w = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        w = np.around(bid_Kunzler, <span class="number">3</span>) - <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    d = (round &gt; <span class="number">2</span>)*(round - <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (round, w, Kunzler(w, d), Arnold(w, d))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="run-simulation-100000-times">Run Simulation 100000 times</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sim = np.array([simulation() <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100000</span>)])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_sim = pd.DataFrame(sim, columns = [<span class="string">&quot;round&quot;</span>, <span class="string">&quot;w&quot;</span>, <span class="string">&quot;Kunzler&quot;</span>, <span class="string">&quot;Arnold&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_sim = df_sim.assign(total = <span class="keyword">lambda</span> x: x.Kunzler + x.Arnold)</span><br></pre></td></tr></table></figure>
<h2 id="scatterplot-kunzler-arnold-hueround-w">Scatterplot (Kunzler, Arnold), hue="round" / "w"</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">sns.scatterplot(data=df_sim, x=<span class="string">&quot;Kunzler&quot;</span>, y=<span class="string">&quot;Arnold&quot;</span>, hue=<span class="string">&quot;round&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/10/09/%E8%B0%88%E5%88%A4%E6%A1%88%E4%BE%8B%E6%A8%A1%E6%8B%9F%EF%BC%9ALeckenby-Company/LeckenbySimulation_files/LeckenbySimulation_14_0.png" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">sns.scatterplot(data=df_sim, x=<span class="string">&quot;Kunzler&quot;</span>, y=<span class="string">&quot;Arnold&quot;</span>, hue=<span class="string">&quot;w&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/10/09/%E8%B0%88%E5%88%A4%E6%A1%88%E4%BE%8B%E6%A8%A1%E6%8B%9F%EF%BC%9ALeckenby-Company/LeckenbySimulation_files/LeckenbySimulation_15_0.png" /></p>
<h2 id="analysis">Analysis</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_sim.sort_values([<span class="string">&quot;round&quot;</span>, <span class="string">&quot;Kunzler&quot;</span>]).groupby(<span class="string">&quot;round&quot;</span>).agg(<span class="string">&quot;mean&quot;</span>)</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
w
</th>
<th>
Kunzler
</th>
<th>
Arnold
</th>
<th>
total
</th>
</tr>
<tr>
<th>
round
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
1.0
</th>
<td>
0.750143
</td>
<td>
-3.750714
</td>
<td>
3.000571
</td>
<td>
-0.750143
</td>
</tr>
<tr>
<th>
2.0
</th>
<td>
0.741615
</td>
<td>
-3.708077
</td>
<td>
2.966462
</td>
<td>
-0.741615
</td>
</tr>
<tr>
<th>
3.0
</th>
<td>
0.645737
</td>
<td>
-3.343684
</td>
<td>
2.527947
</td>
<td>
-0.815737
</td>
</tr>
<tr>
<th>
4.0
</th>
<td>
0.645440
</td>
<td>
-3.487198
</td>
<td>
2.461759
</td>
<td>
-1.025440
</td>
</tr>
<tr>
<th>
5.0
</th>
<td>
0.615148
</td>
<td>
-3.510739
</td>
<td>
2.265591
</td>
<td>
-1.245148
</td>
</tr>
<tr>
<th>
6.0
</th>
<td>
0.603678
</td>
<td>
-3.658389
</td>
<td>
2.134711
</td>
<td>
-1.523678
</td>
</tr>
<tr>
<th>
7.0
</th>
<td>
0.590758
</td>
<td>
-3.828791
</td>
<td>
1.988033
</td>
<td>
-1.840758
</td>
</tr>
<tr>
<th>
8.0
</th>
<td>
0.584565
</td>
<td>
-4.062823
</td>
<td>
1.858259
</td>
<td>
-2.204565
</td>
</tr>
<tr>
<th>
9.0
</th>
<td>
0.575110
</td>
<td>
-4.310551
</td>
<td>
1.705441
</td>
<td>
-2.605110
</td>
</tr>
<tr>
<th>
10.0
</th>
<td>
0.571312
</td>
<td>
-4.616559
</td>
<td>
1.565247
</td>
<td>
-3.051312
</td>
</tr>
<tr>
<th>
11.0
</th>
<td>
0.569395
</td>
<td>
-4.961974
</td>
<td>
1.422579
</td>
<td>
-3.539395
</td>
</tr>
<tr>
<th>
12.0
</th>
<td>
0.561997
</td>
<td>
-5.309983
</td>
<td>
1.247986
</td>
<td>
-4.061997
</td>
</tr>
<tr>
<th>
13.0
</th>
<td>
0.556362
</td>
<td>
-5.696812
</td>
<td>
1.070450
</td>
<td>
-4.626362
</td>
</tr>
<tr>
<th>
14.0
</th>
<td>
0.552071
</td>
<td>
-6.120357
</td>
<td>
0.888286
</td>
<td>
-5.232071
</td>
</tr>
<tr>
<th>
15.0
</th>
<td>
0.559690
</td>
<td>
-6.633452
</td>
<td>
0.743762
</td>
<td>
-5.889690
</td>
</tr>
<tr>
<th>
16.0
</th>
<td>
0.546763
</td>
<td>
-7.073814
</td>
<td>
0.507051
</td>
<td>
-6.566763
</td>
</tr>
<tr>
<th>
17.0
</th>
<td>
0.540981
</td>
<td>
-7.579907
</td>
<td>
0.288926
</td>
<td>
-7.290981
</td>
</tr>
<tr>
<th>
18.0
</th>
<td>
0.540375
</td>
<td>
-8.141877
</td>
<td>
0.081502
</td>
<td>
-8.060375
</td>
</tr>
<tr>
<th>
19.0
</th>
<td>
0.532353
</td>
<td>
-8.696763
</td>
<td>
-0.165590
</td>
<td>
-8.862353
</td>
</tr>
<tr>
<th>
20.0
</th>
<td>
0.581473
</td>
<td>
-9.567364
</td>
<td>
-0.194109
</td>
<td>
-9.761473
</td>
</tr>
<tr>
<th>
21.0
</th>
<td>
0.622667
</td>
<td>
-10.428333
</td>
<td>
-0.264333
</td>
<td>
-10.692667
</td>
</tr>
<tr>
<th>
22.0
</th>
<td>
0.000000
</td>
<td>
-8.000000
</td>
<td>
-3.000000
</td>
<td>
-11.000000
</td>
</tr>
</tbody>
</table>
</div>
<h3 id="try-to-end-it-early...-the-upperright-midpoint-looks-good">Try to end it early... The upperright midpoint looks good</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.set_theme(style=<span class="string">&quot;ticks&quot;</span>, font_scale=<span class="number">1.25</span>,)</span><br><span class="line">sns.jointplot(data=df_sim, y=<span class="string">&quot;Kunzler&quot;</span>, x=<span class="string">&quot;Arnold&quot;</span>, hue=<span class="string">&quot;round&quot;</span>,height=<span class="number">8</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.
  warnings.warn(msg, UserWarning)</code></pre>
<p><img src="/2020/10/09/%E8%B0%88%E5%88%A4%E6%A1%88%E4%BE%8B%E6%A8%A1%E6%8B%9F%EF%BC%9ALeckenby-Company/LeckenbySimulation_files/LeckenbySimulation_19_1.png" /></p>
<h3 id="if-we-end-early-before-round3-what-would-be-a-good-proposition-answer-win-0.50.53">If we end early before round3, what would be a good proposition? Answer: <span class="math inline">\(w\in [0.5,0.53]\)</span></h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_early = df_sim.loc[df_sim[<span class="string">&quot;round&quot;</span>]&lt;=<span class="number">3</span>].sort_values(<span class="string">&quot;total&quot;</span>, ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(df_early[<span class="string">&quot;Kunzler&quot;</span>].min() + df_early[<span class="string">&quot;Kunzler&quot;</span>].max()) / <span class="number">2</span></span><br></pre></td></tr></table></figure>
<pre><code>-2.6799999999999997</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(df_early[<span class="string">&quot;Arnold&quot;</span>].min() + df_early[<span class="string">&quot;Arnold&quot;</span>].max()) / <span class="number">2</span></span><br></pre></td></tr></table></figure>
<pre><code>2.026500000000002</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Arnold(<span class="number">0.53</span>,<span class="number">0</span>), Kunzler(<span class="number">0.53</span>,<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(2.12, -2.6500000000000004)</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Arnold(<span class="number">0.5</span>,<span class="number">0</span>), Kunzler(<span class="number">0.5</span>,<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(2.0, -2.5)</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">sns.scatterplot(data=df_early, x=<span class="string">&quot;Arnold&quot;</span>, y=<span class="string">&quot;Kunzler&quot;</span>, hue=<span class="string">&quot;round&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/10/09/%E8%B0%88%E5%88%A4%E6%A1%88%E4%BE%8B%E6%A8%A1%E6%8B%9F%EF%BC%9ALeckenby-Company/LeckenbySimulation_files/LeckenbySimulation_26_0.png" /></p>
<h1 id="simulation-2-what-if-my-counterpart-does-not-end-early">Simulation 2: What if my counterpart does not end early?</h1>
<h2 id="another-simulation-function-exhibits-the-process-of-negotiation">Another simulation function: Exhibits the process of negotiation</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">simulation</span>():</span> <span class="comment"># Exhibits the process of negotiation</span></span><br><span class="line">    bid_Kunzler = <span class="number">10</span></span><br><span class="line">    bid_Arnold = <span class="number">11</span></span><br><span class="line">    round = <span class="number">0</span></span><br><span class="line">    <span class="comment"># p_Kunzler = 0</span></span><br><span class="line">    <span class="comment"># p_Arnold = 0</span></span><br><span class="line">    rows = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> ((bid_Kunzler+<span class="number">0.004</span> &lt; bid_Arnold) &amp; (round &lt;=<span class="number">22</span>)):</span><br><span class="line">        round += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> (np.random.random()&gt;<span class="number">0.2</span>): <span class="comment"># 20% chance stick to the previous bid</span></span><br><span class="line">            bid_Kunzler += (bid_Arnold - bid_Kunzler)*np.random.triangular(<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> (np.random.random()&gt;<span class="number">0.2</span>):</span><br><span class="line">            bid_Arnold -= (bid_Arnold - bid_Kunzler)*np.random.triangular(<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        w_Kunzler = bid_Kunzler- <span class="number">10</span></span><br><span class="line">        w_Arnold = bid_Arnold<span class="number">-10</span></span><br><span class="line">        d = (round &gt; <span class="number">2</span>)*(round - <span class="number">2</span>)</span><br><span class="line">        Kunzler_ = Kunzler(w_Kunzler, d)</span><br><span class="line">        Arnold_ = Arnold(w_Arnold, d)</span><br><span class="line">        rows.append(np.array([round, w_Kunzler, w_Arnold, Kunzler_, Arnold_]))</span><br><span class="line">    df = pd.DataFrame(np.array(rows), columns = [<span class="string">&quot;round&quot;</span>, <span class="string">&quot;w_Kunzler&quot;</span>, <span class="string">&quot;w_Arnold&quot;</span>, <span class="string">&quot;Kunzler_&quot;</span>, <span class="string">&quot;Arnold_&quot;</span>])</span><br><span class="line">    df = df.assign(sum = <span class="keyword">lambda</span> x: x.Kunzler_ + x.Arnold_)</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">simulation()</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
round
</th>
<th>
w_Kunzler
</th>
<th>
w_Arnold
</th>
<th>
Kunzler_
</th>
<th>
Arnold_
</th>
<th>
sum
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1.0
</td>
<td>
0.000000
</td>
<td>
0.868807
</td>
<td>
0.000000
</td>
<td>
3.475230
</td>
<td>
3.475230
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2.0
</td>
<td>
0.000000
</td>
<td>
0.420516
</td>
<td>
0.000000
</td>
<td>
1.682066
</td>
<td>
1.682066
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3.0
</td>
<td>
0.000297
</td>
<td>
0.420516
</td>
<td>
-0.116485
</td>
<td>
1.627066
</td>
<td>
1.510581
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4.0
</td>
<td>
0.107588
</td>
<td>
0.201068
</td>
<td>
-0.797939
</td>
<td>
0.684272
</td>
<td>
-0.113667
</td>
</tr>
<tr>
<th>
4
</th>
<td>
5.0
</td>
<td>
0.132680
</td>
<td>
0.157974
</td>
<td>
-1.098401
</td>
<td>
0.436896
</td>
<td>
-0.661505
</td>
</tr>
<tr>
<th>
5
</th>
<td>
6.0
</td>
<td>
0.147227
</td>
<td>
0.156502
</td>
<td>
-1.376134
</td>
<td>
0.346006
</td>
<td>
-1.030128
</td>
</tr>
<tr>
<th>
6
</th>
<td>
7.0
</td>
<td>
0.149804
</td>
<td>
0.156216
</td>
<td>
-1.624022
</td>
<td>
0.249866
</td>
<td>
-1.374156
</td>
</tr>
<tr>
<th>
7
</th>
<td>
8.0
</td>
<td>
0.151529
</td>
<td>
0.156216
</td>
<td>
-1.897647
</td>
<td>
0.144866
</td>
<td>
-1.752782
</td>
</tr>
<tr>
<th>
8
</th>
<td>
9.0
</td>
<td>
0.155463
</td>
<td>
0.155972
</td>
<td>
-2.212317
</td>
<td>
0.028890
</td>
<td>
-2.183427
</td>
</tr>
</tbody>
</table>
</div>
<h2 id="tool-for-finding-a-similar-caseround-w_k-w_a-when-negotiating">Tool for finding a similar case(round, w_K, w_A) when negotiating</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Arnold(<span class="number">0</span>, <span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<pre><code>-3.0</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Kunzler(<span class="number">0</span>, <span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<pre><code>-8.0</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_Case</span>(<span class="params">round, w_K, w_A</span>):</span></span><br><span class="line">    <span class="keyword">while</span>(<span class="literal">True</span>):</span><br><span class="line">        df = simulation()</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            row = df.iloc[round<span class="number">-1</span>]</span><br><span class="line">            flag = (np.abs(row[<span class="string">&quot;w_Kunzler&quot;</span>]-w_K)&lt;<span class="number">0.01</span>) &amp; (np.abs(row[<span class="string">&quot;w_Arnold&quot;</span>]-w_A)&lt;<span class="number">0.01</span>)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            flag = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> (flag):</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">find_Case(<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
round
</th>
<th>
w_Kunzler
</th>
<th>
w_Arnold
</th>
<th>
Kunzler_
</th>
<th>
Arnold_
</th>
<th>
sum
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1.0
</td>
<td>
0.000000
</td>
<td>
1.000000
</td>
<td>
0.000000
</td>
<td>
4.000000
</td>
<td>
4.000000
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2.0
</td>
<td>
0.000000
</td>
<td>
1.000000
</td>
<td>
0.000000
</td>
<td>
4.000000
</td>
<td>
4.000000
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3.0
</td>
<td>
0.382837
</td>
<td>
0.997914
</td>
<td>
-2.029186
</td>
<td>
3.936654
</td>
<td>
1.907469
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4.0
</td>
<td>
0.387834
</td>
<td>
0.603973
</td>
<td>
-2.199168
</td>
<td>
2.295893
</td>
<td>
0.096725
</td>
</tr>
<tr>
<th>
4
</th>
<td>
5.0
</td>
<td>
0.495310
</td>
<td>
0.586222
</td>
<td>
-2.911552
</td>
<td>
2.149887
</td>
<td>
-0.761664
</td>
</tr>
<tr>
<th>
5
</th>
<td>
6.0
</td>
<td>
0.529410
</td>
<td>
0.584511
</td>
<td>
-3.287050
</td>
<td>
2.058045
</td>
<td>
-1.229006
</td>
</tr>
<tr>
<th>
6
</th>
<td>
7.0
</td>
<td>
0.529410
</td>
<td>
0.579932
</td>
<td>
-3.522050
</td>
<td>
1.944727
</td>
<td>
-1.577323
</td>
</tr>
<tr>
<th>
7
</th>
<td>
8.0
</td>
<td>
0.533581
</td>
<td>
0.579932
</td>
<td>
-3.807903
</td>
<td>
1.839727
</td>
<td>
-1.968176
</td>
</tr>
<tr>
<th>
8
</th>
<td>
9.0
</td>
<td>
0.541261
</td>
<td>
0.577473
</td>
<td>
-4.141303
</td>
<td>
1.714890
</td>
<td>
-2.426413
</td>
</tr>
<tr>
<th>
9
</th>
<td>
10.0
</td>
<td>
0.541261
</td>
<td>
0.577473
</td>
<td>
-4.466303
</td>
<td>
1.589890
</td>
<td>
-2.876413
</td>
</tr>
<tr>
<th>
10
</th>
<td>
11.0
</td>
<td>
0.541261
</td>
<td>
0.575655
</td>
<td>
-4.821303
</td>
<td>
1.447619
</td>
<td>
-3.373685
</td>
</tr>
<tr>
<th>
11
</th>
<td>
12.0
</td>
<td>
0.572203
</td>
<td>
0.575655
</td>
<td>
-5.361013
</td>
<td>
1.302619
</td>
<td>
-4.058394
</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">find_Case(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
round
</th>
<th>
w_Kunzler
</th>
<th>
w_Arnold
</th>
<th>
Kunzler_
</th>
<th>
Arnold_
</th>
<th>
sum
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1.0
</td>
<td>
0.000000
</td>
<td>
1.000000
</td>
<td>
0.000000
</td>
<td>
4.000000
</td>
<td>
4.000000
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2.0
</td>
<td>
0.000000
</td>
<td>
1.000000
</td>
<td>
0.000000
</td>
<td>
4.000000
</td>
<td>
4.000000
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3.0
</td>
<td>
0.000000
</td>
<td>
0.470998
</td>
<td>
-0.115000
</td>
<td>
1.828994
</td>
<td>
1.713994
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4.0
</td>
<td>
0.000000
</td>
<td>
0.337204
</td>
<td>
-0.260000
</td>
<td>
1.228817
</td>
<td>
0.968817
</td>
</tr>
<tr>
<th>
4
</th>
<td>
5.0
</td>
<td>
0.000000
</td>
<td>
0.094733
</td>
<td>
-0.435000
</td>
<td>
0.183932
</td>
<td>
-0.251068
</td>
</tr>
<tr>
<th>
5
</th>
<td>
6.0
</td>
<td>
0.000000
</td>
<td>
0.094733
</td>
<td>
-0.640000
</td>
<td>
0.098932
</td>
<td>
-0.541068
</td>
</tr>
<tr>
<th>
6
</th>
<td>
7.0
</td>
<td>
0.000000
</td>
<td>
0.072679
</td>
<td>
-0.875000
</td>
<td>
-0.084283
</td>
<td>
-0.959283
</td>
</tr>
<tr>
<th>
7
</th>
<td>
8.0
</td>
<td>
0.009111
</td>
<td>
0.068337
</td>
<td>
-1.185556
</td>
<td>
-0.206651
</td>
<td>
-1.392207
</td>
</tr>
<tr>
<th>
8
</th>
<td>
9.0
</td>
<td>
0.044854
</td>
<td>
0.058973
</td>
<td>
-1.659270
</td>
<td>
-0.359107
</td>
<td>
-2.018377
</td>
</tr>
<tr>
<th>
9
</th>
<td>
10.0
</td>
<td>
0.047908
</td>
<td>
0.056863
</td>
<td>
-1.999541
</td>
<td>
-0.492550
</td>
<td>
-2.492091
</td>
</tr>
<tr>
<th>
10
</th>
<td>
11.0
</td>
<td>
0.048806
</td>
<td>
0.055739
</td>
<td>
-2.359032
</td>
<td>
-0.632043
</td>
<td>
-2.991074
</td>
</tr>
<tr>
<th>
11
</th>
<td>
12.0
</td>
<td>
0.048806
</td>
<td>
0.052226
</td>
<td>
-2.744032
</td>
<td>
-0.791096
</td>
<td>
-3.535128
</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">find_Case(<span class="number">4</span>, <span class="number">0.3</span>, <span class="number">0.6</span>)</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
round
</th>
<th>
w_Kunzler
</th>
<th>
w_Arnold
</th>
<th>
Kunzler_
</th>
<th>
Arnold_
</th>
<th>
sum
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1.0
</td>
<td>
0.155860
</td>
<td>
0.919539
</td>
<td>
-0.779300
</td>
<td>
3.678158
</td>
<td>
2.898858
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2.0
</td>
<td>
0.262073
</td>
<td>
0.819940
</td>
<td>
-1.310366
</td>
<td>
3.279758
</td>
<td>
1.969392
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3.0
</td>
<td>
0.304963
</td>
<td>
0.808350
</td>
<td>
-1.639815
</td>
<td>
3.178401
</td>
<td>
1.538586
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4.0
</td>
<td>
0.304963
</td>
<td>
0.597116
</td>
<td>
-1.784815
</td>
<td>
2.268465
</td>
<td>
0.483650
</td>
</tr>
<tr>
<th>
4
</th>
<td>
5.0
</td>
<td>
0.400535
</td>
<td>
0.532053
</td>
<td>
-2.437677
</td>
<td>
1.933211
</td>
<td>
-0.504466
</td>
</tr>
<tr>
<th>
5
</th>
<td>
6.0
</td>
<td>
0.400584
</td>
<td>
0.417896
</td>
<td>
-2.642919
</td>
<td>
1.391584
</td>
<td>
-1.251335
</td>
</tr>
<tr>
<th>
6
</th>
<td>
7.0
</td>
<td>
0.412972
</td>
<td>
0.415010
</td>
<td>
-2.939859
</td>
<td>
1.285039
</td>
<td>
-1.654820
</td>
</tr>
</tbody>
</table>
</div>
<h2 id="do-not-concede-before-round-4">Do not concede before Round 4!!!</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">%<span class="built_in">cd</span> /content/drive/My Drive/20FA</span><br><span class="line">!jupyter nb</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Simulation</category>
      </categories>
      <tags>
        <tag>Managerial Negotiation</tag>
      </tags>
  </entry>
  <entry>
    <title>IEOR E4706: Martingale Pricing Theory (Discrete Time&amp;Space)</title>
    <url>/2021/02/15/IEOR-E4706-Martingale-Pricing-Theory-Discrete-Time-Space/</url>
    <content><![CDATA[<h2 id="回顾：非齐次线性方程组的解与矩阵的列秩"><a href="#回顾：非齐次线性方程组的解与矩阵的列秩" class="headerlink" title="回顾：非齐次线性方程组的解与矩阵的列秩"></a>回顾：非齐次线性方程组的解与矩阵的列秩</h2><p>假设 $A: (m\times n), x:(n), b:(m)$，$Ax = b$</p>
<p>若方程组有解，则 $b$ 是 $A$ 的列向量的线性组合。</p>
<p>如果 $A$ 的列秩大于等于 $m$，$A$ 的列向量能够张成 $\mathbb{R}^m$，则对于任意 $b$， $Ax=b$ 有解。</p>
<p>典型的 $m\times m$ 满秩矩阵 $A$ 可逆， $Ax=b$ 有唯一解 $A^{-1}b$。</p>
<p>非齐次线性方程组有解的充分必要条件是其系数矩阵和增广矩阵的秩相等，即包含 $b$ 后的列空间没有变大，可以被稀疏矩阵的列向量表示。</p>
<hr>
<h2 id="单阶段的鞅定价理论"><a href="#单阶段的鞅定价理论" class="headerlink" title="单阶段的鞅定价理论"></a>单阶段的鞅定价理论</h2><hr>
<h3 id="符号表示和定义"><a href="#符号表示和定义" class="headerlink" title="符号表示和定义"></a>符号表示和定义</h3><p>$t=1$ 时刻的不同状态: $\omega_1, …, \omega_m$。</p>
<p>$S_0^{(i)}$: $0$ 时刻 $i$ 证券的价值。</p>
<p>$S_1^{(i)}(\omega_j)$: $1$ 时刻第 $j$ 状态第 $i$ 证券的价值。</p>
<p>$P = (p_1, …, p_m)$: 每个状态的<strong>真实</strong>概率分布</p>
<h4 id="套利"><a href="#套利" class="headerlink" title="套利"></a><u>套利</u></h4><ul>
<li>Type A arbitrage: $t=0$ 即刻获利， $t=1$ 没有成本。</li>
<li>Type B arbitrage: $t=0$ 非正成本， $t=1$ 有正概率获利，零概率损失。</li>
</ul>
<h4 id="线性定价-Linear-pricing"><a href="#线性定价-Linear-pricing" class="headerlink" title="线性定价 Linear pricing"></a><u>线性定价 Linear pricing</u></h4><p>Type A 无套利要求，远期获利如果是两个证券的获利的组合，则当前定价也应是两个证券相同系数的线性组合。</p>
<h4 id="Elementary-Securities-Attainability-amp-State-Prices"><a href="#Elementary-Securities-Attainability-amp-State-Prices" class="headerlink" title="Elementary Securities, Attainability &amp; State Prices"></a><u>Elementary Securities, Attainability &amp; State Prices</u></h4><p><strong>基本证券</strong>：</p>
<p>对应 m 个状态，只有一种状态下收益为 1，其余为 0。$\mathbb{R}^m$ 的一组基。</p>
<p><strong>可获得性</strong>：</p>
<p>$$\begin{bmatrix} X(w_1) \…\X(w_m)\end{bmatrix} =<br>\begin{bmatrix} S_1^{(0)}(w_1) &amp;… &amp;S_1^{(N)}(w_1)  \ &amp; …\ S_1^{(0)}(w_m) &amp;… &amp;S_1^{(N)}(w_m) \end{bmatrix}<br>\begin{bmatrix} \theta_0 \…\ \theta_N\end{bmatrix}<br>$$</p>
<p>注：是 A 型套利 - 线性定价的延申。 $X = S_1\theta$， $\theta$ 称为复制组合。$X$ 是该组合 1 时刻的价值。</p>
<p>例: 对于给定的 $S1$，能求出 $\theta$， $X$ 就 attainable。与资产的0时刻成本无关。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">S1 = np.array(</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">1.03</span>, <span class="number">1.03</span>, <span class="number">1.03</span>, <span class="number">1.03</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>],</span><br><span class="line">    ]</span><br><span class="line">).T</span><br><span class="line"></span><br><span class="line">X = np.array([<span class="number">7.47</span>, <span class="number">6.97</span>, <span class="number">9.97</span>, <span class="number">10.47</span>])</span><br><span class="line"></span><br><span class="line">np.linalg.lstsq(S1, X)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(array([<span class="number">-1.</span> ,  <span class="number">1.5</span>,  <span class="number">2.</span> ]),</span><br><span class="line"> array([<span class="number">1.22198944e-30</span>]),</span><br><span class="line"> <span class="number">3</span>,</span><br><span class="line"> array([<span class="number">8.67020663</span>, <span class="number">2.82663493</span>, <span class="number">0.28504733</span>]))</span><br></pre></td></tr></table></figure>

<p><strong>A型套利</strong>: 指 $\theta$ 满足<br>$S_0\theta &lt; 0$， $S_1\theta = 0$</p>
<p><strong>B型套利</strong>:<br>$S_0\theta \leq 0$，  $S_1\theta \geq 0$(且不恒等于0)</p>
<p><strong>状态价格</strong>:</p>
<p>such $\pi :(m,)$ that:</p>
<p>$$P = \pi^TX$$</p>
<p>注：$\pi$ 使得任意可获得的证券 $X$ 能够被公允定价并不存在套利机会。</p>
<p>如果 k-th Elementary security is attainable,</p>
<p>$$P = \pi_k$$</p>
<p>其价格必为 $\pi_k$。 同时所有不同的 state price 向量 $\pi$ 的第 k 项必然相同。</p>
<p><strong>状态价格的求解</strong>：</p>
<p>在例子中，我们有3种security，4种状态。</p>
<p>$S_1$ 矩阵的每一列是一种 security 在 m 种状态下的价值。$S_0$ 是 N 种 security 在 0 时刻的价格。</p>
<p>对某种资产，状态价格加权不同的状态得到 0 时刻资产的价值。</p>
<p>$$S_0[i] = S_1^T[i]\pi$$</p>
<p>对3种资产就建立了关于状态价格向量 $\pi$ 的方程组：</p>
<p>$$S_0 = S_1^T \pi$$</p>
<p>$S_1^T$ 是一个 $3\times 4$ 矩阵。一般的例子里无冗余资产，即 $S_1^T$ 的行满秩。列不满秩因而非齐次方程组 $S_0 = S_1^T\pi$ 的解 $\pi$ 有无穷多解。取合适的通解系数 $\epsilon$ 得到正解。</p>
<h4 id="以-计价标准资产-numeraire-security-折价"><a href="#以-计价标准资产-numeraire-security-折价" class="headerlink" title=" 以 计价标准资产 (numeraire security) 折价 "></a><u> 以 计价标准资产 (numeraire security) 折价 </u></h4><p><strong>计价标准资产</strong>(numeraire security)只需满足任意时刻价格严格为正。</p>
<p>Deflated:</p>
<p>$$\bar{S}_t^{(i)}(w_j) := \frac{S_t^{(i)}(w_j)}{S_t^{(n)}(w_j)}$$</p>
<p>计价资产本身折价后为1。</p>
<p><strong>现金账户</strong>(cash account) </p>
<p>1 -&gt; 1+r。</p>
<p>习惯第0个资产是现金账户。</p>
<h4 id="等价鞅测度-EMM-Equivalent-Martingale-Measure"><a href="#等价鞅测度-EMM-Equivalent-Martingale-Measure" class="headerlink" title=" 等价鞅测度 EMM (Equivalent Martingale Measure) "></a><u> 等价鞅测度 EMM (Equivalent Martingale Measure) </u></h4><p>等价鞅测度 / 风险中性测度 满足：</p>
<ul>
<li>所有状态的概率大于零</li>
<li>折价后的证券价格是鞅</li>
</ul>
<p>$$\bar{S}_0^{(i)} := E_0^Q \left[\bar{S}_1^{(i)} \right]$$</p>
<p>注意EMM 和某个 numeraire 是对应的。更换 numeraire 时发生测度变换，称为 EMM 变换。</p>
<h4 id="完备市场-Complete-markets"><a href="#完备市场-Complete-markets" class="headerlink" title=" 完备市场 Complete markets "></a><u> 完备市场 Complete markets </u></h4><p>完备市场具有所有的基本证券 (Elementary securities) $e = [e_1, …, e_m]^T$。</p>
<p>某种证券 $x$ 在 $1$ 时刻的价值向量 ($m$ 种状态)</p>
<p>$$x = [x_1 … x_m]^T$$</p>
<p>可以表示为基本证券的线性组合</p>
<p>$$x = \sum_{i=1}^{m}{x_ie_i}$$</p>
<p>根据 状态价格(state price) 的定义，也可理解为 $\pi_k$ 是基本证券 $e_k$ 的零时刻定价，</p>
<p>$$S_0 = \sum_{i=1}^{m}{x_i \pi_i}$$</p>
<p>如果所有的基本证券都存在，任意的 $X$ (m 维) 都 attainable，则称市场完备。</p>
<hr>
<h3 id="单阶段模型中的定价"><a href="#单阶段模型中的定价" class="headerlink" title="单阶段模型中的定价"></a>单阶段模型中的定价</h3><blockquote>
<p><strong>Proposition 1</strong>: 若某个等价鞅测度 Q 存在， 则无套利。</p>
</blockquote>
<p>分析：</p>
<p>$$\bar{S}_0^{(i)} := E_0^Q \left[\bar{S}_1^{(i)} \right]$$</p>
<p>结合 EMM 定义中 $q_k&gt;0$ 以及 $A, B$ 型套利的定义可以证明 0 时刻价格无法取到负号。 用到的一个性质是资产价格为正。</p>
<p>如果放松 $q_k&gt;0$ 的约束，就有可能取到负号，从而创造套利机会。</p>
<blockquote>
<p><strong>Theorem 2</strong>: 假设有一个严格为正的价格过程 $S_t^{(n)}$ 可以作为计价资产，且有一系列严格为正的状态价格 $\pi$，则存在一个唯一对应的风险中性测度 $Q$。</p>
</blockquote>
<p>证明：</p>
<p>$$\begin{aligned}<br>S_0^{(j)} &amp;= \sum_{k=1}^{m}{\pi_kS_1^{(j)}(w_k)} \<br>&amp;= \left(\sum_{l=1}^{m}{\pi_lS_1^{(n)}(w_l)}\right) \sum_{k=1}^{m}{ \frac{\pi_k S_1^{(n)}(w_k) }{\sum_{l=1}^{m}{\pi_lS_1^{(n)}(w_l)}} \frac{S_1^{(j)}(w_k)}{S_1^{(n)}(w_k)} }  \<br>&amp;= S_0^{(n)} \sum_{k=1}^{m}{ \frac{\pi_k S_1^{(n)}(w_k) }{\sum_{l=1}^{m}{\pi_lS_1^{(n)}(w_l)}} \frac{S_1^{(j)}(w_k)}{S_1^{(n)}(w_k)} }<br>\end{aligned}$$</p>
<p>定义</p>
<p>$$q_k := \frac{\pi_k S_1^{(n)}(w_k) }{\sum_{l=1}^{m}{\pi_lS_1^{(n)}(w_l)}}$$</p>
<p>可以看出 $Q$ 和 numeraire 的价格、state price 是对应的。</p>
<p>则可以得出 按某计价资产 deflate 的资产价格过程是鞅，以Q为测度。</p>
<p>$$\begin{aligned}<br>\frac{S_0^{(j)}}{S_0^{(n)}} &amp;= \sum_{k=1}^{m}{q_k \frac{S_1^{(j)}(w_k)}{S_1^{(n)}(w_k)} } &amp;= E_0^Q \left[\frac{S_1^{(j)}}{S_1^{(n)}} \right]<br>\end{aligned}$$</p>
<blockquote>
<p><strong>Remark 3</strong>: 真实概率 $P$ 和 $Q$ 的唯一联系是 “等效” —— 有一一映射关系。</p>
</blockquote>
<h4 id="无套利-正状态价格-pi-存在-存在等价鞅测度-EMM"><a href="#无套利-正状态价格-pi-存在-存在等价鞅测度-EMM" class="headerlink" title=" 无套利 = 正状态价格 $\pi$ 存在 = 存在等价鞅测度 EMM "></a><u> 无套利 = 正状态价格 $\pi$ 存在 = 存在等价鞅测度 EMM </u></h4><blockquote>
<p><strong>Theorem 3 &amp; 4</strong>: $Ax=p, p\geq 0$ 只在 $p=0$ 时有解，则存在 $y&gt;0$ 使得 $A^Ty=0$。证明使用对偶规划。</p>
<p>解释了在无套利情况下存在 state price $\pi &gt; 0$。</p>
</blockquote>
<blockquote>
<p><strong>Theorem 5 (First Fundamental Theorem of Asset Pricing)</strong>: 无套利、存在 EMM $Q$ 是等价的。</p>
</blockquote>
<blockquote>
<p><strong>Example 5</strong>:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">S1 = np.array([</span><br><span class="line">    [<span class="number">1.05</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">    [<span class="number">1.05</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">])</span><br><span class="line">S0 = np.array([<span class="number">1</span>, <span class="number">1.9571</span>, <span class="number">2.2048</span>])</span><br></pre></td></tr></table></figure>

<blockquote>
<p>$S_0 = S_1^T\pi$ 系数矩阵 $3\times2$，秩为2，增广矩阵秩为3。无解。</p>
<p>因而没有一组正的 state price $\pi$。有套利机会。</p>
</blockquote>
<blockquote>
<p><strong>Exercise 4</strong>: 找到一个套利策略 $\theta$</p>
<p>模仿 <strong>Theorem 4</strong> 的证明过程，写出 $(m+1)\times(N+1)$ 的矩阵 $A$，<br>$$A = \begin{bmatrix} S_1^{(0)}(w_1) &amp;… &amp;S_1^{(N)}(w_1)  \ &amp; …\ S_1^{(0)}(w_m) &amp;… &amp;S_1^{(N)}(w_m) \-S_0^{(0)} &amp;… &amp;-S_0^{(m)} \end{bmatrix} = \begin{bmatrix} 1.05 &amp;2 &amp;3  \ 1.05 &amp;1 &amp;2 \-1 &amp;-1.9571 &amp; -2.2048 \end{bmatrix}$$<br>若可套利，则说明存在 $x$ 使得 $Ax \geq 0$ 且不恒等于0。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = np.array([</span><br><span class="line">    [<span class="number">1.05</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">    [<span class="number">1.05</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">-1</span>, <span class="number">-1.9571</span>, <span class="number">-2.2048</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000000</span>):</span><br><span class="line">    a = np.random.randn(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">if</span> (np.dot(A, a) &gt;= <span class="number">0</span>).sum() == <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">a, np.dot(A, a)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(array([ <span class="number">0.0886371</span> , <span class="number">-0.30878999</span>,  <span class="number">0.1865141</span> ]),</span><br><span class="line"> array([<span class="number">0.03503128</span>, <span class="number">0.15730717</span>, <span class="number">0.1044695</span> ]))</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>Theorem 6</strong>: 假设无套利，市场 <strong>完备</strong> 的充要条件是 $S_1$ 的秩为 $m$。</p>
<p>考虑到 $S_1$ 的行数为状态数 $m$，则应当有 $m$ 种基本证券，$n\geq m = r$。</p>
<p>$n &lt; m$ 的市场必然是不完备的。</p>
</blockquote>
<blockquote>
<p><strong>Theorem 7 (Second Fundamental Theorem of Asset Pricing)</strong>: 假设存在一个计价资产 (严格为正的价格过程) 且无套利 (存在状态价格，EMM)，则市场完备的充要条件是有且仅有一个 EMM / $\pi$ / Q。</p>
<p>证明：假设存在不可获得的 $X$，使用对偶规划，得出存在另一个风险中性测度 $\hat{Q} \neq Q$。</p>
</blockquote>
<hr>
<h2 id="多阶段鞅定价理论"><a href="#多阶段鞅定价理论" class="headerlink" title="多阶段鞅定价理论"></a>多阶段鞅定价理论</h2><hr>
<h3 id="符号表示和定义-1"><a href="#符号表示和定义-1" class="headerlink" title="符号表示和定义"></a>符号表示和定义</h3><p>多阶段的二叉树表示：</p>
<img src="tree.jpg" height=400 align="middle">

<h4 id="交易策略-theta-t-自负盈亏的交易策略"><a href="#交易策略-theta-t-自负盈亏的交易策略" class="headerlink" title=" 交易策略 $\theta_t$ / 自负盈亏的交易策略 "></a><u> 交易策略 $\theta_t$ / 自负盈亏的交易策略 </u></h4><blockquote>
<p><strong>Definition 10</strong>: 可预测的 (predictable) 随机过程，在给定所有 $t-1$ 时刻的信息时，$t$ 时刻的值 $X_t$ 已知。</p>
</blockquote>
<blockquote>
<p><strong>Definition 11</strong>: 交易策略 (trading strategy) 指 $\theta_t = \left[\theta_t^{(0)}(w) … \theta_t^{(N)}(w)\right]^T$，是关于$t, w$的函数。事实上是 $t-1$ 到 $t$ 之间的持有数量，在 $t-1$ 时刻被确定 (根据可预测性， $w$ 已知)。</p>
<p>有时省略掉 $w$，直接表示为 $\theta_t^{(i)}$。</p>
</blockquote>
<p>根据交易策略的可预测性，同一节点伸出的枝如 $w_1, w_2, w_3$ 对应前一时间点相同的交易策略。</p>
<blockquote>
<p><strong>Definition 12</strong>: 价值过程 value process $V_t(\theta)$<br>$$V_t = \begin{cases} \sum_{i=0}^m{\theta_1^{(i)}S_0^{(i)}}, &amp;t=0 \ \sum_{i=0}^m{\theta_t^{(i)}S_t^{(i)}}, &amp;t \geq 1  \end{cases}$$</p>
</blockquote>
<blockquote>
<p><strong>Definition 13</strong> self-financing trading strategy: 指满足下式的 $\theta_t$<br>$$V_t = \sum_{i=0}^N{\theta_{t+1}^{(i)}S_{t}^{(i)}}, t = 1, …, T-1$$<br>当然，定义仍然满足<br>$$V_t = \sum_{i=0}^m{\theta_t^{(i)}S_t^{(i)}}, t = 1, …, T-1$$<br>也就是说在 $t$ 时刻对交易策略进行 rebalancing 后投资组合的价值不变。</p>
</blockquote>
<p>self-financing 的交易策略满足：<br>$$V_{t+1} - V_t = \sum_{i=0}^{N}{\theta_{t+1}^{(i)} \left( S_{t+1}^{(i)} - S_{t}^{(i)} \right)}$$</p>
<p>该式表明投资组合价格的变动来自于价格的变动。</p>
<p>连续时间 self-financing trading strategy 则满足</p>
<p>$$dV = \theta_t^T dS_t$$</p>
<h4 id="套利：多阶段"><a href="#套利：多阶段" class="headerlink" title=" 套利：多阶段 "></a><u> 套利：多阶段 </u></h4><blockquote>
<p><strong>Definition 14</strong>:</p>
<p>A 型套利机会指 self-financing 的交易策略 $\theta_t$ 满足 $V_0(\theta) &lt; 0$, $V_T(\theta) =0$; </p>
<p>B 型套利机会指 self-financing 的交易策略 $\theta_t$ 满足 $V_0(\theta) = 0$, $V_T(\theta) \geq 0$ 以及 $E_0^P[V_T(\theta)] &gt; 0$。</p>
</blockquote>
<h4 id="可获得性和完备市场-Attainability-amp-Complete-Markets"><a href="#可获得性和完备市场-Attainability-amp-Complete-Markets" class="headerlink" title=" 可获得性和完备市场 Attainability &amp; Complete Markets "></a><u> 可获得性和完备市场 Attainability &amp; Complete Markets </u></h4><blockquote>
<p><strong>Definition 15 &amp; 16</strong>: Contingent claim 或有索取权 $C$ 在 $T$ 时刻的价值在那时已知。$C$ 是可获得的，如果存在 self-financing 的 $\theta_t$ 及其价值过程 $V_t$，使得 $V_T = C$。</p>
<p>$V_0$ 可以看作是 $C$ 的一个复制组合 replicating portfolio。</p>
<p>根据无套利 $C$ 的价值是 $V_0$。</p>
</blockquote>
<blockquote>
<p><strong>Definition 17</strong>: 如果所有 $C$ 都可获得则 市场完备。</p>
</blockquote>
<h4 id="等价鞅测度-EMM"><a href="#等价鞅测度-EMM" class="headerlink" title=" 等价鞅测度 EMM "></a><u> 等价鞅测度 EMM </u></h4><p>假设一个特定的计价资产 $S_t^{(n)}$。</p>
<blockquote>
<p><strong>Definition 18</strong>：一个 EMM (又称风险中性概率)，$Q = (q_1, …, q_m)$，满足:</p>
<ul>
<li>$q_i &gt; 0$</li>
<li>折价后的证券价格是鞅。即对于任意 $s,t \geq 0, i = 0, …, N$<br>$$\bar{S}_t^{(i)} = E_t^Q\left[ \bar{S}_{t+s}^{(i)} \right]$$<br>$E$ 的下标 $t$ 指该条件期望基于 $t$ 时刻的已有信息。</li>
</ul>
</blockquote>
<hr>
<h3 id="多阶段模型中的定价"><a href="#多阶段模型中的定价" class="headerlink" title="多阶段模型中的定价"></a>多阶段模型中的定价</h3><h4 id="无套利-存在EMM"><a href="#无套利-存在EMM" class="headerlink" title=" 无套利 = 存在EMM "></a><u> 无套利 = 存在EMM </u></h4><blockquote>
<p><strong>Proposition 8</strong>: 如果某 EMM $Q$ 存在，则任意 self-financing 的交易策略折价后的价值过程 $V_t$ 是一个 $Q$-鞅。</p>
<p>证明：由于 <strong>Definition 18</strong> EMM 的定义，证券价格 $\bar{S}_t$ 是一个 $Q$-鞅，所以 $V_t$ 作为 $\bar{S}_t$ 的加权也是 $Q$-鞅。<br>$$E_t^Q[\bar{V}_{t+1}] = \bar{V}_t$$</p>
</blockquote>
<blockquote>
<p><strong>Theorem 10 (Fundamental Theorem of Asset Pricing: Pt1)</strong></p>
<p>在多阶段模型中，无套利 = 存在 EMM $Q$</p>
</blockquote>
<h4 id="完备市场-唯一的EMM"><a href="#完备市场-唯一的EMM" class="headerlink" title=" 完备市场 = 唯一的EMM "></a><u> 完备市场 = 唯一的EMM </u></h4><blockquote>
<p><strong>Proposition 11</strong>: 多阶段模型市场完备的充要条件是每个单阶段模型都完备。</p>
<p>证明： </p>
<p>单阶段完备即：任意 $X = [x_1, …, x_m]^T$, $X = S_1\theta$ 有解。<br>多阶段完备则 $X = S_T…S_1\theta$ 有解。</p>
<p>充分性：根据递推易知，假设单阶段完备，从最后向前递推，$X = S_T \theta$ 有解 $\theta_T$，$\theta_T = S_{T-1}\theta$ 有解 $\theta_{T-1}$，如此递推下去$X = S_T…S_1\theta$ 有解，则多阶段完备；</p>
<p>必要性：若递推中的一个环节无解，则多阶段完备自然不满足。</p>
</blockquote>
<blockquote>
<p><strong>Theorem 12 (Fundamental Theorem of Asset Pricing: Pt2)</strong><br> 假设存在 numeraire 的 value process，无套利，则市场完备的充要条件是有且仅有一个风险中性的鞅测度 $Q$。</p>
<p>证明：多阶段完备 $\leftrightarrow$ 单阶段完备。每个阶段都有一个条件概率分布。根据条件概率公式可以得到唯一的总的概率分布。</p>
</blockquote>
<h4 id="状态价格的表示"><a href="#状态价格的表示" class="headerlink" title=" 状态价格的表示 "></a><u> 状态价格的表示 </u></h4><p>CRR 二叉树的中间节点并不直接对应某一种确定的状态 —— 可能对应几个不同的叶子节点。对于 $I_1^{4,5}$ 这个节点状态价格的符号表示应当包含所有的最终状态 $\pi_{0}^{1}(w_4, w_5)$。 通用的表达式 $I_t^{t+s}(\Lambda)$，$\Lambda$ 表示该中间结点所有 $t+s$ 时刻叶子节点状态的集合。</p>
<hr>
<h2 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h2><h3 id="样例9：完备市场"><a href="#样例9：完备市场" class="headerlink" title="样例9：完备市场"></a>样例9：完备市场</h3><img src= "ex9.jpg" height=800 >

]]></content>
      <categories>
        <category>Finance</category>
      </categories>
      <tags>
        <tag>Financial Engineering</tag>
      </tags>
  </entry>
</search>
